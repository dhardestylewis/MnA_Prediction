# -*- coding: utf-8 -*-
"""mna_colab_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1...

# M&A Prediction Pipeline (Redesigned)
**Persona**: Senior Quant ML Engineer
**Objective**: Robust, reproducible M&A forecasting with multi-horizon targets (3m-24m) and S&P 500 benchmarking.
"""

# %% [markdown]
# # Cell A: Setup, Config & Mount Drive
#
# **Goal**: Initialize environment, install missing dependencies, and define global configuration.

# %%
import os
import sys
import time
import json
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# 1. Install dependencies if missing (Colab specific)
try:
    import yfinance as yf
    print("[INFO] yfinance already installed.")
except ImportError:
    print("[INFO] Installing yfinance...")
    !pip install yfinance
    import yfinance as yf

# 2. Mount Google Drive (for large parquet files)
try:
    from google.colab import drive
    drive.mount('/content/drive')
    DRIVE_DIR = "/content/drive/MyDrive"
    IN_COLAB = True
except ImportError:
    DRIVE_DIR = os.getcwd()
    IN_COLAB = False
    print(f"[WARN] Not running in Colab. Using local DRIVE_DIR: {DRIVE_DIR}")

# 3. Clone GitHub repo (for pipeline code + smaller data files)
GITHUB_REPO = "https://github.com/dhardestylewis/MnA_Prediction.git"
REPO_DIR = "/content/MnA_Prediction" if IN_COLAB else os.getcwd()

if IN_COLAB and not os.path.exists(REPO_DIR):
    print(f"[INFO] Cloning GitHub repo to {REPO_DIR}...")
    !git clone {GITHUB_REPO} {REPO_DIR}
elif IN_COLAB:
    print(f"[INFO] Repo already exists at {REPO_DIR}. Pulling latest...")
    !cd {REPO_DIR} && git pull

# 4. Helper: Resolve file path (GitHub repo first, then Drive)
def resolve_path(filename, prefer_repo=True):
    """Find file in GitHub repo first, fallback to Google Drive."""
    repo_path = os.path.join(REPO_DIR, filename)
    drive_path = os.path.join(DRIVE_DIR, filename)
    
    if prefer_repo and os.path.exists(repo_path):
        return repo_path
    elif os.path.exists(drive_path):
        return drive_path
    elif os.path.exists(repo_path):
        return repo_path
    else:
        print(f"[WARN] File not found: {filename}")
        return None

# 5. Global Config
CONFIG = {
    "run_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
    "repo_dir": REPO_DIR,
    "drive_dir": DRIVE_DIR,
    "artifact_subfolder": "mna_artifacts",
    "inputs": {
        # Large files -> Google Drive only
        "fundq": "fundq_full.parquet",
        "funda": "funda_full.parquet",
        # Smaller files -> GitHub repo preferred (new directory structure)
        "deals": "data/deals/dma_corpus_metadata_with_factset_id.csv",
        "factset_xls_dir": "data/deals/factset_xls/factset_2000_2025",
        "fundamentals_csv": "data/fundamentals/compustat_funda_2000on.csv"
    },
    "horizons_months": [3, 6, 9, 12, 15, 18, 21, 24],
    "safe_lag_days": 90,
    "filters": {
        "keyset": "STD",  # Require Standard Feed
        "min_market_cap_log": 0.0  # Optional raw filter
    },
    "calibration": {
        "test_size_qtrs": 8,  # Not strictly used in expanding window, but for ref
        "n_dryrun_trials": 5
    }
}

# 6. Create Artifact Directory (on Google Drive for persistence)
ARTIFACT_DIR = os.path.join(DRIVE_DIR, CONFIG["artifact_subfolder"], CONFIG["run_id"])
os.makedirs(ARTIFACT_DIR, exist_ok=True)
print(f"[INFO] Artifacts will be saved to: {ARTIFACT_DIR}")

# 7. Save Config
config_path = os.path.join(ARTIFACT_DIR, "config.json")
with open(config_path, "w") as f:
    json.dump(CONFIG, f, indent=4, default=str)

def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

# ================================================
# CHECKPOINT / RESUME INFRASTRUCTURE
# ================================================
# Set RESUME_FROM_RUN_ID to resume from a previous run's checkpoints
# Leave as None to start fresh
RESUME_FROM_RUN_ID = None  # e.g., "20260129_222354" to resume

# Determine checkpoint directory
if RESUME_FROM_RUN_ID:
    CHECKPOINT_DIR = os.path.join(DRIVE_DIR, CONFIG["artifact_subfolder"], RESUME_FROM_RUN_ID)
    if os.path.exists(CHECKPOINT_DIR):
        log(f"â™»ï¸ RESUMING from run: {RESUME_FROM_RUN_ID}")
    else:
        log(f"[WARN] Resume run {RESUME_FROM_RUN_ID} not found, starting fresh")
        CHECKPOINT_DIR = ARTIFACT_DIR
else:
    CHECKPOINT_DIR = ARTIFACT_DIR

def checkpoint_exists(name):
    """Check if a checkpoint file exists."""
    path = os.path.join(CHECKPOINT_DIR, f"{name}.parquet")
    return os.path.exists(path)

def save_checkpoint(df, name):
    """Save DataFrame checkpoint to Drive."""
    path = os.path.join(ARTIFACT_DIR, f"{name}.parquet")
    df.to_parquet(path, index=False)
    log(f"ðŸ’¾ Checkpoint saved: {name} ({len(df)} rows)")
    return path

def load_checkpoint(name):
    """Load DataFrame from checkpoint."""
    path = os.path.join(CHECKPOINT_DIR, f"{name}.parquet")
    if os.path.exists(path):
        df = pd.read_parquet(path)
        log(f"ðŸ“‚ Checkpoint loaded: {name} ({len(df)} rows)")
        return df
    return None

log("âœ… Cell A Complete: Environment Setup.")

# %% [markdown]
# # Cell A2: Extend Deals Data (FactSet Batches 2000-2025)
#
# **Goal**: Load all 7 FactSet XLS batches and consolidate into a unified deals dataset.
# This extends coverage beyond the 2020 cutoff in dma_corpus_metadata.
# Schema is preserved to match existing pipeline expectations.

# %%
def load_extended_deals(config):
    """
    Load deals from two sources:
    1. dma_corpus_metadata_with_factset_id.csv (2000-2020, richer text data)
    2. FactSet XLS batches (2000-2025, for extending labels to 2021+)
    
    Returns a unified DataFrame with consistent schema.
    """
    import glob
    
    # --- Source 1: DMA Corpus (primary, has text excerpts) ---
    dma_path = resolve_path(config["inputs"]["deals"], prefer_repo=True)
    if dma_path and os.path.exists(dma_path):
        log(f"Loading DMA corpus from {dma_path}...")
        dma_df = pd.read_csv(dma_path, sep="|", low_memory=False)
        dma_df["source"] = "dma_corpus"
        log(f"  DMA corpus: {len(dma_df)} deals, years {dma_df['year'].min()}-{dma_df['year'].max()}")
    else:
        dma_df = pd.DataFrame()
        log("[WARN] DMA corpus not found.")
    
    # --- Source 2: FactSet XLS Batches (for 2021+ extension) ---
    xls_dir = resolve_path(config["inputs"]["factset_xls_dir"], prefer_repo=True)
    xls_dfs = []
    
    if xls_dir and os.path.isdir(xls_dir):
        log(f"Loading FactSet XLS batches from {xls_dir}...")
        xls_files = sorted(glob.glob(os.path.join(xls_dir, "*.xls")))
        
        for xls_file in xls_files:
            try:
                batch_df = pd.read_excel(xls_file)
                batch_df["source"] = os.path.basename(xls_file)
                xls_dfs.append(batch_df)
                log(f"  Loaded {os.path.basename(xls_file)}: {len(batch_df)} deals")
            except Exception as e:
                log(f"  [WARN] Failed to load {xls_file}: {e}")
        
        if xls_dfs:
            factset_df = pd.concat(xls_dfs, ignore_index=True)
            log(f"  Total FactSet XLS: {len(factset_df)} deals")
        else:
            factset_df = pd.DataFrame()
    else:
        factset_df = pd.DataFrame()
        log("[WARN] FactSet XLS directory not found.")
    
    # --- Standardize FactSet XLS schema to match DMA corpus ---
    if not factset_df.empty:
        # Debug: show actual column names from XLS files
        log(f"  FactSet columns: {list(factset_df.columns)[:10]}...")
        
        # Flexible column mapping (case-insensitive matching)
        col_lower_map = {c.lower(): c for c in factset_df.columns}
        
        # Find target column
        for key in ["target", "target name", "targetname", "target company"]:
            if key in col_lower_map:
                factset_df = factset_df.rename(columns={col_lower_map[key]: "target"})
                break
        
        # Find acquirer column
        for key in ["acquirer", "acquirer name", "acquirername", "buyer"]:
            if key in col_lower_map:
                factset_df = factset_df.rename(columns={col_lower_map[key]: "acquirer"})
                break
        
        # Find date column (for year extraction)
        date_col = None
        for key in ["announce date", "announced", "ann date", "announcement date", "date announced", "date"]:
            if key in col_lower_map:
                date_col = col_lower_map[key]
                break
        
        if date_col:
            factset_df["date_announcement"] = pd.to_datetime(factset_df[date_col], errors="coerce")
            factset_df["year"] = factset_df["date_announcement"].dt.year
            log(f"  Extracted year from '{date_col}', range: {factset_df['year'].min()}-{factset_df['year'].max()}")
        else:
            # Fallback: try to infer from any datetime column
            for col in factset_df.columns:
                try:
                    parsed = pd.to_datetime(factset_df[col], errors="coerce")
                    if parsed.notna().sum() > len(factset_df) * 0.5:  # >50% valid dates
                        factset_df["date_announcement"] = parsed
                        factset_df["year"] = parsed.dt.year
                        log(f"  Inferred year from '{col}', range: {factset_df['year'].min()}-{factset_df['year'].max()}")
                        break
                except:
                    pass
            else:
                log("  [WARN] Could not extract year from FactSet XLS - no date column found")
    
    # --- Merge: DMA corpus + NEW deals from FactSet (2021+) ---
    if not dma_df.empty and not factset_df.empty and "year" in factset_df.columns:
        # Only add FactSet deals that are AFTER DMA corpus coverage
        dma_max_year = dma_df["year"].max()
        new_deals = factset_df[factset_df["year"] > dma_max_year].copy()
        log(f"  Extending with {len(new_deals)} new deals from {dma_max_year+1}+")
        
        # Align columns (keep only columns present in DMA corpus)
        common_cols = [c for c in dma_df.columns if c in new_deals.columns]
        if common_cols:
            new_deals = new_deals[common_cols]
            combined_df = pd.concat([dma_df, new_deals], ignore_index=True)
        else:
            log("[WARN] No common columns between DMA and FactSet. Using DMA only.")
            combined_df = dma_df
    elif not dma_df.empty:
        combined_df = dma_df
    elif not factset_df.empty:
        combined_df = factset_df
    else:
        combined_df = pd.DataFrame()
        log("[ERROR] No deals data loaded!")
    
    # --- Final summary ---
    if not combined_df.empty:
        log(f"âœ… Extended deals dataset: {len(combined_df)} deals, years {combined_df['year'].min()}-{combined_df['year'].max()}")
        
        # Show company names if available
        if "target" in combined_df.columns:
            log(f"  Sample targets: {combined_df['target'].dropna().head(3).tolist()}")
    
    return combined_df

# Load extended deals
DEALS_DF = load_extended_deals(CONFIG)
log("âœ… Cell A2 Complete: Extended Deals Loaded.")

# %% [markdown]
# # Cell A3: Extend Fundamentals Data (Schema-Preserving)
#
# **Goal**: Extend `fundq_full.parquet` and `funda_full.parquet` using available CSV data.
# The existing parquet schema is treated as ground truth - new data must match exactly.

# %%
def extend_fundamentals(config, save_extended=False):
    """
    Extend fundq/funda parquet files with newer data from CSVs.
    Schema is determined by existing parquet - only matching columns are appended.
    
    Available extension sources:
    - compustat_funda_2000on.csv (annual fundamentals)
    
    Args:
        config: Pipeline config dict
        save_extended: If True, saves extended parquet back to Drive
    
    Returns:
        fundq_df, funda_df: Extended DataFrames (or originals if no extension possible)
    """
    results = {}
    
    for data_type, parquet_key, csv_candidates in [
        ("quarterly", "fundq", []),  # No quarterly CSV currently available
        ("annual", "funda", [config["inputs"].get("fundamentals_csv", "data/fundamentals/compustat_funda_2000on.csv")]),
    ]:
        parquet_file = config["inputs"][parquet_key]
        parquet_path = os.path.join(config["drive_dir"], parquet_file)
        
        log(f"\n--- {data_type.upper()} Fundamentals ({parquet_key}) ---")
        
        # 1. Load existing parquet and extract schema
        if os.path.exists(parquet_path):
            existing_df = pd.read_parquet(parquet_path)
            schema_cols = list(existing_df.columns)
            schema_dtypes = existing_df.dtypes.to_dict()
            
            log(f"Loaded {parquet_file}: {len(existing_df)} rows")
            log(f"Schema: {len(schema_cols)} columns")
            log(f"Date range: {existing_df['datadate'].min()} to {existing_df['datadate'].max()}")
            
            # Show sample companies if available
            if "conm" in existing_df.columns:
                log(f"Sample companies: {existing_df['conm'].dropna().head(3).tolist()}")
        else:
            log(f"[WARN] {parquet_file} not found at {parquet_path}")
            results[data_type] = pd.DataFrame()
            continue
        
        # 2. Look for extension CSVs
        extended_df = existing_df.copy()
        max_existing_date = pd.to_datetime(existing_df["datadate"]).max()
        
        for csv_name in csv_candidates:
            csv_path = resolve_path(csv_name, prefer_repo=True)
            if not csv_path or not os.path.exists(csv_path):
                continue
            
            log(f"Checking extension source: {csv_name}...")
            new_df = pd.read_csv(csv_path, low_memory=False)
            new_df["datadate"] = pd.to_datetime(new_df["datadate"], errors="coerce")
            
            # Only keep rows AFTER existing data
            new_df = new_df[new_df["datadate"] > max_existing_date]
            
            if new_df.empty:
                log(f"  No new rows after {max_existing_date.date()}")
                continue
            
            log(f"  Found {len(new_df)} new rows after {max_existing_date.date()}")
            
            # 3. Align to existing schema (drop extra cols, add missing as NaN)
            common_cols = [c for c in schema_cols if c in new_df.columns]
            missing_cols = [c for c in schema_cols if c not in new_df.columns]
            
            log(f"  Common columns: {len(common_cols)}/{len(schema_cols)}")
            if missing_cols:
                log(f"  Missing columns (will be NaN): {missing_cols[:5]}{'...' if len(missing_cols) > 5 else ''}")
            
            # Subset to common columns, add missing as NaN
            new_df_aligned = new_df[common_cols].copy()
            for col in missing_cols:
                new_df_aligned[col] = np.nan
            
            # Reorder to match schema
            new_df_aligned = new_df_aligned[schema_cols]
            
            # Cast dtypes to match
            for col, dtype in schema_dtypes.items():
                try:
                    new_df_aligned[col] = new_df_aligned[col].astype(dtype)
                except (ValueError, TypeError):
                    pass  # Keep as-is if cast fails
            
            # 4. Append
            extended_df = pd.concat([extended_df, new_df_aligned], ignore_index=True)
            log(f"  Extended to {len(extended_df)} total rows")
        
        # 5. Optionally save
        if save_extended and len(extended_df) > len(existing_df):
            save_path = parquet_path.replace(".parquet", "_extended.parquet")
            extended_df.to_parquet(save_path, index=False)
            log(f"  Saved extended data to {save_path}")
        
        results[data_type] = extended_df
    
    return results.get("quarterly", pd.DataFrame()), results.get("annual", pd.DataFrame())

# Run extension check (don't save by default - just report)
FUNDQ_EXTENDED, FUNDA_EXTENDED = extend_fundamentals(CONFIG, save_extended=False)
log("âœ… Cell A3 Complete: Fundamentals Extension Check.")

# %% [markdown]
# # Cell B: Load & Clean Panel (Truncation Diagnostics)
#
# **Goal**: Load Compustat data, apply 'STD' filter, and diagnose year coverage to ensure no data is silently dropped.


# %%
def load_and_prep_compustat(config):
    # Large parquet files -> Google Drive only (not on GitHub)
    fundq_path = os.path.join(config["drive_dir"], config["inputs"]["fundq"])
    funda_path = os.path.join(config["drive_dir"], config["inputs"]["funda"])

    # Load
    log(f"Loading {fundq_path}...")
    if os.path.exists(fundq_path):
        q_df = pd.read_parquet(fundq_path)
        q_df["freq"] = "Q"
        log(f"Loaded Quarterly: {len(q_df)} rows. Date Range: {q_df['datadate'].min()} to {q_df['datadate'].max()}")
    else:
        q_df = pd.DataFrame()
        log("[WARN] Quarterly data missing.")

    log(f"Loading {funda_path}...")
    if os.path.exists(funda_path):
        a_df = pd.read_parquet(funda_path)
        a_df["freq"] = "A"
        log(f"Loaded Annual: {len(a_df)} rows. Date Range: {a_df['datadate'].min()} to {a_df['datadate'].max()}")
    else:
        a_df = pd.DataFrame()
        log("[WARN] Annual data missing.")

    df = pd.concat([q_df, a_df], ignore_index=True, sort=False)
    df["datadate"] = pd.to_datetime(df["datadate"], errors="coerce")
    
    # --- DIAGNOSTIC 1: Raw Year Counts ---
    df["year"] = df["datadate"].dt.year
    log("Raw Data Year Counts (Top 5 recent):")
    print(df["year"].value_counts().sort_index(ascending=False).head(5))

    # --- FILTER: Keyset (STD) ---
    keyset_col = next((c for c in df.columns if c.lower() == "keyset"), None)
    if keyset_col:
        # Check unique keysets before filtering
        log(f"Unique keysets found: {df[keyset_col].unique()}")
        
        before_len = len(df)
        target_keyset = config["filters"]["keyset"]
        
        # Keep STD, Drop PRE/PFO/etc. explicitly if needed, but usually strictly STD is safe
        df = df[df[keyset_col].astype(str).str.upper() == target_keyset].copy()
        
        dropped = before_len - len(df)
        log(f"Filtered for {keyset_col}={target_keyset}. Dropped {dropped} rows ({dropped/before_len:.1%}).")
        
        # --- DIAGNOSTIC 2: Post-Filter Year Counts ---
        log("Post-Filter Year Counts (Top 5 recent):")
        print(df["year"].value_counts().sort_index(ascending=False).head(5))
    else:
        log("[WARN] 'keyset' column not found. Skipping STD filter.")

    # --- PIT Risk Flag (AJEX/CFAC) Before Scrubbing ---
    log("Computing PIT Risk Flag...")
    adj_cols = [c for c in ["ajexq", "ajex", "cfacshr", "cfacpr"] if c in df.columns]
    if adj_cols:
        # Ensure numeric
        for c in adj_cols:
            df[c] = pd.to_numeric(df[c], errors="coerce")
            
        # Sort by Entity-Time
        df = df.sort_values(["gvkey", "datadate"]).reset_index(drop=True)
        
        same_firm = df["gvkey"] == df["gvkey"].shift(1)
        has_change = pd.Series(False, index=df.index)
        
        for c in adj_cols:
            curr = df[c].fillna(-9999)
            prev = df[c].shift(1).fillna(-9999)
            has_change |= ((curr != prev) & same_firm)
            
        # Cumsum: Once risky, always risky? Or just localized? 
        # Requirement: "Detect ANY change... Flag potential back-adjustment"
        # Usually we flag the whole firm history if unstable, or forward from change.
        # Here we flag forward from first change to be safe (conservative).
        
        # Use has_change directly - simpler and avoids NA issues
        df["_raw_change"] = has_change.fillna(False).astype(int)
        df["price_pit_risk_flag"] = df.groupby("gvkey")["_raw_change"].transform("cumsum").fillna(0).gt(0).astype(int)
        df.drop(columns=["_raw_change"], inplace=True)
    else:
        df["price_pit_risk_flag"] = 0

    # --- Scrub Columns ---
    # (Simple scrub of AJEX/CFAC to prevent leakage)
    drop_patterns = ["ajex", "cfac", "adjex"]
    to_drop = [c for c in df.columns if any(p in c.lower() for p in drop_patterns) and c != "price_pit_risk_flag"]
    if to_drop:
        df.drop(columns=to_drop, inplace=True)
        log(f"Scrubbed {len(to_drop)} adjustment columns.")

    # --- Snapshot Definition ---
    # Max(datadate + 90, rdq)
    if "rdq" in df.columns:
        df["rdq"] = pd.to_datetime(df["rdq"], errors="coerce")
        base_snap = df["datadate"] + pd.Timedelta(days=config["safe_lag_days"])
        df["snapshot_ts"] = base_snap
        
        # Update where rdq is valid and later
        mask_rdq = df["rdq"].notna()
        # elementwise max
        df.loc[mask_rdq, "snapshot_ts"] = pd.to_datetime(np.maximum(
            base_snap[mask_rdq].values, 
            df.loc[mask_rdq, "rdq"].values
        ))
    else:
        df["snapshot_ts"] = df["datadate"] + pd.Timedelta(days=config["safe_lag_days"])

    return df

# Check for checkpoint before running expensive computation
if checkpoint_exists("panel_clean"):
    panel_clean = load_checkpoint("panel_clean")
else:
    panel_clean = load_and_prep_compustat(CONFIG)
    save_checkpoint(panel_clean, "panel_clean")

log(f"âœ… Cell B Complete. Panel has {len(panel_clean)} rows.")

# %% [markdown]
# # Cell C: Load & Normalize Deals
#
# **Goal**: Load crude deal data, normalize columns, and handle date parsing robustly.

# %%
import csv

def load_deals_robust(config, preloaded_df=None):
    if preloaded_df is not None and not preloaded_df.empty:
        log(f"Using preloaded deals dataframe ({len(preloaded_df)} rows)...")
        df = preloaded_df.copy()
        
        # Ensure date_announcement is mapped to ann_date if not already
        if "date_announcement" in df.columns and "ann_date" not in df.columns:
            df["ann_date"] = df["date_announcement"]
            
    else:
        # Fallback: Load from CSV
        csv_path = resolve_path(config["inputs"]["deals"])
        if not csv_path or not os.path.exists(csv_path):
            log(f"[ERROR] Deal CSV not found.")
            return pd.DataFrame()

        # Sniff delimiter
        with open(csv_path, "r", encoding="utf-8", errors="replace") as f:
            sample = f.read(2048)
            try:
                dialect = csv.Sniffer().sniff(sample)
                sep = dialect.delimiter
            except:
                sep = ","
        
        # Load
        log(f"Loading deals with separator='{sep}'...")
        df = pd.read_csv(csv_path, sep=sep, engine="python")
    
    # Normalize Cols
    col_map = {c: c.lower().strip().replace(" ", "").replace("_", "") for c in df.columns}
    rev_map = {v: k for k, v in col_map.items()} # normalized -> original
    
    # Rename known targets
    # Expecting: 'cik', 'dateannouncement', 'dealvalue'
    rename_dict = {}
    if "dateannouncement" in rev_map: rename_dict[rev_map["dateannouncement"]] = "ann_date"
    if "cik" in rev_map: rename_dict[rev_map["cik"]] = "cik"
    
    # Find value col
    val_candidates = ["dealvalue", "transactionvalue"]
    val_orig = next((rev_map[v] for v in val_candidates if v in rev_map), None)
    if val_orig:
        rename_dict[val_orig] = "_deal_value_str"
    
    df.rename(columns=rename_dict, inplace=True)
    
    # CIK extraction if url exists but cik col doesn't (fallback)
    if "cik" not in df.columns and "url" in df.columns:
        df["cik"] = df["url"].astype(str).str.extract(r'/data/(\d{1,10})/')
        
    # Clean CIK
    df["cik"] = pd.to_numeric(df["cik"], errors="coerce").astype("Int64")
    df.dropna(subset=["cik"], inplace=True)
    
    # Clean Date
    df["ann_date"] = pd.to_datetime(df["ann_date"], errors="coerce")
    df.dropna(subset=["ann_date"], inplace=True)
    
    # Clean Value
    if "_deal_value_str" in df.columns:
        df["_deal_value_num"] = pd.to_numeric(
            df["_deal_value_str"].astype(str).str.replace(r"[^0-9.]", "", regex=True),
            errors="coerce"
        )
        df["_deal_value_log"] = np.log1p(df["_deal_value_num"].fillna(0))
    else:
        df["_deal_value_num"] = np.nan
        df["_deal_value_log"] = 0.0
        
    # Select final
    out = df[["cik", "ann_date", "_deal_value_num", "_deal_value_log"]].copy()
    out = out.sort_values("ann_date").reset_index(drop=True)
    return out

deals_df = load_deals_robust(CONFIG, preloaded_df=DEALS_DF)

# Save
deals_path = os.path.join(ARTIFACT_DIR, "deals.parquet")
deals_df.to_parquet(deals_path, index=False)
log(f"âœ… Cell C Complete. Loaded {len(deals_df)} deals. Saved to {deals_path}")

# %% [markdown]
# # Cell D: Feature Engineering (Vectorized)
#
# **Goal**: Create financial ratios, growth rates, and return features using vectorized operations.

# %%
from tqdm.auto import tqdm

def feature_engineering(df):
    log("Starting Vectorized Feature Engineering...")
    
    # 1. Base Var Coalescing (bfill optimized)
    base_vars = {
        "assets":        ["atq", "at"],
        "revenue":       ["saleq", "sale"],
        "net_income":    ["niq", "ni"],
        "debt_longterm": ["dlttq", "dltt"],
        "equity_book":   ["ceqq", "ceq"],
        "oibdp":         ["oibdpq", "oibdp"],
        "act":           ["actq", "act"],
        "lct":           ["lctq", "lct"],
        "che":           ["cheq", "che"],
        "xint":          ["xintq", "xint"],
        "xrd":           ["xrdq", "xrd"],
        "xsga":          ["xsgaq", "xsga"],
        "capx":          ["capxq", "capx"],
        "oancf":         ["oancfq", "oancf"],
        "csho":          ["cshoq", "csho"],
        "cshtrq":        ["cshtrq", "cshtr"],
        "ppent":         ["ppentq", "ppent"],
        "mkvalt":        ["mkvaltq", "mkvalt"],
    }
    
    for var, cands in tqdm(base_vars.items(), desc="Coalescing Base Vars", leave=False):
        existing = [c for c in cands if c in df.columns]
        if not existing:
            df[var] = np.nan
        elif len(existing) == 1:
            df[var] = df[existing[0]]
        else:
            df[var] = df[existing].bfill(axis=1).iloc[:, 0]

    # 2. Ratios
    EPS = 1e-9
    def safe_mag(s): return s.fillna(0).abs() + EPS
    
    df["profit_margin"] = df["net_income"] / safe_mag(df["revenue"])
    df["roa"]           = df["net_income"] / safe_mag(df["assets"])
    df["oper_margin"]   = df["oibdp"] / safe_mag(df["revenue"])
    df["leverage"]      = df["debt_longterm"] / safe_mag(df["assets"])
    df["curr_ratio"]    = df["act"] / safe_mag(df["lct"])
    df["cash_ratio"]    = df["che"] / safe_mag(df["assets"])
    df["int_coverage"]  = df["oibdp"] / safe_mag(df["xint"])
    
    df["rd_int"]   = df["xrd"]   / safe_mag(df["revenue"])
    df["sgna_int"] = df["xsga"]  / safe_mag(df["revenue"])
    df["capx_int"] = df["capx"]  / safe_mag(df["assets"])
    df["ocf_int"]  = df["oancf"] / safe_mag(df["assets"])
    
    # Clip Ratios (Outlier protection)
    ratio_cols = ["profit_margin", "roa", "oper_margin", "leverage", "curr_ratio",
                  "cash_ratio", "int_coverage", "rd_int", "sgna_int", "capx_int", "ocf_int"]
    df[ratio_cols] = df[ratio_cols].fillna(0).clip(-5, 5)

    # 3. Size & Turnover
    df["log_csho"] = np.log1p(df["csho"].fillna(0).clip(lower=0))
    df["turnover"] = df["cshtrq"] / safe_mag(df["csho"])
    df["log_turnover"] = np.log1p(df["turnover"].fillna(0).clip(lower=0))

    # 4. Returns (fwd/back)
    # Strict Sort required for shift
    df = df.sort_values(["gvkey", "datadate"]).reset_index(drop=True)
    
    # --- Returns (1Q) ---
    # Need to be careful with 'prccq'. 
    if "prccq" in df.columns:
        # Backward 1Q (for momentum/correlation)
        # Shift 1, check firm match
        prev_price = df.groupby("gvkey")["prccq"].shift(1)
        df["ret_back_1q"] = (df["prccq"] / prev_price - 1.0)
        
        # Forward 1Q (for target/PnL)
        # Shift -1, check firm match
        next_price = df.groupby("gvkey")["prccq"].shift(-1)
        df["ret_fwd_1q"] = (next_price / df["prccq"] - 1.0)
    else:
        df["ret_back_1q"] = np.nan
        df["ret_fwd_1q"] = np.nan

    # 5. Growth Rates (Global Shift Masked)
    log_growth_inputs = ["assets", "revenue", "ppent", "mkvalt"]
    input_cols = []
    for raw in log_growth_inputs:
        col = f"log_{raw}"
        df[col] = np.log1p(df[raw].fillna(0).clip(lower=0))
        input_cols.append(col)
        
    # Shifts
    # Note: df is already sorted by gvkey, datadate
    gvkey = df["gvkey"]
    mask_1 = (gvkey == gvkey.shift(1))
    mask_4 = (gvkey == gvkey.shift(4))
    
    df_shift_1 = df[input_cols].shift(1)
    df_shift_4 = df[input_cols].shift(4)
    
    is_q = (df["freq"] == "Q")
    
    for i, raw in enumerate(log_growth_inputs):
        col = input_cols[i]
        
        # QoQ
        prev_1 = df_shift_1[col].where(mask_1)
        df[f"dlog_{raw}_qoq"] = np.where(is_q, df[col] - prev_1, np.nan)
        
        # YoY
        prev_4 = df_shift_4[col].where(mask_4)
        prev_1_annual = df_shift_1[col].where(mask_1) # Fallback for annual data
        yoy_prev = np.where(is_q, prev_4, prev_1_annual)
        
        df[f"dlog_{raw}_yoy"] = df[col] - yoy_prev

    return df

# Run
features_df = feature_engineering(panel_clean.copy())

# Save
feat_path = os.path.join(ARTIFACT_DIR, "features_panel.parquet")
features_df.to_parquet(feat_path, index=False)

# Save feature list
numeric_cols = features_df.select_dtypes(include=np.number).columns.tolist()
exclude = {"gvkey", "cik", "year", "price_pit_risk_flag", "ret_fwd_1q"}
final_feats = [c for c in numeric_cols if c not in exclude and not c.startswith("label")]

with open(os.path.join(ARTIFACT_DIR, "feature_list.json"), "w") as f:
    json.dump(final_feats, f)

log(f"âœ… Cell D Complete. Saved features to {feat_path}")

# %% [markdown]
# # Cell E: Multi-Horizon Labeling (Vectorized)
#
# **Goal**: Generate binary targets for horizons [3m, 6m, ..., 24m] using efficient merge_asof.

# %%
def label_panel_multi_horizon(panel_df, deals_df, horizons_months):
    log("Starting Multi-Horizon Labeling...")
    
    # 1. Prepare Tables
    # Panel side
    # Clean CIK
    panel_df["cik"] = pd.to_numeric(panel_df["cik"], errors="coerce").fillna(-1).astype("Int64")
    # Must have snapshot_ts
    panel = panel_df.dropna(subset=["snapshot_ts"]).copy()
    panel = panel.sort_values("snapshot_ts").reset_index(drop=True)
    
    # Deal side
    events = deals_df[["cik", "ann_date"]].copy()
    events["cik"] = pd.to_numeric(events["cik"], errors="coerce").astype("Int64")
    events = events.dropna().sort_values("ann_date").reset_index(drop=True)
    
    # 2. Iterate Horizons
    for h_mo in horizons_months:
        h_days = int(h_mo * 30.5)
        col_name = f"label_deal_0_{h_mo}m"
        
        # We want: Event in (snapshot, snapshot + h_days]
        # merge_asof forward lookahead
        # "Find the nearest event AFTER snapshot"
        
        merged = pd.merge_asof(
            panel[["cik", "snapshot_ts"]], # Left
            events,                        # Right
            left_on="snapshot_ts",
            right_on="ann_date",
            by="cik",
            direction="forward",
            tolerance=pd.Timedelta(days=h_days)
        )
        
        # If match found, and it's strictly > snapshot (merge_asof forward >=, but strict > is safer for lookahead)
        # Actually merge_asof direction='forward' matches closest >= value. 
        # So event_date >= snapshot_ts.
        # We usually want strict future: event_date > snapshot_ts.
        
        # tolerance limits dist to h_days. 
        # So we have snapshot_ts <= ann_date <= snapshot_ts + tolerance
        
        # Check strict inequality
        is_future = merged["ann_date"] > merged["snapshot_ts"]
        panel[col_name] = (merged["ann_date"].notna() & is_future).astype(int)
        
        pos_count = panel[col_name].sum()
        log(f"Horizon {h_mo}m ({h_days}d): {pos_count} positives ({pos_count/len(panel):.2%})")

    return panel

# Run
labeled_panel = label_panel_multi_horizon(features_df, deals_df, CONFIG["horizons_months"])

# Save
label_path = os.path.join(ARTIFACT_DIR, "labeled_panel.parquet")
labeled_panel.to_parquet(label_path, index=False)

# Coverage Report
cov_report = labeled_panel.groupby("year")[[f"label_deal_0_{m}m" for m in CONFIG["horizons_months"]]].sum()
cov_path = os.path.join(ARTIFACT_DIR, "label_coverage.csv")
cov_report.to_csv(cov_path)

log(f"âœ… Cell E Complete. Saved labeled panel to {label_path}")
print("\nLabel Coverage by Year:")
print(cov_report.tail(5))

# %% [markdown]
# # Cell F: Training Dry-Run Harness (Stability)
#
# **Goal**: Verify model stability by running multiple trials on a single holdout quarter. Measure Jaccard overlap of top predictions.

# %%
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import roc_auc_score, brier_score_loss, precision_recall_curve, auc

def dry_run_harness(df, target_col, n_trials=5, top_k=50):
    log(f"Running Dry-Run Harness for target: {target_col}")
    
    # 1. Select Holdout Quarter (recent but with labels)
    # Pick the last quarter that has at least 50 positives to be decent
    # Or just hardcode a known good quarter e.g. 2020Q4 if available
    valid_q = df.groupby("panel_q")[target_col].sum()
    candidates = valid_q[valid_q > 10].index.sort_values(ascending=False)
    
    if len(candidates) > 4:
        holdout_q = candidates[4] # Go back a bit to ensure data settlement
    else:
        holdout_q = candidates[0]
        
    log(f"Selected Holdout Quarter: {holdout_q}")
    
    train_mask = df["panel_q"] < holdout_q
    test_mask = df["panel_q"] == holdout_q
    
    X_cols = [c for c in df.columns if c in final_feats]
    
    train_df = df[train_mask].copy()
    test_df = df[test_mask].copy()
    
    log(f"Train size: {len(train_df)}, Test size: {len(test_df)}")
    
    results = []
    top_k_sets = []
    
    for i in range(n_trials):
        # Vary seed
        seed = 42 + i
        
        # Simple Downsampling of negatives for speed/balance
        pos = train_df[train_df[target_col]==1]
        neg = train_df[train_df[target_col]==0].sample(n=len(pos)*10, random_state=seed) # 1:10 ratio
        train_sub = pd.concat([pos, neg]).sample(frac=1, random_state=seed)
        
        clf = HistGradientBoostingClassifier(
            learning_rate=0.05, 
            max_iter=100, 
            max_depth=5,
            random_state=seed
        )
        
        clf.fit(train_sub[X_cols], train_sub[target_col])
        probs = clf.predict_proba(test_df[X_cols])[:, 1]
        
        # Metrics
        roc = roc_auc_score(test_df[target_col], probs)
        precision, recall, _ = precision_recall_curve(test_df[target_col], probs)
        pr_auc = auc(recall, precision)
        
        # Top K
        top_k_idx = np.argsort(probs)[-top_k:]
        top_k_ciks = test_df.iloc[top_k_idx]["cik"].values
        top_k_sets.append(set(top_k_ciks))
        
        results.append({
            "trial": i,
            "seed": seed,
            "auc": roc,
            "pr_auc": pr_auc
        })
        
    # Stability: Jaccard
    jaccards = []
    for i in range(len(top_k_sets)):
        for j in range(i+1, len(top_k_sets)):
            s1, s2 = top_k_sets[i], top_k_sets[j]
            jaccards.append(len(s1 & s2) / len(s1 | s2))
            
    mean_jaccard = np.mean(jaccards)
    log(f"Dry Run Complete. Mean AUC: {pd.DataFrame(results)['auc'].mean():.3f}")
    log(f"Stability (Mean Jaccard of Top {top_k}): {mean_jaccard:.2%}")
    
    return pd.DataFrame(results), mean_jaccard

# Prepare quarter column
features_df["panel_q"] = features_df["datadate"].dt.to_period("Q")

# Run Dry Run
dry_res, stability_score = dry_run_harness(features_df, "label_deal_0_3m", n_trials=CONFIG["calibration"]["n_dryrun_trials"])

# Save
dry_path = os.path.join(ARTIFACT_DIR, "dryrun_trials.parquet")
dry_res.to_parquet(dry_path)
log(f"âœ… Cell F Complete. Stability Score: {stability_score:.2f}")

# %% [markdown]
# # Cell G: Full Backtest & Baselines + Visuals
#
# **Goal**: Expanding window backtest comparisons vs S&P 500.

# %%
# 1. Basics
TARGET_HORIZON = "label_deal_0_3m"

# 2. S&P 500 Baseline (yfinance)
log("Fetching S&P 500 (SPY) data...")
start_date = features_df["datadate"].min().strftime("%Y-%m-%d")
end_date = datetime.now().strftime("%Y-%m-%d")

try:
    spy = yf.download("SPY", start=start_date, end=end_date, progress=False)
    # Resample to Quarterly Returns
    spy_q = spy["Close"].resample("Q").last().pct_change()
    # Align to panel quarters
    spy_q.index = spy_q.index.to_period("Q")
    spy_map = spy_q.to_dict()
    log(f"Loaded SPY data. Quarters covered: {len(spy_map)}")
except Exception as e:
    log(f"[WARN] Failed to load SPY: {e}. Using dummy baseline.")
    spy_map = {}

# 3. Expanding Window Loop
quarters = sorted(features_df["panel_q"].unique())
start_idx = 10 # Burn-in
results_backtest = []

X_cols = [c for c in features_df.columns if c in final_feats]

log(f"Starting Backtest over {len(quarters)-start_idx} quarters...")

for q_idx in range(start_idx, len(quarters)):
    q = quarters[q_idx]
    
    # Train window: < q
    train_mask = features_df["panel_q"] < q
    test_mask = features_df["panel_q"] == q
    
    train = features_df[train_mask]
    test = features_df[test_mask]
    
    if len(test) == 0: continue
    if train[TARGET_HORIZON].sum() < 10: continue # Skip if no training signal
    
    # Downsample train
    pos = train[train[TARGET_HORIZON]==1]
    neg = train[train[TARGET_HORIZON]==0].sample(n=len(pos)*10, random_state=42)
    train_bal = pd.concat([pos, neg])
    
    # Train
    clf = HistGradientBoostingClassifier(learning_rate=0.05, max_depth=4, random_state=42)
    clf.fit(train_bal[X_cols], train_bal[TARGET_HORIZON])
    
    # Predict
    probs = clf.predict_proba(test[X_cols])[:, 1]
    
    # Evaluate Strategy
    # Select Top 50
    top_50_idx = np.argsort(probs)[-50:]
    selected = test.iloc[top_50_idx].copy()
    
    # Returns (fwd 1q)
    port_ret = selected["ret_fwd_1q"].clip(-0.5, 1.0).mean() # Clip outliers
    
    # Baselines
    univ_ret = test["ret_fwd_1q"].clip(-0.5, 1.0).mean()
    spy_ret = spy_map.get(q, np.nan)
    if isinstance(spy_ret, pd.Series): spy_ret = spy_ret.item() # Handle yfinance multi-index quirk
    
    # Stats
    try:
        auc_score = roc_auc_score(test[TARGET_HORIZON], probs)
    except:
        auc_score = 0.5
        
    results_backtest.append({
        "quarter": str(q),
        "auc": auc_score,
        "port_ret": port_ret,
        "univ_ret": univ_ret,
        "spy_ret": spy_ret,
        "excess_vs_spy": (port_ret - spy_ret) if pd.notna(spy_ret) else np.nan
    })
    
    if q_idx % 4 == 0:
        log(f"Q {q}: AUC={auc_score:.2f}, Port={port_ret:.1%}, SPY={spy_ret:.1%}")

res_df = pd.DataFrame(results_backtest)
res_path = os.path.join(ARTIFACT_DIR, "results_backtest.parquet")
res_df.to_parquet(res_path)

# 4. Visuals (Equity Curve)
# Fillna with 0 for plotting
res_df_plot = res_df.fillna(0)
res_df_plot["cum_port"] = (1 + res_df_plot["port_ret"]).cumprod()
res_df_plot["cum_spy"] = (1 + res_df_plot["spy_ret"]).cumprod()

plt.figure(figsize=(10, 6))
plt.plot(res_df_plot["quarter"], res_df_plot["cum_port"], label="Model Strategy", linewidth=2.5)
plt.plot(res_df_plot["quarter"], res_df_plot["cum_spy"], label="S&P 500", linestyle="--", color="gray")
plt.title("Cumulative Wealth: Model vs S&P 500")
plt.grid(True, alpha=0.3)
plt.legend()
plt.xticks(rotation=45)
plt.show()

log("âœ… Cell G Complete. Backtest finished.")
log("Pipeline Complete.")

# Calibration Footer (Required by Guidelines)
print("\n" + "="*60)
print("Persona Used: Senior Quant ML Engineer")
print("Difficulty Tuning: ~70% ZPD (Modular Pipeline), ~20% Staff+ (Stability Harness)")
print("Concrete Suggestion: Run the 'Dry Run' cell with n_trials=20 to get statistically significant stability metrics.")
print("="*60)

# %% [markdown]
# # Cell H: Event Study Verification (Jump Plot)
#
# **Goal**: Validate data alignment by plotting normalized price around M&A announcements.

# %%
def run_event_study(panel_df, deals_df, min_deal_value=100):
    """
    High-Precision Event Study: Price vs. Announcement Date.
    Filters for major deals (> $100M) to ensure clear signal.
    """
    log("Running Event Study Verification...")
    
    if "_deal_value_num" not in deals_df.columns:
        log("[SKIP] _deal_value_num missing in deals. Cannot run event study.")
        return
    
    # Filter for Major Deals
    precise_events = deals_df[deals_df["_deal_value_num"] > min_deal_value][["cik", "ann_date"]].copy()
    precise_events = precise_events.rename(columns={"ann_date": "event_date"})
    
    if "prccq" not in panel_df.columns:
        log("[SKIP] prccq missing in panel. Cannot run event study.")
        return
    
    price_history = panel_df[["cik", "datadate", "prccq"]].dropna().copy()
    
    # Merge
    merged = pd.merge(price_history, precise_events, on="cik", how="inner")
    merged["days_rel"] = (merged["datadate"] - merged["event_date"]).dt.days
    window = merged[(merged["days_rel"] >= -365) & (merged["days_rel"] <= 365)].copy()
    
    if window.empty:
        log("[WARN] No matching price/deal pairs for event study.")
        return
    
    def normalize_precise(g):
        baseline = g[(g["days_rel"] >= -90) & (g["days_rel"] <= -10)]
        if not baseline.empty:
            base_price = baseline.sort_values("days_rel", ascending=False).iloc[0]["prccq"]
            if base_price > 0:
                g["norm_price"] = g["prccq"] / base_price
                return g
        return None
    
    norm_df = window.groupby(["cik", "event_date"]).apply(normalize_precise)
    
    if norm_df is None or norm_df.empty:
        log("[WARN] Not enough data for event study normalization.")
        return
    
    norm_df["week_rel"] = (norm_df["days_rel"] / 7).round().astype(int)
    stats = norm_df.groupby("week_rel")["norm_price"].quantile([0.25, 0.50, 0.75]).unstack()
    
    # Plot
    plt.figure(figsize=(12, 7))
    plt.plot(stats.index, stats[0.50], color="crimson", linewidth=3, label="Median Stock Price")
    plt.fill_between(stats.index, stats[0.25], stats[0.75], color="crimson", alpha=0.1, label="IQR (25-75%)")
    plt.axvline(0, color="black", linestyle="--", linewidth=2, label="Announcement Day")
    plt.axhline(1.0, color="gray", linestyle=":", alpha=0.5)
    plt.title("Event Study: Price vs. M&A Announcement", fontsize=14)
    plt.xlabel("Weeks Relative to Announcement")
    plt.ylabel("Normalized Price (1.0 = Pre-Deal)")
    plt.xlim(-20, 20)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(ARTIFACT_DIR, "event_study_plot.png"), dpi=150)
    plt.show()
    
    # Verdict
    try:
        pre = stats.loc[-2, 0.50] if -2 in stats.index else 1.0
        post = stats.loc[2, 0.50] if 2 in stats.index else 1.0
        jump = post / pre - 1
        log(f"Median Jump (-2wk to +2wk): {jump:.1%}")
        if jump > 0.15:
            log("âœ… VERDICT: Sharp Step-Function Detected. Data is aligned.")
        else:
            log("âš ï¸ VERDICT: Signal muted. Check for rumor leak or deal mix.")
    except Exception as e:
        log(f"[WARN] Could not compute jump: {e}")

# Run Event Study
run_event_study(labeled_panel, deals_df)
log("âœ… Cell H Complete: Event Study.")

# %% [markdown]
# # Cell I: Correlation Screening & Advanced Portfolio Construction
#
# **Goal**: Build correlation matrix on trailing returns and select portfolio with correlation caps.

# %%
# Hyperparameters
DEAL_PREMIUM_EST = 0.25   # Assumed median deal premium (quarter-scale)
COST_PER_TRADE = 0.003    # 30 bps per roundtrip
NET_ALPHA_MIN = 0.0       # Threshold for net_alpha > 0
K_MAX = 50                # Hard cap on bets
K_MIN = 10                # Minimum bets
CORR_LAG_Q = 8            # Trailing quarters for correlation
RHO_MAX = 0.80            # Max allowed |corr| between selected names
N_SIMS_RANDOM = 200       # Random baseline simulations

def clip_returns(s, lower=-0.8, upper=0.8):
    """Clip extreme returns."""
    return s.replace([np.inf, -np.inf], np.nan).clip(lower=lower, upper=upper)

def build_corr_matrix(panel_df, candidate_ciks, holdout_qtr, lag_q=CORR_LAG_Q):
    """
    Build cross-sectional correlation matrix of trailing backward returns.
    """
    if lag_q <= 0 or len(candidate_ciks) < 2:
        return None
    
    panel_df["panel_q"] = panel_df["datadate"].dt.to_period("Q")
    q_min = holdout_qtr - lag_q
    mask = (panel_df["panel_q"] >= q_min) & (panel_df["panel_q"] < holdout_qtr)
    
    df_hist = panel_df.loc[mask, ["datadate", "cik", "ret_back_1q"]].copy()
    df_hist = df_hist.dropna(subset=["ret_back_1q"])
    df_hist = df_hist[df_hist["cik"].isin(candidate_ciks)]
    
    if df_hist["cik"].nunique() < 2:
        return None
    
    # Aggregate duplicates
    df_hist = df_hist.groupby(["datadate", "cik"], as_index=False)["ret_back_1q"].mean()
    
    try:
        pivot = df_hist.pivot(index="datadate", columns="cik", values="ret_back_1q")
    except ValueError:
        return None
    
    if pivot.shape[1] < 2 or pivot.shape[0] < 2:
        return None
    
    corr = pivot.corr().replace([np.inf, -np.inf], np.nan).fillna(0.0)
    return corr

def select_with_corr_screen(candidates, corr_mat, k_max=K_MAX, k_min=K_MIN, rho_max=RHO_MAX):
    """
    Greedy selection with correlation cap.
    """
    if candidates.empty:
        return candidates.copy()
    
    selected_rows = []
    selected_ciks = []
    use_corr = corr_mat is not None and corr_mat.shape[0] >= 2
    
    for _, row in candidates.iterrows():
        if len(selected_rows) >= k_max:
            break
        
        net_a = row.get("net_alpha", 0.0)
        if net_a <= NET_ALPHA_MIN and len(selected_rows) >= k_min:
            break
        
        cik_i = row["cik"]
        
        # Correlation screen
        if use_corr and selected_ciks and (cik_i in corr_mat.index):
            valid_sel = [c for c in selected_ciks if c in corr_mat.columns]
            if valid_sel:
                max_abs = np.abs(corr_mat.loc[cik_i, valid_sel]).max()
                if max_abs > rho_max:
                    continue
        
        selected_rows.append(row)
        selected_ciks.append(cik_i)
    
    return pd.DataFrame(selected_rows) if selected_rows else candidates.head(0)

def simulate_random_portfolios(ret_series, K, n_sims=N_SIMS_RANDOM, seed=123):
    """Simulate equal-weight random K-name portfolios."""
    rng = np.random.default_rng(seed)
    ret_clean = clip_returns(ret_series).dropna()
    n = len(ret_clean)
    if n == 0 or n < K or K <= 0:
        return None
    
    vals = ret_clean.to_numpy()
    return np.array([vals[rng.choice(n, size=K, replace=False)].mean() for _ in range(n_sims)])

log("âœ… Cell I Complete: Correlation Screening Functions Defined.")

# %% [markdown]
# # Cell J: Quarter Deep-Dive Reports (Best/Worst/Median)
#
# **Goal**: Generate detailed reports for selected quarters.

# %%
def generate_quarter_report(panel_df, quarter, target_col, probs, artifact_dir):
    """
    Generate a detailed report for a specific quarter.
    """
    q_mask = panel_df["panel_q"] == quarter
    df_q = panel_df[q_mask].copy()
    df_q["prob_acq"] = probs
    
    # Top 50 bets
    top_50 = df_q.nlargest(50, "prob_acq")
    
    # Calculate hit rate
    hit_rate = top_50[target_col].mean() if target_col in top_50.columns else np.nan
    port_ret = clip_returns(top_50["ret_fwd_1q"]).mean() if "ret_fwd_1q" in top_50.columns else np.nan
    
    report = {
        "quarter": str(quarter),
        "n_firms": len(df_q),
        "n_positives": int(df_q[target_col].sum()) if target_col in df_q.columns else 0,
        "hit_rate_top50": float(hit_rate) if not np.isnan(hit_rate) else None,
        "port_ret_top50": float(port_ret) if not np.isnan(port_ret) else None,
        "top_10_ciks": top_50["cik"].head(10).tolist(),
        "top_10_probs": top_50["prob_acq"].head(10).tolist(),
    }
    
    # Save
    q_dir = os.path.join(artifact_dir, "quarter_reports", str(quarter).replace("/", "_"))
    os.makedirs(q_dir, exist_ok=True)
    
    with open(os.path.join(q_dir, "report.json"), "w") as f:
        json.dump(report, f, indent=2, default=str)
    
    top_50.to_parquet(os.path.join(q_dir, "bets.parquet"), index=False)
    
    return report

def select_analysis_quarters(results_df, metric_col="excess_vs_spy", n_random=3, seed=42):
    """
    Select best, worst, median, and random quarters for deep-dive.
    """
    valid = results_df.dropna(subset=[metric_col])
    if valid.empty:
        return []
    
    sorted_df = valid.sort_values(metric_col)
    
    quarters = []
    quarters.append(("worst", sorted_df.iloc[0]["quarter"]))
    quarters.append(("best", sorted_df.iloc[-1]["quarter"]))
    quarters.append(("median", sorted_df.iloc[len(sorted_df)//2]["quarter"]))
    
    # Random
    rng = np.random.default_rng(seed)
    random_idx = rng.choice(len(valid), size=min(n_random, len(valid)), replace=False)
    for i, idx in enumerate(random_idx):
        quarters.append((f"random_{i+1}", valid.iloc[idx]["quarter"]))
    
    return quarters

log("âœ… Cell J Complete: Quarter Report Functions Defined.")

# %% [markdown]
# # Cell K: Network Correlation GIF
#
# **Goal**: Create animated network visualization of firm correlations over time.

# %%
try:
    import networkx as nx
    import imageio
    NETWORK_AVAILABLE = True
except ImportError:
    log("[WARN] networkx or imageio not installed. Network GIF will be skipped.")
    NETWORK_AVAILABLE = False

def create_network_frame(panel_df, deals_df, quarter, target_col, top_n=30, corr_threshold=0.5):
    """
    Create a single network frame for a quarter.
    Nodes = firms, Edges = correlation > threshold, Color = M&A event.
    """
    if not NETWORK_AVAILABLE:
        return None
    
    q_mask = panel_df["panel_q"] == quarter
    df_q = panel_df[q_mask].copy()
    
    if len(df_q) < 5:
        return None
    
    # Get top N by some metric (e.g., market cap or assets)
    if "mkvalt" in df_q.columns:
        top_firms = df_q.nlargest(top_n, "mkvalt")["cik"].unique()
    else:
        top_firms = df_q["cik"].unique()[:top_n]
    
    # Build correlation matrix
    corr_mat = build_corr_matrix(panel_df, top_firms, quarter, lag_q=4)
    if corr_mat is None or corr_mat.shape[0] < 3:
        return None
    
    # Build graph
    G = nx.Graph()
    
    for cik in top_firms:
        if cik in corr_mat.index:
            # Check if this firm had an event
            had_event = 0
            if target_col in df_q.columns:
                firm_data = df_q[df_q["cik"] == cik]
                if not firm_data.empty:
                    had_event = int(firm_data[target_col].max())
            G.add_node(cik, event=had_event)
    
    # Add edges
    for i, cik1 in enumerate(corr_mat.index):
        for cik2 in corr_mat.index[i+1:]:
            corr_val = corr_mat.loc[cik1, cik2]
            if abs(corr_val) > corr_threshold:
                G.add_edge(cik1, cik2, weight=abs(corr_val))
    
    if len(G.nodes) < 3:
        return None
    
    # Draw
    fig, ax = plt.subplots(figsize=(10, 10))
    pos = nx.spring_layout(G, seed=42)
    
    colors = ["red" if G.nodes[n].get("event", 0) == 1 else "lightblue" for n in G.nodes]
    sizes = [300 if G.nodes[n].get("event", 0) == 1 else 100 for n in G.nodes]
    
    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=sizes, ax=ax)
    nx.draw_networkx_edges(G, pos, alpha=0.3, ax=ax)
    nx.draw_networkx_labels(G, pos, font_size=6, ax=ax)
    
    ax.set_title(f"Firm Correlation Network - {quarter}\nRed = M&A Event")
    ax.axis("off")
    
    # Save to buffer
    fig.canvas.draw()
    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype="uint8")
    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))
    plt.close(fig)
    
    return image

def create_network_gif(panel_df, deals_df, quarters, target_col, output_path, fps=1):
    """
    Create animated GIF of network evolution across quarters.
    """
    if not NETWORK_AVAILABLE:
        log("[SKIP] Network GIF creation skipped - dependencies not available.")
        return
    
    log(f"Creating Network GIF for {len(quarters)} quarters...")
    
    frames = []
    for q in tqdm(quarters, desc="Network Frames"):
        frame = create_network_frame(panel_df, deals_df, q, target_col)
        if frame is not None:
            frames.append(frame)
    
    if len(frames) < 2:
        log("[WARN] Not enough frames for GIF.")
        return
    
    imageio.mimsave(output_path, frames, fps=fps, loop=0)
    log(f"âœ… Network GIF saved to {output_path}")

# Run Network GIF (on last 8 quarters)
if NETWORK_AVAILABLE and "labeled_panel" in dir() and "panel_q" in labeled_panel.columns:
    recent_quarters = sorted(labeled_panel["panel_q"].unique())[-8:]
    gif_path = os.path.join(ARTIFACT_DIR, "network_correlation.gif")
    create_network_gif(labeled_panel, deals_df, recent_quarters, TARGET_HORIZON, gif_path)

log("âœ… Cell K Complete: Network GIF.")

# %% [markdown]
# # Cell L: Full Advanced Backtest with Correlation Screening
#
# **Goal**: Run the complete backtest with all advanced features.

# %%
def run_advanced_backtest(panel_df, target_col, feature_cols, horizons=[3]):
    """
    Run full expanding-window backtest with:
    - Correlation screening
    - Dynamic K selection
    - Multiple baselines (Universe, Random, S&P 500)
    """
    log("Starting Advanced Backtest...")
    
    panel_df["panel_q"] = panel_df["datadate"].dt.to_period("Q")
    quarters = sorted(panel_df["panel_q"].unique())
    
    # Need at least 10 quarters of burn-in
    if len(quarters) < 12:
        log("[WARN] Not enough quarters for advanced backtest.")
        return None
    
    results = []
    
    for q_idx in tqdm(range(10, len(quarters)), desc="Backtest"):
        q = quarters[q_idx]
        
        train_mask = panel_df["panel_q"] < q
        test_mask = panel_df["panel_q"] == q
        
        train = panel_df[train_mask]
        test = panel_df[test_mask].copy()
        
        if len(test) == 0 or train[target_col].sum() < 10:
            continue
        
        # Downsample train
        pos = train[train[target_col] == 1]
        neg = train[train[target_col] == 0]
        if len(neg) > len(pos) * 10:
            neg = neg.sample(n=len(pos) * 10, random_state=42)
        train_bal = pd.concat([pos, neg])
        
        # Train
        X_cols = [c for c in feature_cols if c in train_bal.columns]
        clf = HistGradientBoostingClassifier(learning_rate=0.05, max_depth=5, random_state=42)
        clf.fit(train_bal[X_cols].fillna(0), train_bal[target_col])
        
        # Predict
        test["prob_acq"] = clf.predict_proba(test[X_cols].fillna(0))[:, 1]
        test["net_alpha"] = (test["prob_acq"] - train[target_col].mean()) * DEAL_PREMIUM_EST - COST_PER_TRADE
        
        # Correlation screening
        candidates = test.sort_values("net_alpha", ascending=False).head(int(K_MAX * 4))
        corr_mat = build_corr_matrix(panel_df, candidates["cik"].unique(), q)
        selected = select_with_corr_screen(candidates, corr_mat)
        
        if len(selected) < K_MIN:
            selected = test.nlargest(K_MIN, "prob_acq")
        
        # Portfolio return
        port_ret = clip_returns(selected["ret_fwd_1q"]).mean() if "ret_fwd_1q" in selected.columns else np.nan
        univ_ret = clip_returns(test["ret_fwd_1q"]).mean() if "ret_fwd_1q" in test.columns else np.nan
        spy_ret = spy_map.get(q, np.nan) if "spy_map" in dir() else np.nan
        
        # AUC
        try:
            auc_val = roc_auc_score(test[target_col], test["prob_acq"])
        except:
            auc_val = 0.5
        
        results.append({
            "quarter": str(q),
            "auc": auc_val,
            "n_bets": len(selected),
            "port_ret": port_ret,
            "univ_ret": univ_ret,
            "spy_ret": spy_ret,
            "excess_vs_univ": port_ret - univ_ret if not np.isnan(univ_ret) else np.nan,
            "excess_vs_spy": port_ret - spy_ret if not np.isnan(spy_ret) else np.nan,
            "hit_rate": selected[target_col].mean() if target_col in selected.columns else np.nan,
        })
    
    return pd.DataFrame(results)

# Run Advanced Backtest
if "labeled_panel" in dir() and "final_feats" in dir():
    adv_results = run_advanced_backtest(labeled_panel, TARGET_HORIZON, final_feats)
    if adv_results is not None:
        adv_path = os.path.join(ARTIFACT_DIR, "results_advanced_backtest.parquet")
        adv_results.to_parquet(adv_path)
        log(f"âœ… Advanced Backtest saved to {adv_path}")
        
        # Summary
        print("\n" + "="*60)
        print("ADVANCED BACKTEST SUMMARY")
        print("="*60)
        print(f"Mean AUC:         {adv_results['auc'].mean():.3f}")
        print(f"Mean Port Return: {adv_results['port_ret'].mean():.2%}")
        print(f"Mean Excess/Univ: {adv_results['excess_vs_univ'].mean():.2%}")
        print(f"Mean Excess/SPY:  {adv_results['excess_vs_spy'].dropna().mean():.2%}")
        print(f"Mean Hit Rate:    {adv_results['hit_rate'].mean():.2%}")

log("âœ… Pipeline Complete with All Features.")

# Final Calibration Footer
print("\n" + "="*60)
print("Persona Used: Senior Quant ML Engineer")
print("Difficulty Tuning: ~70% ZPD (Full Pipeline), ~20% Staff+ (Correlation Screening + Network GIF), ~10% Aspirational (Advanced Portfolio Construction)")
print("Concrete Suggestion: Review the network GIF to identify clustered firms that may co-move during M&A waves.")
print("="*60)

# %% [markdown]
# # Cell M: Probability Calibration Pipeline
#
# **Goal**: Correct raw model probabilities for downsampling bias and calibrate to empirical frequencies.
#
# **Two-Layer Approach**:
# 1. **Layer A (Prior Correction)**: Adjust for case-control sampling using log-odds correction
# 2. **Layer B (Calibration)**: Map to empirical frequencies using isotonic regression or Platt scaling
#
# ---
# ## Compute / GPU Utilization Note (A100)
#
# The current pipeline is primarily **Pandas** for data prep/feature engineering and **scikit-learn HistGradientBoostingClassifier** for modeling.
# This configuration is **CPU-bound** and will **not materially utilize an NVIDIA A100 (80GB) GPU**.
#
# GPU acceleration will only occur if migrated to:
# - **RAPIDS cuDF** for DataFrame operations
# - **XGBoost/LightGBM** with CUDA-enabled training
#
# The redesign prioritizes **correctness and reproducibility** first. GPU paths are optional accelerations.

# %%
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.linear_model import LogisticRegression
import pickle

# Configuration
USE_GPU = False  # Set True to enable GPU paths (requires XGBoost + CUDA)
CAL_WINDOW_Q = 4  # Number of quarters for calibration set

def sigmoid(x):
    """Numerically stable sigmoid."""
    return np.where(x >= 0, 
                    1 / (1 + np.exp(-x)), 
                    np.exp(x) / (1 + np.exp(x)))

def logit(p, eps=1e-7):
    """Safe logit transformation."""
    p = np.clip(p, eps, 1 - eps)
    return np.log(p / (1 - p))

def prior_correction(p_sample, pi_true, pi_sample):
    """
    Layer A: Prior correction for case-control (downsampling) adjustment.
    
    Given model trained on downsampled data with sample base rate pi_sample,
    correct to true population base rate pi_true.
    
    Formula: logit(p) = logit(p_s) + log(pi/(1-pi)) - log(pi_s/(1-pi_s))
    """
    if pi_true <= 0 or pi_true >= 1 or pi_sample <= 0 or pi_sample >= 1:
        return p_sample  # Cannot correct, return raw
    
    log_odds_true = np.log(pi_true / (1 - pi_true))
    log_odds_sample = np.log(pi_sample / (1 - pi_sample))
    
    logit_p_sample = logit(p_sample)
    logit_p_corrected = logit_p_sample + log_odds_true - log_odds_sample
    
    return sigmoid(logit_p_corrected)

def fit_platt_calibrator(probs, labels):
    """
    Layer B Option 1: Platt scaling (logistic regression on logits).
    Returns calibrator function and parameters.
    """
    if len(np.unique(labels)) < 2:
        return None, None
    
    X_logit = logit(probs).reshape(-1, 1)
    lr = LogisticRegression(solver='lbfgs', max_iter=1000)
    lr.fit(X_logit, labels)
    
    params = {"coef": float(lr.coef_[0, 0]), "intercept": float(lr.intercept_[0])}
    
    def calibrator(p):
        return lr.predict_proba(logit(p).reshape(-1, 1))[:, 1]
    
    return calibrator, params

def fit_isotonic_calibrator(probs, labels):
    """
    Layer B Option 2: Isotonic regression calibration.
    """
    from sklearn.isotonic import IsotonicRegression
    
    if len(np.unique(labels)) < 2:
        return None, None
    
    ir = IsotonicRegression(out_of_bounds='clip')
    ir.fit(probs, labels)
    
    def calibrator(p):
        return ir.predict(p)
    
    return calibrator, {"method": "isotonic"}

def compute_calibration_metrics(y_true, p_pred, n_bins=10):
    """
    Compute calibration metrics: Brier score, ECE, reliability bins.
    """
    # Brier score
    brier = np.mean((p_pred - y_true) ** 2)
    
    # Reliability bins
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(p_pred, bin_edges) - 1
    bin_indices = np.clip(bin_indices, 0, n_bins - 1)
    
    bins = []
    ece = 0.0
    for i in range(n_bins):
        mask = bin_indices == i
        if mask.sum() > 0:
            mean_pred = p_pred[mask].mean()
            mean_true = y_true[mask].mean()
            count = mask.sum()
            bins.append({
                "bin": i,
                "mean_predicted": mean_pred,
                "mean_realized": mean_true,
                "count": int(count)
            })
            ece += (count / len(p_pred)) * abs(mean_pred - mean_true)
    
    return {
        "brier": brier,
        "ece": ece,
        "bins": bins
    }

def calibrate_quarter(panel_df, train_df, test_df, target_col, model, feature_cols, 
                      pi_train_full, pi_train_sample, artifact_dir, quarter_str):
    """
    Full calibration pipeline for a single quarter.
    
    Returns: DataFrame with p_raw, p_prior_corrected, p_calibrated columns
    """
    # Get raw probabilities
    X_test = test_df[feature_cols].fillna(0)
    p_raw = model.predict_proba(X_test)[:, 1]
    
    # Layer A: Prior correction
    if pi_train_sample != pi_train_full:
        p_prior_corrected = prior_correction(p_raw, pi_train_full, pi_train_sample)
        log(f"Prior correction applied: Ï€_full={pi_train_full:.4f}, Ï€_sample={pi_train_sample:.4f}")
    else:
        p_prior_corrected = p_raw.copy()
        log("No downsampling detected, skipping prior correction.")
    
    # Layer B: Calibration using held-out calibration set
    # Use most recent CAL_WINDOW_Q quarters before holdout as calibration set
    if "panel_q" not in train_df.columns:
        train_df["panel_q"] = train_df["datadate"].dt.to_period("Q")
    
    quarters = sorted(train_df["panel_q"].unique())
    cal_quarters = quarters[-CAL_WINDOW_Q:] if len(quarters) > CAL_WINDOW_Q else quarters
    
    cal_mask = train_df["panel_q"].isin(cal_quarters)
    cal_set = train_df[cal_mask]
    
    pi_cal = cal_set[target_col].mean() if len(cal_set) > 0 else pi_train_full
    
    if len(cal_set) > 50 and len(np.unique(cal_set[target_col])) == 2:
        # Get calibration set predictions
        X_cal = cal_set[feature_cols].fillna(0)
        p_cal_raw = model.predict_proba(X_cal)[:, 1]
        
        # Apply prior correction to cal set too
        if pi_train_sample != pi_train_full:
            p_cal_corrected = prior_correction(p_cal_raw, pi_train_full, pi_train_sample)
        else:
            p_cal_corrected = p_cal_raw
        
        # Fit calibrator on calibration set
        try:
            calibrator, cal_params = fit_isotonic_calibrator(p_cal_corrected, cal_set[target_col].values)
            if calibrator is not None:
                p_calibrated = calibrator(p_prior_corrected)
                log("Isotonic calibration applied.")
            else:
                p_calibrated = p_prior_corrected.copy()
                cal_params = {"method": "none", "reason": "calibrator_fit_failed"}
        except Exception as e:
            log(f"Isotonic failed ({e}), falling back to Platt scaling.")
            calibrator, cal_params = fit_platt_calibrator(p_cal_corrected, cal_set[target_col].values)
            if calibrator is not None:
                p_calibrated = calibrator(p_prior_corrected)
            else:
                p_calibrated = p_prior_corrected.copy()
                cal_params = {"method": "none", "reason": str(e)}
    else:
        p_calibrated = p_prior_corrected.copy()
        cal_params = {"method": "none", "reason": "insufficient_cal_data"}
        log(f"Calibration skipped: insufficient calibration data ({len(cal_set)} rows)")
    
    # Save calibration artifacts
    cal_dir = os.path.join(artifact_dir, "calibration", f"quarter_{quarter_str}")
    os.makedirs(cal_dir, exist_ok=True)
    
    with open(os.path.join(cal_dir, "calibrator_params.json"), "w") as f:
        json.dump(cal_params, f, indent=2, default=str)
    
    # Compute metrics
    y_test = test_df[target_col].values
    metrics_raw = compute_calibration_metrics(y_test, p_raw)
    metrics_corrected = compute_calibration_metrics(y_test, p_prior_corrected)
    metrics_calibrated = compute_calibration_metrics(y_test, p_calibrated)
    
    # Print calibration report
    print(f"\n{'='*50}")
    print(f"CALIBRATION REPORT: {quarter_str}")
    print(f"{'='*50}")
    print(f"Ï€_train_full (true base rate):    {pi_train_full:.4f}")
    print(f"Ï€_train_sample (after downsample): {pi_train_sample:.4f}")
    print(f"Ï€_calibration_set:                 {pi_cal:.4f}")
    print(f"Ï€_holdout:                         {y_test.mean():.4f}")
    print(f"\nBrier Scores:")
    print(f"  p_raw:              {metrics_raw['brier']:.4f}")
    print(f"  p_prior_corrected:  {metrics_corrected['brier']:.4f}")
    print(f"  p_calibrated:       {metrics_calibrated['brier']:.4f}")
    print(f"ECE (p_calibrated):   {metrics_calibrated['ece']:.4f}")
    
    # Save calibration bins
    bins_df = pd.DataFrame(metrics_calibrated["bins"])
    bins_df.to_csv(os.path.join(cal_dir, "calibration_bins.csv"), index=False)
    
    # Return result
    result_df = test_df.copy()
    result_df["p_raw"] = p_raw
    result_df["p_prior_corrected"] = p_prior_corrected
    result_df["p_calibrated"] = p_calibrated
    
    return result_df, {
        "quarter": quarter_str,
        "pi_train_full": pi_train_full,
        "pi_train_sample": pi_train_sample,
        "pi_cal": pi_cal,
        "pi_holdout": float(y_test.mean()),
        "brier_raw": metrics_raw["brier"],
        "brier_corrected": metrics_corrected["brier"],
        "brier_calibrated": metrics_calibrated["brier"],
        "ece_calibrated": metrics_calibrated["ece"],
    }

def plot_reliability_diagram(y_true, p_raw, p_corrected, p_calibrated, title, save_path):
    """
    Plot reliability diagram comparing raw, corrected, and calibrated probabilities.
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for ax, (probs, name) in zip(axes, [(p_raw, "Raw"), (p_corrected, "Prior Corrected"), (p_calibrated, "Calibrated")]):
        prob_true, prob_pred = calibration_curve(y_true, probs, n_bins=10, strategy='uniform')
        
        ax.plot([0, 1], [0, 1], "k--", label="Perfect")
        ax.plot(prob_pred, prob_true, "s-", color="crimson", label=name)
        ax.set_xlabel("Mean Predicted Probability")
        ax.set_ylabel("Fraction of Positives")
        ax.set_title(f"{name}")
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    fig.suptitle(title, fontsize=14)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.show()

def plot_probability_histograms(p_raw, p_calibrated, title, save_path):
    """
    Plot histograms showing probability compression after calibration.
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    axes[0].hist(p_raw, bins=50, color='steelblue', alpha=0.7, edgecolor='white')
    axes[0].set_xlabel("Probability")
    axes[0].set_ylabel("Count")
    axes[0].set_title("Raw Model Probabilities")
    axes[0].set_xlim(0, 1)
    
    axes[1].hist(p_calibrated, bins=50, color='crimson', alpha=0.7, edgecolor='white')
    axes[1].set_xlabel("Probability")
    axes[1].set_ylabel("Count")
    axes[1].set_title("Calibrated Probabilities")
    axes[1].set_xlim(0, 1)
    
    fig.suptitle(title, fontsize=14)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.show()

log("âœ… Cell M Complete: Probability Calibration Pipeline Defined.")

# %% [markdown]
# # Cell N: Calibrated Backtest with Probability Rescaling
#
# **Goal**: Run the full backtest using calibrated probabilities instead of raw scores.

# %%
def run_calibrated_backtest(panel_df, target_col, feature_cols, artifact_dir):
    """
    Full backtest with probability calibration.
    Strategy uses p_calibrated instead of raw probabilities.
    """
    log("Starting Calibrated Backtest...")
    
    panel_df = panel_df.copy()
    panel_df["panel_q"] = panel_df["datadate"].dt.to_period("Q")
    quarters = sorted(panel_df["panel_q"].unique())
    
    if len(quarters) < 12:
        log("[WARN] Not enough quarters for calibrated backtest.")
        return None, None
    
    results = []
    calibration_summary = []
    
    for q_idx in tqdm(range(10, len(quarters)), desc="Calibrated Backtest"):
        q = quarters[q_idx]
        quarter_str = str(q).replace("/", "_")
        
        train_mask = panel_df["panel_q"] < q
        test_mask = panel_df["panel_q"] == q
        
        train_full = panel_df[train_mask]
        test = panel_df[test_mask].copy()
        
        if len(test) == 0 or train_full[target_col].sum() < 10:
            continue
        
        # True base rate (before downsampling)
        pi_train_full = train_full[target_col].mean()
        
        # Downsample
        pos = train_full[train_full[target_col] == 1]
        neg = train_full[train_full[target_col] == 0]
        if len(neg) > len(pos) * 10:
            neg = neg.sample(n=len(pos) * 10, random_state=42)
        train_sample = pd.concat([pos, neg])
        
        # Sample base rate (after downsampling)
        pi_train_sample = train_sample[target_col].mean()
        
        # Train model
        X_cols = [c for c in feature_cols if c in train_sample.columns]
        clf = HistGradientBoostingClassifier(learning_rate=0.05, max_depth=5, random_state=42)
        clf.fit(train_sample[X_cols].fillna(0), train_sample[target_col])
        
        # Calibrate
        test_calibrated, cal_metrics = calibrate_quarter(
            panel_df, train_full, test, target_col, clf, X_cols,
            pi_train_full, pi_train_sample, artifact_dir, quarter_str
        )
        calibration_summary.append(cal_metrics)
        
        # Portfolio using calibrated probabilities
        test_calibrated["net_alpha"] = (test_calibrated["p_calibrated"] - pi_train_full) * DEAL_PREMIUM_EST - COST_PER_TRADE
        candidates = test_calibrated.sort_values("net_alpha", ascending=False).head(K_MAX * 2)
        
        # Correlation screening
        corr_mat = build_corr_matrix(panel_df, candidates["cik"].unique(), q) if "cik" in candidates.columns else None
        selected = select_with_corr_screen(candidates, corr_mat) if corr_mat is not None else candidates.head(K_MAX)
        
        if len(selected) < K_MIN:
            selected = test_calibrated.nlargest(K_MIN, "p_calibrated")
        
        # Calculate returns
        port_ret = clip_returns(selected["ret_fwd_1q"]).mean() if "ret_fwd_1q" in selected.columns else np.nan
        univ_ret = clip_returns(test["ret_fwd_1q"]).mean() if "ret_fwd_1q" in test.columns else np.nan
        spy_ret = spy_map.get(q, np.nan) if "spy_map" in dir() else np.nan
        
        # AUC
        try:
            auc_val = roc_auc_score(test[target_col], test_calibrated["p_calibrated"])
        except:
            auc_val = 0.5
        
        # Expected deals in selection
        expected_deals = selected["p_calibrated"].sum() if "p_calibrated" in selected.columns else np.nan
        realized_deals = selected[target_col].sum() if target_col in selected.columns else np.nan
        
        results.append({
            "quarter": str(q),
            "auc": auc_val,
            "n_bets": len(selected),
            "port_ret": port_ret,
            "univ_ret": univ_ret,
            "spy_ret": spy_ret,
            "excess_vs_univ": port_ret - univ_ret if not np.isnan(univ_ret) else np.nan,
            "excess_vs_spy": port_ret - spy_ret if not np.isnan(spy_ret) else np.nan,
            "hit_rate": selected[target_col].mean() if target_col in selected.columns else np.nan,
            "expected_deals": expected_deals,
            "realized_deals": realized_deals,
            "mean_p_calibrated_top50": test_calibrated.nlargest(50, "p_calibrated")["p_calibrated"].mean(),
        })
        
        # Print top 10 bets with calibrated probabilities
        if q_idx % 8 == 0:  # Print every 8th quarter
            print(f"\n--- Top 10 Bets for {q} ---")
            top_10 = selected.nlargest(10, "p_calibrated")
            cols_show = [c for c in ["cik", "p_calibrated", "p_raw", target_col] if c in top_10.columns]
            print(top_10[cols_show].to_string(index=False))
    
    # Save results
    results_df = pd.DataFrame(results)
    results_path = os.path.join(artifact_dir, "results_calibrated_backtest.parquet")
    results_df.to_parquet(results_path)
    
    cal_summary_df = pd.DataFrame(calibration_summary)
    cal_summary_path = os.path.join(artifact_dir, "calibration_summary.parquet")
    cal_summary_df.to_parquet(cal_summary_path)
    
    log(f"âœ… Calibrated Backtest saved to {results_path}")
    log(f"âœ… Calibration Summary saved to {cal_summary_path}")
    
    # Plot calibration over time
    if len(cal_summary_df) > 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # Brier scores over time
        axes[0, 0].plot(cal_summary_df["quarter"], cal_summary_df["brier_raw"], label="Raw", marker="o")
        axes[0, 0].plot(cal_summary_df["quarter"], cal_summary_df["brier_corrected"], label="Corrected", marker="s")
        axes[0, 0].plot(cal_summary_df["quarter"], cal_summary_df["brier_calibrated"], label="Calibrated", marker="^")
        axes[0, 0].set_xlabel("Quarter")
        axes[0, 0].set_ylabel("Brier Score")
        axes[0, 0].set_title("Brier Score Over Time (Lower is Better)")
        axes[0, 0].legend()
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # ECE over time
        axes[0, 1].plot(cal_summary_df["quarter"], cal_summary_df["ece_calibrated"], color="crimson", marker="o")
        axes[0, 1].set_xlabel("Quarter")
        axes[0, 1].set_ylabel("ECE")
        axes[0, 1].set_title("Expected Calibration Error Over Time")
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # Base rates
        axes[1, 0].plot(cal_summary_df["quarter"], cal_summary_df["pi_train_full"], label="Train Full", marker="o")
        axes[1, 0].plot(cal_summary_df["quarter"], cal_summary_df["pi_holdout"], label="Holdout", marker="s")
        axes[1, 0].set_xlabel("Quarter")
        axes[1, 0].set_ylabel("Base Rate")
        axes[1, 0].set_title("Base Rates Over Time")
        axes[1, 0].legend()
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # Mean calibrated prob vs hit rate
        if len(results_df) > 0:
            axes[1, 1].scatter(results_df["mean_p_calibrated_top50"], results_df["hit_rate"], alpha=0.7)
            axes[1, 1].plot([0, 0.5], [0, 0.5], "k--", label="Perfect Calibration")
            axes[1, 1].set_xlabel("Mean Calibrated Prob (Top 50)")
            axes[1, 1].set_ylabel("Realized Hit Rate (Top 50)")
            axes[1, 1].set_title("Calibration vs Realized Hit Rate")
            axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(artifact_dir, "calibration_diagnostics.png"), dpi=150)
        plt.show()
    
    # Summary
    print("\n" + "="*60)
    print("CALIBRATED BACKTEST SUMMARY")
    print("="*60)
    print(f"Mean AUC:               {results_df['auc'].mean():.3f}")
    print(f"Mean Port Return:       {results_df['port_ret'].mean():.2%}")
    print(f"Mean Excess/Universe:   {results_df['excess_vs_univ'].mean():.2%}")
    print(f"Mean Excess/SPY:        {results_df['excess_vs_spy'].dropna().mean():.2%}")
    print(f"Mean Hit Rate:          {results_df['hit_rate'].mean():.2%}")
    print(f"Mean Expected Deals:    {results_df['expected_deals'].mean():.2f}")
    print(f"Mean Realized Deals:    {results_df['realized_deals'].mean():.2f}")
    print(f"Mean Brier (Calibrated):{cal_summary_df['brier_calibrated'].mean():.4f}")
    
    return results_df, cal_summary_df

# Run Calibrated Backtest
if "labeled_panel" in dir() and "final_feats" in dir():
    cal_results, cal_summary = run_calibrated_backtest(labeled_panel, TARGET_HORIZON, final_feats, ARTIFACT_DIR)

log("âœ… Cell N Complete: Calibrated Backtest.")

# %% [markdown]
# # Performance Budget Table
#
# | Stage | CPU Time | GPU Time (Optional) | Notes |
# |-------|----------|---------------------|-------|
# | Data Loading | ~10s | N/A | I/O bound, GPU won't help |
# | Feature Engineering | ~30-60s | ~5-10s (cuDF) | Pandas -> RAPIDS cuDF |
# | Training (per quarter) | ~2-5s | ~0.5-1s (XGBoost CUDA) | sklearn -> XGBoost GPU |
# | Backtest Loop | ~5-15min | ~2-5min | Mostly training time |
# | Plotting | ~10s | N/A | Matplotlib is CPU-only |

# %%
# Final Footer with All Features
print("\n" + "="*70)
print("M&A PREDICTION PIPELINE - COMPLETE")
print("="*70)
print("\nFeatures Implemented:")
print("  âœ… Multi-horizon labeling (3m-24m)")
print("  âœ… S&P 500 baseline comparison")
print("  âœ… Stability dry-run harness (Jaccard overlap)")
print("  âœ… Event study verification (jump plot)")
print("  âœ… Correlation screening for portfolio construction")
print("  âœ… Network correlation GIF")
print("  âœ… Quarter deep-dive reports")
print("  âœ… Probability calibration (prior correction + isotonic/Platt)")
print("\nCompute Mode: CPU (HistGradientBoosting)")
print("GPU Acceleration: Not enabled (set USE_GPU=True for XGBoost CUDA)")
print("\nPersona Used: Senior Quant ML Engineer")
print("Difficulty: ~70% ZPD, ~20% Staff+, ~10% Aspirational")
print("="*70)
