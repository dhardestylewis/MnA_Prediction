# -*- coding: utf-8 -*-
"""mna_colab_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1...

# M&A Prediction Pipeline (Redesigned)
**Persona**: Senior Quant ML Engineer
**Objective**: Robust, reproducible M&A forecasting with multi-horizon targets (3m-24m) and S&P 500 benchmarking.
"""

# %% [markdown]
# # Cell A: Setup, Config & Mount Drive
#
# **Goal**: Initialize environment, install missing dependencies, and define global configuration.

# %%
import os
import sys
import time
import json
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

# 1. Install dependencies if missing (Colab specific)
try:
    import yfinance as yf
    print("[INFO] yfinance already installed.")
except ImportError:
    print("[INFO] Installing yfinance...")
    !pip install yfinance
    import yfinance as yf

# 2. Mount Google Drive (for large parquet files)
try:
    from google.colab import drive
    drive.mount('/content/drive')
    DRIVE_DIR = "/content/drive/MyDrive"
    IN_COLAB = True
except ImportError:
    DRIVE_DIR = os.getcwd()
    IN_COLAB = False
    print(f"[WARN] Not running in Colab. Using local DRIVE_DIR: {DRIVE_DIR}")

# 3. Clone GitHub repo (for pipeline code + smaller data files)
GITHUB_REPO = "https://github.com/dhardestylewis/MnA_Prediction.git"
REPO_DIR = "/content/MnA_Prediction" if IN_COLAB else os.getcwd()

if IN_COLAB and not os.path.exists(REPO_DIR):
    print(f"[INFO] Cloning GitHub repo to {REPO_DIR}...")
    !git clone {GITHUB_REPO} {REPO_DIR}
elif IN_COLAB:
    print(f"[INFO] Repo already exists at {REPO_DIR}. Pulling latest...")
    !cd {REPO_DIR} && git pull

# 4. Helper: Resolve file path (GitHub repo first, then Drive)
def resolve_path(filename, prefer_repo=True):
    """Find file in GitHub repo first, fallback to Google Drive."""
    repo_path = os.path.join(REPO_DIR, filename)
    drive_path = os.path.join(DRIVE_DIR, filename)
    
    if prefer_repo and os.path.exists(repo_path):
        return repo_path
    elif os.path.exists(drive_path):
        return drive_path
    elif os.path.exists(repo_path):
        return repo_path
    else:
        print(f"[WARN] File not found: {filename}")
        return None

# 5. Global Config
CONFIG = {
    "run_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
    "repo_dir": REPO_DIR,
    "drive_dir": DRIVE_DIR,
    "artifact_subfolder": "mna_artifacts",
    "inputs": {
        # Large files -> Google Drive only
        "fundq": "fundq_full.parquet",
        "funda": "funda_full.parquet",
        # Smaller files -> GitHub repo preferred (new directory structure)
        "deals": "data/deals/dma_corpus_metadata_with_factset_id.csv",
        "factset_xls_dir": "data/deals/factset_xls/factset_2000_2025",
        "fundamentals_csv": "data/fundamentals/compustat_funda_2000on.csv"
    },
    "horizons_months": [3, 6, 9, 12, 15, 18, 21, 24],
    "safe_lag_days": 90,
    "filters": {
        "keyset": "STD",  # Require Standard Feed
        "min_market_cap_log": 0.0  # Optional raw filter
    },
    "calibration": {
        "test_size_qtrs": 8,  # Not strictly used in expanding window, but for ref
        "n_dryrun_trials": 5
    },
    # ================================================
    # COMPILE-ONCE / TRAIN-MANY CONFIGURATION (NEW)
    # ================================================
    "compile": {
        "force_recompile": False,  # Set True to bypass cache
        "use_memmap": True,        # Use memmap for X_global (memory efficient)
    },
    "sampling": {
        "enabled": True,           # Enable negative sampling for speed
        "neg_pos_ratio_dev": 10,   # 10:1 for dev mode
        "neg_pos_ratio_std": 20,   # 20:1 for standard mode  
        "use_sample_weights": True,  # Correct for sampling bias
    },
    "inner_loop": {
        "enabled": False,          # Set True to run single quarter (fast dev)
        "quarter": None,           # e.g., "2020Q4" - if None, pick automatically
        "horizon": 3,              # Single horizon for inner loop
    },
    "automl": {
        "n_configs": 8,            # Number of random HP configs
        "budget_small": 100,       # Trees for initial halving round
        "top_k_advance": 2,        # How many configs advance to full training
    },
    "stability": {
        "enabled": True,
        "min_repeats": 5,          # Start with 5 repeats
        "max_repeats": 20,         # Max repeats if CI not met
        "top_k": 50,               # For Jaccard@K
        "jaccard_ci_threshold": 0.10,  # CI half-width threshold
        "return_ci_threshold": 0.02,   # 2% CI half-width for returns
    },
    "benchmark": {
        "spy_fail_fast": True,     # Fail if SPY returns missing
    },
}

# 6. Create Artifact Directory (on Google Drive for persistence)
ARTIFACT_DIR = os.path.join(DRIVE_DIR, CONFIG["artifact_subfolder"], CONFIG["run_id"])
os.makedirs(ARTIFACT_DIR, exist_ok=True)
print(f"[INFO] Artifacts will be saved to: {ARTIFACT_DIR}")

# 7. Save Config
config_path = os.path.join(ARTIFACT_DIR, "config.json")
with open(config_path, "w") as f:
    json.dump(CONFIG, f, indent=4, default=str)

def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

# ================================================
# CHECKPOINT / RESUME INFRASTRUCTURE
# ================================================
# Set RESUME_FROM_RUN_ID to resume from a specific run, OR set AUTO_RESUME=True
# to automatically pick the most recent run folder.
RESUME_FROM_RUN_ID = None  # e.g., "20260129_222354"
AUTO_RESUME = True        # If True, looks for latest folder in artifact_subfolder

def find_latest_run_id(drive_base_dir, subfolder):
    """Finds the most recent run ID folder."""
    artifacts_path = os.path.join(drive_base_dir, subfolder)
    if not os.path.exists(artifacts_path):
        return None
    
    # List directories (run IDs are YYYYMMDD_HHMMSS)
    runs = [d for d in os.listdir(artifacts_path) 
            if os.path.isdir(os.path.join(artifacts_path, d)) and "_" in d]
    
    if not runs:
        return None
        
    # Sort descending
    runs.sort(reverse=True)
    
    # Filter out current run_id (we want the *previous* one)
    # CONFIG["run_id"] is the current one we just created
    prev_runs = [r for r in runs if r != CONFIG["run_id"]]
    
    return prev_runs[0] if prev_runs else None

# Determine checkpoint directory
if AUTO_RESUME and not RESUME_FROM_RUN_ID:
    latest_run = find_latest_run_id(DRIVE_DIR, CONFIG["artifact_subfolder"])
    if latest_run:
        RESUME_FROM_RUN_ID = latest_run
        log(f"ðŸ”Ž Auto-detected latest run: {latest_run}")

if RESUME_FROM_RUN_ID:
    CHECKPOINT_DIR = os.path.join(DRIVE_DIR, CONFIG["artifact_subfolder"], RESUME_FROM_RUN_ID)
    if os.path.exists(CHECKPOINT_DIR):
        log(f"â™»ï¸ RESUMING from run: {RESUME_FROM_RUN_ID}")
    else:
        log(f"[WARN] Resume run {RESUME_FROM_RUN_ID} not found, starting fresh")
        CHECKPOINT_DIR = ARTIFACT_DIR
else:
    CHECKPOINT_DIR = ARTIFACT_DIR

def checkpoint_exists(name):
    """Check if a checkpoint file exists."""
    path = os.path.join(CHECKPOINT_DIR, f"{name}.parquet")
    return os.path.exists(path)

def save_checkpoint(df, name):
    """Save DataFrame checkpoint to Drive."""
    path = os.path.join(ARTIFACT_DIR, f"{name}.parquet")
    df.to_parquet(path, index=False)
    log(f"ðŸ’¾ Checkpoint saved: {name} ({len(df)} rows)")
    return path

def load_checkpoint(name):
    """Load DataFrame from checkpoint."""
    path = os.path.join(CHECKPOINT_DIR, f"{name}.parquet")
    if os.path.exists(path):
        df = pd.read_parquet(path)
        log(f"ðŸ“‚ Checkpoint loaded: {name} ({len(df)} rows)")
        return df
    return None

log("âœ… Cell A Complete: Environment Setup.")

# %% [markdown]
# # Cell A1: COMPILE-ONCE / TRAIN-MANY Infrastructure
#
# **Goal**: Implement two-phase architecture where feature engineering happens once (cached),
# and training/eval uses only array slicing for fast iteration.
#
# **Phase A: COMPILE** (slow, amortized, cached)
# - Load raw data, clean/filter, feature engineering, multi-horizon labeling
# - Produce global float32 design matrix and row indices by quarter
#
# **Phase B: TRAIN/EVAL** (fast, repeated, â‰¤60s target)
# - Slice by precomputed indices, optional sampling, train with early stopping
# - No Pandas joins, no dtype inference, no feature engineering

# %%
import hashlib
import subprocess
import psutil
from dataclasses import dataclass, field
from typing import Optional, Dict, List, Tuple, Any
from scipy import stats

# Try to import LightGBM (preferred) or fall back to sklearn
try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
    log("[INFO] LightGBM available")
except ImportError:
    HAS_LIGHTGBM = False
    log("[WARN] LightGBM not available, using HistGradientBoosting")

# ================================================
# COMPILE-ONCE INFRASTRUCTURE
# ================================================

@dataclass
class CompileArtifacts:
    """References to compiled artifacts for fast train/eval."""
    # Core arrays
    X_global: np.ndarray = None          # float32 design matrix
    feature_names: List[str] = None       # Column names aligned to X
    y_by_horizon: Dict[int, np.ndarray] = None  # uint8 labels per horizon
    quarter_ids: np.ndarray = None        # Quarter string per row
    
    # Pre-computed splits (train rows before quarter, test rows in quarter)
    rows_train_by_quarter: Dict[str, np.ndarray] = None
    rows_test_by_quarter: Dict[str, np.ndarray] = None
    
    # Metadata for attribution
    metadata_gvkey: np.ndarray = None
    metadata_cik: np.ndarray = None
    metadata_datadate: np.ndarray = None
    metadata_ret_fwd: np.ndarray = None   # Forward returns for portfolio
    
    # Data coverage tracking (R5.D)
    dropped_deals_by_quarter: Dict[str, int] = None  # Deals dropped due to missing CIK
    coverage_rate_by_quarter: Dict[str, float] = None  # % of deals with valid mapping
    
    # Cache info
    cache_key: str = ""
    cache_hit: bool = False
    compile_seconds: float = 0.0
    n_rows: int = 0
    n_features: int = 0

def get_git_commit_hash():
    """Get current git commit hash for cache keying."""
    try:
        result = subprocess.run(["git", "rev-parse", "HEAD"], 
                                capture_output=True, text=True, timeout=5, cwd=REPO_DIR)
        if result.returncode == 0:
            return result.stdout.strip()[:12]
    except Exception:
        pass
    return "unknown"

def compute_file_hash(filepath):
    """Compute hash from file mtime and size."""
    if not os.path.exists(filepath):
        return "missing"
    stat = os.stat(filepath)
    return hashlib.md5(f"{stat.st_mtime}:{stat.st_size}".encode()).hexdigest()[:16]

def compute_compile_cache_key(config):
    """Compute deterministic cache key for compile phase."""
    parts = []
    # Input file hashes
    for key in ["fundq", "funda"]:
        path = os.path.join(config["drive_dir"], config["inputs"][key])
        parts.append(f"{key}:{compute_file_hash(path)}")
    
    deals_path = resolve_path(config["inputs"]["deals"])
    parts.append(f"deals:{compute_file_hash(deals_path) if deals_path else 'missing'}")
    
    # Config parameters affecting compile
    parts.append(f"lag:{config['safe_lag_days']}")
    parts.append(f"horizons:{','.join(map(str, config['horizons_months']))}")
    parts.append(f"keyset:{config['filters']['keyset']}")
    parts.append(f"git:{get_git_commit_hash()}")
    
    return hashlib.sha256("|".join(parts).encode()).hexdigest()[:32]

def get_rss_mb():
    """Get current process RSS in MB."""
    try:
        return psutil.Process().memory_info().rss / 1e6
    except:
        return 0.0

# ================================================
# TIMING LOGGER
# ================================================

class TimingLogger:
    """Structured timing logger for compile/train phases."""
    def __init__(self, artifact_dir):
        self.artifact_dir = artifact_dir
        self.logs_dir = os.path.join(artifact_dir, "logs")
        os.makedirs(self.logs_dir, exist_ok=True)
        self.csv_path = os.path.join(self.logs_dir, "timing.csv")
        self.rows = []
        
    def log_phase(self, phase, **kwargs):
        """Log a timing entry."""
        entry = {"phase": phase, "run_id": CONFIG["run_id"], "timestamp": datetime.now().isoformat()}
        entry.update(kwargs)
        self.rows.append(entry)
        
    def save(self):
        """Write timing.csv."""
        if self.rows:
            df = pd.DataFrame(self.rows)
            df.to_csv(self.csv_path, index=False)
            log(f"ðŸ“Š Timing log saved: {self.csv_path}")
            
    def write_summary(self, compile_time, backtest_time, stability_time, cache_hits, cache_misses, dropped_deals):
        """Write summary.txt."""
        summary_path = os.path.join(self.logs_dir, "summary.txt")
        with open(summary_path, "w") as f:
            f.write(f"Compile time: {compile_time:.1f}s\n")
            f.write(f"Backtest time: {backtest_time:.1f}s\n")
            f.write(f"Stability suite time: {stability_time:.1f}s\n")
            f.write(f"Cache hits: {cache_hits}\n")
            f.write(f"Cache misses: {cache_misses}\n")
            f.write(f"Dropped deals (missing CIK): {dropped_deals}\n")

TIMING_LOGGER = TimingLogger(ARTIFACT_DIR)

# ================================================
# TRAIN_EVAL_ONE: Fast array-only training
# ================================================

@dataclass
class TrainTask:
    """Specification for a single train/eval task."""
    quarter: str
    horizon_months: int
    seed: int = 42
    config_id: str = "default"
    hp_config: Dict = field(default_factory=dict)
    
@dataclass 
class FitResult:
    """Result from a single train/eval run."""
    predictions: np.ndarray              # p_raw
    predictions_calibrated: np.ndarray = None   # p_calibrated (isotonic)
    predictions_rescaled: np.ndarray = None     # p_rescaled (empirical prior)
    metrics: Dict[str, float] = None
    timing: Dict[str, float] = None
    memory: Dict[str, float] = None
    n_train: int = 0
    n_test: int = 0

def train_eval_one(task: TrainTask, artifacts: CompileArtifacts, 
                   sampling_config: Dict = None) -> FitResult:
    """
    Fast train/eval for a single (quarter, horizon, seed, hp_config).
    
    CRITICAL: This function uses ONLY array slicing. No Pandas operations.
    Target: â‰¤60 seconds with sampling enabled.
    """
    from sklearn.metrics import roc_auc_score, brier_score_loss, precision_recall_curve, auc
    
    t_start = time.time()
    rss_before = get_rss_mb()
    
    # 1. Slice by precomputed indices (FAST - no Pandas)
    t_slice_start = time.time()
    train_idx = artifacts.rows_train_by_quarter.get(task.quarter, np.array([], dtype=np.int32))
    test_idx = artifacts.rows_test_by_quarter.get(task.quarter, np.array([], dtype=np.int32))
    
    if len(train_idx) == 0 or len(test_idx) == 0:
        return FitResult(
            predictions=np.array([]),
            metrics={"error": "empty_split"},
            timing={"total_seconds": time.time() - t_start},
            memory={"rss_before_mb": rss_before, "rss_after_mb": get_rss_mb()},
            n_train=0, n_test=0
        )
    
    # Get label column
    y_full = artifacts.y_by_horizon.get(task.horizon_months)
    if y_full is None:
        return FitResult(
            predictions=np.array([]),
            metrics={"error": "missing_horizon"},
            timing={"total_seconds": time.time() - t_start},
            memory={"rss_before_mb": rss_before, "rss_after_mb": get_rss_mb()},
            n_train=0, n_test=0
        )
    
    # Slice arrays
    X_train = artifacts.X_global[train_idx]
    y_train = y_full[train_idx]
    X_test = artifacts.X_global[test_idx]
    y_test = y_full[test_idx]
    
    t_slice = time.time() - t_slice_start
    
    # 2. Apply sampling (optional, for speed)
    t_sample_start = time.time()
    sample_weight = None
    if sampling_config and sampling_config.get("enabled", False):
        pos_mask = y_train == 1
        neg_mask = y_train == 0
        n_pos = pos_mask.sum()
        n_neg = neg_mask.sum()
        
        if n_pos > 0 and n_neg > n_pos:
            ratio = sampling_config.get("neg_pos_ratio_dev", 10)
            n_neg_sample = min(n_neg, n_pos * ratio)
            
            rng = np.random.default_rng(task.seed)
            neg_idx = np.where(neg_mask)[0]
            neg_sample_idx = rng.choice(neg_idx, size=int(n_neg_sample), replace=False)
            pos_idx = np.where(pos_mask)[0]
            
            sample_idx = np.concatenate([pos_idx, neg_sample_idx])
            rng.shuffle(sample_idx)
            
            X_train = X_train[sample_idx]
            y_train = y_train[sample_idx]
            
            # Compute sample weights to correct for sampling bias
            if sampling_config.get("use_sample_weights", True):
                # Weight negatives higher to account for undersampling
                true_neg_ratio = n_neg / (n_pos + n_neg)
                sample_neg_ratio = n_neg_sample / (n_pos + n_neg_sample)
                neg_weight = true_neg_ratio / sample_neg_ratio
                sample_weight = np.where(y_train == 1, 1.0, neg_weight)
    
    t_sample = time.time() - t_sample_start
    
    # 3. Train model
    t_fit_start = time.time()
    hp = task.hp_config or {}
    
    if HAS_LIGHTGBM:
        params = {
            "objective": "binary",
            "metric": "auc",
            "verbosity": -1,
            "seed": task.seed,
            "num_leaves": hp.get("num_leaves", 31),
            "min_data_in_leaf": hp.get("min_data_in_leaf", 20),
            "feature_fraction": hp.get("feature_fraction", 0.8),
            "bagging_fraction": hp.get("bagging_fraction", 0.8),
            "bagging_freq": hp.get("bagging_freq", 5),
            "lambda_l2": hp.get("lambda_l2", 0.1),
            "learning_rate": hp.get("learning_rate", 0.05),
            "n_estimators": hp.get("n_estimators", 200),
        }
        
        train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weight, free_raw_data=False)
        
        # Early stopping with 10% validation split from training
        n_val = max(100, int(len(X_train) * 0.1))
        val_data = lgb.Dataset(X_train[-n_val:], label=y_train[-n_val:], reference=train_data)
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=params.pop("n_estimators"),
            valid_sets=[val_data],
            callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]
        )
        
        t_fit = time.time() - t_fit_start
        
        # 4. Predict
        t_predict_start = time.time()
        predictions = model.predict(X_test)
        t_predict = time.time() - t_predict_start
        
    else:
        # Fallback to sklearn
        from sklearn.ensemble import HistGradientBoostingClassifier
        
        model = HistGradientBoostingClassifier(
            learning_rate=hp.get("learning_rate", 0.05),
            max_depth=hp.get("max_depth", 5),
            max_iter=hp.get("n_estimators", 200),
            random_state=task.seed,
            early_stopping=True,
            validation_fraction=0.1,
            n_iter_no_change=20
        )
        
        model.fit(X_train, y_train, sample_weight=sample_weight)
        t_fit = time.time() - t_fit_start
        
        t_predict_start = time.time()
        predictions = model.predict_proba(X_test)[:, 1]
        t_predict = time.time() - t_predict_start
    
    # 5. Compute metrics
    t_eval_start = time.time()
    metrics = {}
    
    try:
        metrics["auc"] = roc_auc_score(y_test, predictions)
    except:
        metrics["auc"] = 0.5
    
    try:
        metrics["brier"] = brier_score_loss(y_test, predictions)
    except:
        metrics["brier"] = 0.25
    
    try:
        precision, recall, _ = precision_recall_curve(y_test, predictions)
        metrics["pr_auc"] = auc(recall, precision)
    except:
        metrics["pr_auc"] = 0.0
    
    metrics["n_positives_train"] = int(y_train.sum())
    metrics["n_positives_test"] = int(y_test.sum())
    metrics["base_rate_test"] = float(y_test.mean())
    
    t_eval = time.time() - t_eval_start
    rss_after = get_rss_mb()
    
    # Log timing
    TIMING_LOGGER.log_phase(
        "train_eval",
        quarter=task.quarter,
        horizon_mo=task.horizon_months,
        repeat_id=task.seed,
        config_id=task.config_id,
        n_train_rows=len(train_idx),
        n_test_rows=len(test_idx),
        n_features=artifacts.n_features,
        slice_seconds=t_slice,
        fit_seconds=t_fit,
        predict_seconds=t_predict,
        eval_seconds=t_eval,
        total_seconds=time.time() - t_start,
        rss_before_mb=rss_before,
        rss_after_mb=rss_after
    )
    
    # 6. Calibration (Isotonic regression on train data)
    predictions_calibrated = None
    try:
        from sklearn.isotonic import IsotonicRegression
        # Fit isotonic regression on train predictions vs actuals
        if len(y_train) > 50:
            ir = IsotonicRegression(out_of_bounds='clip')
            # Get predictions on training data
            if HAS_LIGHTGBM:
                train_preds = model.predict(X_train)
            else:
                train_preds = model.predict_proba(X_train)[:, 1]
            ir.fit(train_preds, y_train)
            predictions_calibrated = ir.transform(predictions)
    except Exception as e:
        log(f"  [WARN] Calibration failed: {e}")
        predictions_calibrated = predictions  # Fallback to raw
    
    # 7. Rescaling (empirical prior adjustment for strategy alignment)
    # Rescale predictions to match realized deal rates
    predictions_rescaled = None
    try:
        base_rate_train = y_train.mean()
        base_rate_pop = 0.02  # Approximate population base rate for M&A deals
        if base_rate_train > 0 and base_rate_pop > 0:
            # Adjust for sampling bias in training data
            scale_factor = base_rate_pop / base_rate_train
            predictions_rescaled = predictions * scale_factor
            predictions_rescaled = np.clip(predictions_rescaled, 0, 1)
    except Exception as e:
        log(f"  [WARN] Rescaling failed: {e}")
        predictions_rescaled = predictions
    
    return FitResult(
        predictions=predictions,
        predictions_calibrated=predictions_calibrated,
        predictions_rescaled=predictions_rescaled,
        metrics=metrics,
        timing={
            "slice_seconds": t_slice,
            "sample_seconds": t_sample,
            "fit_seconds": t_fit,
            "predict_seconds": t_predict,
            "eval_seconds": t_eval,
            "total_seconds": time.time() - t_start
        },
        memory={"rss_before_mb": rss_before, "rss_after_mb": rss_after},
        n_train=len(X_train),
        n_test=len(X_test)
    )

# ================================================
# PORTFOLIO SIMULATION
# ================================================

def simulate_portfolio(predictions: np.ndarray, 
                       metadata_ret_fwd: np.ndarray,
                       strategy_config: Dict = None) -> Dict:
    """
    Simulate portfolio returns from predictions.
    
    Args:
        predictions: Model probability scores for test set
        metadata_ret_fwd: Forward returns aligned to test set
        strategy_config: Configuration dict with:
            - top_k: Number of holdings (default: 50)
            - weight_scheme: 'equal' or 'score_weighted'
            - clip_returns: (min, max) to clip outlier returns
    
    Returns:
        Dict with returns, exposures, turnover, concentration (HHI)
    """
    if strategy_config is None:
        strategy_config = {}
    
    top_k = strategy_config.get("top_k", 50)
    weight_scheme = strategy_config.get("weight_scheme", "equal")
    clip_min, clip_max = strategy_config.get("clip_returns", (-0.5, 1.0))
    
    if len(predictions) == 0 or metadata_ret_fwd is None:
        return {"error": "insufficient_data"}
    
    # Select top-K
    top_k_actual = min(top_k, len(predictions))
    top_k_idx = np.argsort(predictions)[-top_k_actual:]
    
    # Get returns for selected holdings
    selected_returns = metadata_ret_fwd[top_k_idx]
    selected_returns = np.clip(selected_returns, clip_min, clip_max)
    
    # Compute weights
    if weight_scheme == "score_weighted":
        scores = predictions[top_k_idx]
        weights = scores / scores.sum()
    else:  # equal
        weights = np.ones(len(top_k_idx)) / len(top_k_idx)
    
    # Portfolio return
    portfolio_return = np.sum(weights * selected_returns)
    
    # Concentration (HHI)
    hhi = np.sum(weights ** 2)
    
    # Universe return for comparison
    universe_return = np.nanmean(np.clip(metadata_ret_fwd, clip_min, clip_max))
    
    return {
        "portfolio_return": float(portfolio_return),
        "universe_return": float(universe_return),
        "excess_return": float(portfolio_return - universe_return),
        "n_holdings": int(top_k_actual),
        "hhi": float(hhi),
        "top_score_min": float(predictions[top_k_idx].min()),
        "top_score_max": float(predictions[top_k_idx].max()),
        "hit_rate": float(np.mean(selected_returns > 0)),
    }

# ================================================
# ONE-SHOT AUTOML WITH HALVING
# ================================================

def generate_hp_configs(n_configs, seed=42):
    """Generate random hyperparameter configurations for LightGBM."""
    rng = np.random.default_rng(seed)
    configs = []
    
    for i in range(n_configs):
        configs.append({
            "config_id": f"hp_{i}",
            "num_leaves": int(rng.choice([15, 31, 63, 127])),
            "min_data_in_leaf": int(rng.choice([10, 20, 50, 100])),
            "feature_fraction": float(rng.uniform(0.6, 1.0)),
            "bagging_fraction": float(rng.uniform(0.6, 1.0)),
            "lambda_l2": float(rng.choice([0.0, 0.1, 1.0, 10.0])),
            "learning_rate": float(rng.choice([0.01, 0.03, 0.05, 0.1])),
            "n_estimators": 100,  # Small budget for initial round
        })
    
    return configs

def run_one_shot_automl(quarter, horizon, artifacts, automl_config, sampling_config):
    """
    One-shot AutoML with successive halving.
    
    1. Start with n_configs random HP configs at small budget
    2. Keep top_k_advance best configs
    3. Train top configs to full early stopping
    4. Return best config + trial table
    """
    log(f"ðŸ” Running One-Shot AutoML: {quarter} horizon={horizon}m")
    t_start = time.time()
    
    n_configs = automl_config.get("n_configs", 8)
    budget_small = automl_config.get("budget_small", 100)
    top_k = automl_config.get("top_k_advance", 2)
    
    # Round 1: Small budget evaluation
    configs = generate_hp_configs(n_configs)
    round1_results = []
    
    for cfg in configs:
        cfg["n_estimators"] = budget_small
        task = TrainTask(quarter=quarter, horizon_months=horizon, seed=42, 
                        config_id=cfg["config_id"], hp_config=cfg)
        result = train_eval_one(task, artifacts, sampling_config)
        round1_results.append({
            "config": cfg,
            "auc": result.metrics.get("auc", 0.5),
            "fit_seconds": result.timing.get("fit_seconds", 0)
        })
    
    # Sort by AUC descending
    round1_results.sort(key=lambda x: x["auc"], reverse=True)
    top_configs = [r["config"] for r in round1_results[:top_k]]
    
    log(f"  Round 1: Top {top_k} of {n_configs} configs selected (best AUC={round1_results[0]['auc']:.3f})")
    
    # Round 2: Full training with early stopping
    round2_results = []
    for cfg in top_configs:
        cfg["n_estimators"] = 500  # Full budget
        task = TrainTask(quarter=quarter, horizon_months=horizon, seed=42,
                        config_id=cfg["config_id"], hp_config=cfg)
        result = train_eval_one(task, artifacts, sampling_config)
        round2_results.append({
            "config": cfg,
            "auc": result.metrics.get("auc", 0.5),
            "predictions": result.predictions
        })
    
    # Select best
    round2_results.sort(key=lambda x: x["auc"], reverse=True)
    best = round2_results[0]
    
    elapsed = time.time() - t_start
    log(f"  AutoML complete: best AUC={best['auc']:.3f} in {elapsed:.1f}s")
    
    TIMING_LOGGER.log_phase(
        "automl_trial",
        quarter=quarter,
        horizon_mo=horizon,
        n_configs=n_configs,
        top_k=top_k,
        best_auc=best["auc"],
        total_seconds=elapsed
    )
    
    return best["config"], best["predictions"], round1_results + round2_results

# ================================================
# STABILITY SUITE
# ================================================

def compute_jaccard(set1, set2):
    """Compute Jaccard similarity between two sets."""
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0.0

def run_stability_suite(quarter, horizon, artifacts, stability_config, sampling_config):
    """
    Run stability certification suite with sequential stopping.
    
    Measures:
    - Rank stability (Spearman, Jaccard@K)
    - Probability stability (per-name std, distribution metrics)
    - Returns stability (portfolio return distribution)
    
    Sequential stopping: Start R=5, increase to R=10/20 if CI thresholds not met.
    """
    log(f"ðŸ“Š Running Stability Suite: {quarter} horizon={horizon}m")
    
    min_repeats = stability_config.get("min_repeats", 5)
    max_repeats = stability_config.get("max_repeats", 20)
    top_k = stability_config.get("top_k", 50)
    jaccard_threshold = stability_config.get("jaccard_ci_threshold", 0.10)
    return_threshold = stability_config.get("return_ci_threshold", 0.02)
    
    test_idx = artifacts.rows_test_by_quarter.get(quarter, np.array([]))
    if len(test_idx) == 0:
        return {"error": "no_test_data"}
    
    # Get forward returns for portfolio simulation
    ret_fwd = artifacts.metadata_ret_fwd[test_idx] if artifacts.metadata_ret_fwd is not None else None
    
    all_predictions = []
    all_top_k_sets = []
    all_portfolio_returns = []
    
    def check_convergence(jaccards, returns):
        """Check if CI half-widths are below thresholds."""
        if len(jaccards) < 3:
            return False
        
        jaccard_ci = 1.96 * np.std(jaccards) / np.sqrt(len(jaccards))
        
        if len(returns) > 0 and not all(np.isnan(returns)):
            valid_returns = [r for r in returns if not np.isnan(r)]
            if len(valid_returns) >= 3:
                return_ci = 1.96 * np.std(valid_returns) / np.sqrt(len(valid_returns))
            else:
                return_ci = float('inf')
        else:
            return_ci = 0  # No returns to check
        
        return jaccard_ci <= jaccard_threshold and return_ci <= return_threshold
    
    for r in range(max_repeats):
        seed = 42 + r
        task = TrainTask(quarter=quarter, horizon_months=horizon, seed=seed, config_id=f"repeat_{r}")
        result = train_eval_one(task, artifacts, sampling_config)
        
        if len(result.predictions) == 0:
            continue
        
        all_predictions.append(result.predictions)
        
        # Top-K selection
        top_k_idx = np.argsort(result.predictions)[-top_k:]
        top_k_set = set(test_idx[top_k_idx])
        all_top_k_sets.append(top_k_set)
        
        # Portfolio return
        if ret_fwd is not None:
            top_k_returns = ret_fwd[top_k_idx]
            port_ret = np.nanmean(np.clip(top_k_returns, -0.5, 1.0))
            all_portfolio_returns.append(port_ret)
        
        # Compute pairwise Jaccard for current repeats
        jaccards = []
        for i in range(len(all_top_k_sets)):
            for j in range(i+1, len(all_top_k_sets)):
                jaccards.append(compute_jaccard(all_top_k_sets[i], all_top_k_sets[j]))
        
        # Check sequential stopping
        if r >= min_repeats - 1 and check_convergence(jaccards, all_portfolio_returns):
            log(f"  Converged at R={r+1} repeats")
            break
    
    # Final stability metrics
    n_repeats = len(all_predictions)
    
    # Jaccard metrics
    all_jaccards = []
    for i in range(n_repeats):
        for j in range(i+1, n_repeats):
            all_jaccards.append(compute_jaccard(all_top_k_sets[i], all_top_k_sets[j]))
    
    mean_jaccard = np.mean(all_jaccards) if all_jaccards else 0.0
    std_jaccard = np.std(all_jaccards) if all_jaccards else 0.0
    
    # Probability stability (per-position std across repeats)
    if n_repeats >= 2:
        pred_matrix = np.vstack(all_predictions)
        prob_std_per_name = pred_matrix.std(axis=0)
        mean_prob_std = prob_std_per_name.mean()
        
        # KS statistic between first and last repeat distributions
        ks_stat, ks_pval = stats.ks_2samp(all_predictions[0], all_predictions[-1])
        
        # PSI (Population Stability Index) - binned distribution comparison
        def compute_psi(expected, actual, bins=10):
            """Compute PSI between two probability distributions."""
            min_val, max_val = 0.0, 1.0
            bin_edges = np.linspace(min_val, max_val, bins + 1)
            exp_counts = np.histogram(expected, bins=bin_edges)[0] + 1  # Add 1 to avoid log(0)
            act_counts = np.histogram(actual, bins=bin_edges)[0] + 1
            exp_pct = exp_counts / exp_counts.sum()
            act_pct = act_counts / act_counts.sum()
            psi = np.sum((act_pct - exp_pct) * np.log(act_pct / exp_pct))
            return psi
        
        psi_value = compute_psi(all_predictions[0], all_predictions[-1])
        
        # ECE (Expected Calibration Error) - uses first repeat's predictions
        def compute_ece(probs, labels, n_bins=10):
            """Compute Expected Calibration Error."""
            bin_edges = np.linspace(0, 1, n_bins + 1)
            ece = 0.0
            for i in range(n_bins):
                mask = (probs >= bin_edges[i]) & (probs < bin_edges[i+1])
                if mask.sum() > 0:
                    bin_prob = probs[mask].mean()
                    bin_acc = labels[mask].mean()
                    ece += mask.sum() * abs(bin_prob - bin_acc)
            return ece / len(probs)
        
        # Get labels for ECE
        y = artifacts.y_by_horizon.get(horizon, np.array([]))
        if len(y) > 0:
            y_test = y[test_idx]
            ece_value = compute_ece(all_predictions[0], y_test)
        else:
            ece_value = np.nan
    else:
        mean_prob_std = 0.0
        ks_stat, ks_pval = 0.0, 1.0
        psi_value = 0.0
        ece_value = np.nan
    
    # Rank stability (Spearman correlation between first and last repeat)
    if n_repeats >= 2:
        spearman_corr, _ = stats.spearmanr(all_predictions[0], all_predictions[-1])
    else:
        spearman_corr = 1.0
    
    # Returns stability + Path metrics
    if all_portfolio_returns:
        mean_port_return = np.mean(all_portfolio_returns)
        std_port_return = np.std(all_portfolio_returns)
        
        # Sharpe ratio (annualized from quarterly)
        if std_port_return > 0:
            sharpe_ratio = (mean_port_return * 4) / (std_port_return * 2)  # Annualize
        else:
            sharpe_ratio = np.nan
        
        # Max drawdown (across repeats' cumulative paths)
        cum_returns = np.cumprod(1 + np.array(all_portfolio_returns))
        running_max = np.maximum.accumulate(cum_returns)
        drawdowns = (cum_returns - running_max) / running_max
        max_drawdown = drawdowns.min() if len(drawdowns) > 0 else 0.0
        
        # Hit rate (% of positive returns)
        hit_rate = np.mean(np.array(all_portfolio_returns) > 0)
    else:
        mean_port_return = np.nan
        std_port_return = np.nan
        sharpe_ratio = np.nan
        max_drawdown = np.nan
        hit_rate = np.nan
    
    report = {
        "quarter": quarter,
        "horizon": horizon,
        "n_repeats": n_repeats,
        # Rank stability
        "mean_jaccard_at_k": mean_jaccard,
        "std_jaccard_at_k": std_jaccard,
        "jaccard_ci_half_width": 1.96 * std_jaccard / np.sqrt(max(1, len(all_jaccards))),
        "spearman_first_last": spearman_corr,
        # Probability stability
        "mean_prob_std": mean_prob_std,
        "ks_statistic": ks_stat,
        "ks_pvalue": ks_pval,
        "psi": psi_value,
        "ece": ece_value,
        # Returns / Strategy stability
        "mean_portfolio_return": mean_port_return,
        "std_portfolio_return": std_port_return,
        "return_ci_half_width": 1.96 * std_port_return / np.sqrt(max(1, n_repeats)) if not np.isnan(std_port_return) else np.nan,
        "sharpe_ratio": sharpe_ratio,
        "max_drawdown": max_drawdown,
        "hit_rate": hit_rate,
    }
    
    log(f"  Stability: Jaccard@{top_k}={mean_jaccard:.2%}, Spearman={spearman_corr:.3f}, "
        f"KS={ks_stat:.3f}, PSI={psi_value:.3f}, ECE={ece_value:.3f if not np.isnan(ece_value) else 'N/A'}")
    log(f"  Returns: Mean={mean_port_return:.2%}, Sharpe={sharpe_ratio:.2f if not np.isnan(sharpe_ratio) else 'N/A'}, "
        f"MaxDD={max_drawdown:.2%}, HitRate={hit_rate:.1%}")
    
    TIMING_LOGGER.log_phase(
        "stability_aggregate",
        quarter=quarter,
        horizon_mo=horizon,
        n_repeats=n_repeats,
        mean_jaccard=mean_jaccard,
        spearman=spearman_corr,
        ks_stat=ks_stat,
        psi=psi_value,
        ece=ece_value,
        sharpe=sharpe_ratio
    )
    
    return report

# ================================================
# SPY BENCHMARK (ROBUST)
# ================================================

def get_spy_quarterly_returns(start_date, end_date, fail_fast=True):
    """
    Get SPY quarterly returns with validation.
    
    If any quarter is missing/NaN and fail_fast=True, raises error.
    """
    log("Fetching SPY quarterly returns...")
    
    try:
        spy = yf.download("SPY", start=start_date, end=end_date, progress=False)
        
        if spy.empty:
            if fail_fast:
                raise ValueError("SPY data download returned empty DataFrame")
            return {}
        
        # Handle multi-level columns from yfinance
        if isinstance(spy.columns, pd.MultiIndex):
            spy.columns = spy.columns.get_level_values(0)
        
        # Resample to quarterly
        spy_q = spy["Close"].resample("QE").last().pct_change()
        spy_q.index = spy_q.index.to_period("Q")
        spy_map = spy_q.to_dict()
        
        # Validate no NaN
        nan_quarters = [q for q, v in spy_map.items() if pd.isna(v)]
        if nan_quarters and fail_fast:
            raise ValueError(f"SPY returns missing for quarters: {nan_quarters[:5]}...")
        
        log(f"  Loaded SPY returns for {len(spy_map)} quarters")
        return spy_map
        
    except Exception as e:
        if fail_fast:
            raise ValueError(f"SPY benchmark failed: {e}")
        log(f"[WARN] SPY fetch failed: {e}")
        return {}

log("âœ… Cell A1 Complete: Compile-Once Infrastructure Defined.")

# %% [markdown]
# # Cell A2: Extend Deals Data (FactSet Batches 2000-2025)
#
# **Goal**: Load all 7 FactSet XLS batches and consolidate into a unified deals dataset.
# This extends coverage beyond the 2020 cutoff in dma_corpus_metadata.
# Schema is preserved to match existing pipeline expectations.

# %%
def load_extended_deals(config):
    """
    Load deals from two sources:
    1. dma_corpus_metadata_with_factset_id.csv (2000-2020, richer text data)
    2. FactSet XLS batches (2000-2025, for extending labels to 2021+)
    
    Returns a unified DataFrame with consistent schema.
    """
    import glob
    
    # --- Source 1: DMA Corpus (primary, has text excerpts) ---
    dma_path = resolve_path(config["inputs"]["deals"], prefer_repo=True)
    if dma_path and os.path.exists(dma_path):
        log(f"Loading DMA corpus from {dma_path}...")
        dma_df = pd.read_csv(dma_path, sep="|", low_memory=False)
        dma_df["source"] = "dma_corpus"
        log(f"  DMA corpus: {len(dma_df)} deals, years {dma_df['year'].min()}-{dma_df['year'].max()}")
    else:
        dma_df = pd.DataFrame()
        log("[WARN] DMA corpus not found.")
    
    # --- Source 2: FactSet XLS Batches (for 2021+ extension) ---
    xls_dir = resolve_path(config["inputs"]["factset_xls_dir"], prefer_repo=True)
    xls_dfs = []
    
    if xls_dir and os.path.isdir(xls_dir):
        log(f"Loading FactSet XLS batches from {xls_dir}...")
        xls_files = sorted(glob.glob(os.path.join(xls_dir, "*.xls")))
        
        for xls_file in xls_files:
            try:
                batch_df = pd.read_excel(xls_file)
                batch_df["source"] = os.path.basename(xls_file)
                xls_dfs.append(batch_df)
                log(f"  Loaded {os.path.basename(xls_file)}: {len(batch_df)} deals")
            except Exception as e:
                log(f"  [WARN] Failed to load {xls_file}: {e}")
        
        if xls_dfs:
            factset_df = pd.concat(xls_dfs, ignore_index=True)
            log(f"  Total FactSet XLS: {len(factset_df)} deals")
        else:
            factset_df = pd.DataFrame()
    else:
        factset_df = pd.DataFrame()
        log("[WARN] FactSet XLS directory not found.")
    
    # --- Standardize FactSet XLS schema to match DMA corpus ---
    if not factset_df.empty:
        # Debug: show actual column names from XLS files
        log(f"  FactSet columns: {list(factset_df.columns)[:10]}...")
        
        # Flexible column mapping (case-insensitive matching)
        col_lower_map = {c.lower(): c for c in factset_df.columns}
        
        # Find target column
        for key in ["target", "target name", "targetname", "target company"]:
            if key in col_lower_map:
                factset_df = factset_df.rename(columns={col_lower_map[key]: "target"})
                break
        
        # Find acquirer column
        for key in ["acquirer", "acquirer name", "acquirername", "buyer"]:
            if key in col_lower_map:
                factset_df = factset_df.rename(columns={col_lower_map[key]: "acquirer"})
                break
        
        # Find date column (for year extraction)
        date_col = None
        for key in ["announce date", "announced", "ann date", "announcement date", "date announced", "date"]:
            if key in col_lower_map:
                date_col = col_lower_map[key]
                break
        
        if date_col:
            factset_df["date_announcement"] = pd.to_datetime(factset_df[date_col], errors="coerce")
            factset_df["year"] = factset_df["date_announcement"].dt.year
            log(f"  Extracted year from '{date_col}', range: {factset_df['year'].min()}-{factset_df['year'].max()}")
        else:
            # Fallback: try to infer from any datetime column
            for col in factset_df.columns:
                try:
                    parsed = pd.to_datetime(factset_df[col], errors="coerce")
                    if parsed.notna().sum() > len(factset_df) * 0.5:  # >50% valid dates
                        factset_df["date_announcement"] = parsed
                        factset_df["year"] = parsed.dt.year
                        log(f"  Inferred year from '{col}', range: {factset_df['year'].min()}-{factset_df['year'].max()}")
                        break
                except:
                    pass
            else:
                log("  [WARN] Could not extract year from FactSet XLS - no date column found")
    
    # --- Merge: DMA corpus + NEW deals from FactSet (2021+) ---
    if not dma_df.empty and not factset_df.empty and "year" in factset_df.columns:
        # Only add FactSet deals that are AFTER DMA corpus coverage
        dma_max_year = dma_df["year"].max()
        new_deals = factset_df[factset_df["year"] > dma_max_year].copy()
        log(f"  Extending with {len(new_deals)} new deals from {dma_max_year+1}+")
        
        # Align columns (keep only columns present in DMA corpus)
        common_cols = [c for c in dma_df.columns if c in new_deals.columns]
        if common_cols:
            new_deals = new_deals[common_cols]
            combined_df = pd.concat([dma_df, new_deals], ignore_index=True)
        else:
            log("[WARN] No common columns between DMA and FactSet. Using DMA only.")
            combined_df = dma_df
    elif not dma_df.empty:
        combined_df = dma_df
    elif not factset_df.empty:
        combined_df = factset_df
    else:
        combined_df = pd.DataFrame()
        log("[ERROR] No deals data loaded!")
    
    # --- Final summary ---
    if not combined_df.empty:
        log(f"âœ… Extended deals dataset: {len(combined_df)} deals, years {combined_df['year'].min()}-{combined_df['year'].max()}")
        
        # Show company names if available
        if "target" in combined_df.columns:
            log(f"  Sample targets: {combined_df['target'].dropna().head(3).tolist()}")
    
    return combined_df

# Load extended deals
DEALS_DF = load_extended_deals(CONFIG)
log("âœ… Cell A2 Complete: Extended Deals Loaded.")

# %% [markdown]
# # Cell A3: Extend Fundamentals Data (Schema-Preserving)
#
# **Goal**: Extend `fundq_full.parquet` and `funda_full.parquet` using available CSV data.
# The existing parquet schema is treated as ground truth - new data must match exactly.

# %%
def extend_fundamentals(config, save_extended=False):
    """
    Extend fundq/funda parquet files with newer data from CSVs.
    Schema is determined by existing parquet - only matching columns are appended.
    
    Available extension sources:
    - compustat_funda_2000on.csv (annual fundamentals)
    
    Args:
        config: Pipeline config dict
        save_extended: If True, saves extended parquet back to Drive
    
    Returns:
        fundq_df, funda_df: Extended DataFrames (or originals if no extension possible)
    """
    results = {}
    
    for data_type, parquet_key, csv_candidates in [
        ("quarterly", "fundq", []),  # No quarterly CSV currently available
        ("annual", "funda", [config["inputs"].get("fundamentals_csv", "data/fundamentals/compustat_funda_2000on.csv")]),
    ]:
        parquet_file = config["inputs"][parquet_key]
        parquet_path = os.path.join(config["drive_dir"], parquet_file)
        
        log(f"\n--- {data_type.upper()} Fundamentals ({parquet_key}) ---")
        
        # 1. Load existing parquet and extract schema
        if os.path.exists(parquet_path):
            existing_df = pd.read_parquet(parquet_path)
            schema_cols = list(existing_df.columns)
            schema_dtypes = existing_df.dtypes.to_dict()
            
            log(f"Loaded {parquet_file}: {len(existing_df)} rows")
            log(f"Schema: {len(schema_cols)} columns")
            log(f"Date range: {existing_df['datadate'].min()} to {existing_df['datadate'].max()}")
            
            # Show sample companies if available
            if "conm" in existing_df.columns:
                log(f"Sample companies: {existing_df['conm'].dropna().head(3).tolist()}")
        else:
            log(f"[WARN] {parquet_file} not found at {parquet_path}")
            results[data_type] = pd.DataFrame()
            continue
        
        # 2. Look for extension CSVs
        extended_df = existing_df.copy()
        max_existing_date = pd.to_datetime(existing_df["datadate"]).max()
        
        for csv_name in csv_candidates:
            csv_path = resolve_path(csv_name, prefer_repo=True)
            if not csv_path or not os.path.exists(csv_path):
                continue
            
            log(f"Checking extension source: {csv_name}...")
            new_df = pd.read_csv(csv_path, low_memory=False)
            new_df["datadate"] = pd.to_datetime(new_df["datadate"], errors="coerce")
            
            # Only keep rows AFTER existing data
            new_df = new_df[new_df["datadate"] > max_existing_date]
            
            if new_df.empty:
                log(f"  No new rows after {max_existing_date.date()}")
                continue
            
            log(f"  Found {len(new_df)} new rows after {max_existing_date.date()}")
            
            # 3. Align to existing schema (drop extra cols, add missing as NaN)
            common_cols = [c for c in schema_cols if c in new_df.columns]
            missing_cols = [c for c in schema_cols if c not in new_df.columns]
            
            log(f"  Common columns: {len(common_cols)}/{len(schema_cols)}")
            if missing_cols:
                log(f"  Missing columns (will be NaN): {missing_cols[:5]}{'...' if len(missing_cols) > 5 else ''}")
            
            # Subset to common columns, add missing as NaN
            new_df_aligned = new_df[common_cols].copy()
            for col in missing_cols:
                new_df_aligned[col] = np.nan
            
            # Reorder to match schema
            new_df_aligned = new_df_aligned[schema_cols]
            
            # Cast dtypes to match
            for col, dtype in schema_dtypes.items():
                try:
                    new_df_aligned[col] = new_df_aligned[col].astype(dtype)
                except (ValueError, TypeError):
                    pass  # Keep as-is if cast fails
            
            # 4. Append
            extended_df = pd.concat([extended_df, new_df_aligned], ignore_index=True)
            log(f"  Extended to {len(extended_df)} total rows")
        
        # 5. Optionally save
        if save_extended and len(extended_df) > len(existing_df):
            save_path = parquet_path.replace(".parquet", "_extended.parquet")
            extended_df.to_parquet(save_path, index=False)
            log(f"  Saved extended data to {save_path}")
        
        results[data_type] = extended_df
    
    return results.get("quarterly", pd.DataFrame()), results.get("annual", pd.DataFrame())

# Run extension check (don't save by default - just report)
FUNDQ_EXTENDED, FUNDA_EXTENDED = extend_fundamentals(CONFIG, save_extended=False)
log("âœ… Cell A3 Complete: Fundamentals Extension Check.")

# %% [markdown]
# # Cell B: Load & Clean Panel (Truncation Diagnostics)
#
# **Goal**: Load Compustat data, apply 'STD' filter, and diagnose year coverage to ensure no data is silently dropped.


# %%
def load_and_prep_compustat(config):
    # Large parquet files -> Google Drive only (not on GitHub)
    fundq_path = os.path.join(config["drive_dir"], config["inputs"]["fundq"])
    funda_path = os.path.join(config["drive_dir"], config["inputs"]["funda"])

    # Load
    log(f"Loading {fundq_path}...")
    if os.path.exists(fundq_path):
        q_df = pd.read_parquet(fundq_path)
        q_df["freq"] = "Q"
        log(f"Loaded Quarterly: {len(q_df)} rows. Date Range: {q_df['datadate'].min()} to {q_df['datadate'].max()}")
    else:
        q_df = pd.DataFrame()
        log("[WARN] Quarterly data missing.")

    log(f"Loading {funda_path}...")
    if os.path.exists(funda_path):
        a_df = pd.read_parquet(funda_path)
        a_df["freq"] = "A"
        log(f"Loaded Annual: {len(a_df)} rows. Date Range: {a_df['datadate'].min()} to {a_df['datadate'].max()}")
    else:
        a_df = pd.DataFrame()
        log("[WARN] Annual data missing.")

    df = pd.concat([q_df, a_df], ignore_index=True, sort=False)
    df["datadate"] = pd.to_datetime(df["datadate"], errors="coerce")
    
    # --- DIAGNOSTIC 1: Raw Year Counts ---
    df["year"] = df["datadate"].dt.year
    log("Raw Data Year Counts (Top 5 recent):")
    print(df["year"].value_counts().sort_index(ascending=False).head(5))

    # --- FILTER: Keyset (STD) ---
    keyset_col = next((c for c in df.columns if c.lower() == "keyset"), None)
    if keyset_col:
        # Check unique keysets before filtering
        log(f"Unique keysets found: {df[keyset_col].unique()}")
        
        before_len = len(df)
        target_keyset = config["filters"]["keyset"]
        
        # Keep STD, Drop PRE/PFO/etc. explicitly if needed, but usually strictly STD is safe
        df = df[df[keyset_col].astype(str).str.upper() == target_keyset].copy()
        
        dropped = before_len - len(df)
        log(f"Filtered for {keyset_col}={target_keyset}. Dropped {dropped} rows ({dropped/before_len:.1%}).")
        
        # --- DIAGNOSTIC 2: Post-Filter Year Counts ---
        log("Post-Filter Year Counts (Top 5 recent):")
        print(df["year"].value_counts().sort_index(ascending=False).head(5))
    else:
        log("[WARN] 'keyset' column not found. Skipping STD filter.")

    # --- PIT Risk Flag (AJEX/CFAC) Before Scrubbing ---
    log("Computing PIT Risk Flag...")
    adj_cols = [c for c in ["ajexq", "ajex", "cfacshr", "cfacpr"] if c in df.columns]
    if adj_cols:
        # Ensure numeric
        for c in adj_cols:
            df[c] = pd.to_numeric(df[c], errors="coerce")
            
        # Sort by Entity-Time
        df = df.sort_values(["gvkey", "datadate"]).reset_index(drop=True)
        
        same_firm = df["gvkey"] == df["gvkey"].shift(1)
        has_change = pd.Series(False, index=df.index)
        
        for c in adj_cols:
            curr = df[c].fillna(-9999)
            prev = df[c].shift(1).fillna(-9999)
            has_change |= ((curr != prev) & same_firm)
            
        # Cumsum: Once risky, always risky? Or just localized? 
        # Requirement: "Detect ANY change... Flag potential back-adjustment"
        # Usually we flag the whole firm history if unstable, or forward from change.
        # Here we flag forward from first change to be safe (conservative).
        
        # Use has_change directly - simpler and avoids NA issues
        df["_raw_change"] = has_change.fillna(False).astype(int)
        df["price_pit_risk_flag"] = df.groupby("gvkey")["_raw_change"].transform("cumsum").fillna(0).gt(0).astype(int)
        df.drop(columns=["_raw_change"], inplace=True)
    else:
        df["price_pit_risk_flag"] = 0

    # --- Scrub Columns ---
    # (Simple scrub of AJEX/CFAC to prevent leakage)
    drop_patterns = ["ajex", "cfac", "adjex"]
    to_drop = [c for c in df.columns if any(p in c.lower() for p in drop_patterns) and c != "price_pit_risk_flag"]
    if to_drop:
        df.drop(columns=to_drop, inplace=True)
        log(f"Scrubbed {len(to_drop)} adjustment columns.")

    # --- Snapshot Definition ---
    # Max(datadate + 90, rdq)
    if "rdq" in df.columns:
        df["rdq"] = pd.to_datetime(df["rdq"], errors="coerce")
        base_snap = df["datadate"] + pd.Timedelta(days=config["safe_lag_days"])
        df["snapshot_ts"] = base_snap
        
        # Update where rdq is valid and later
        mask_rdq = df["rdq"].notna()
        # elementwise max
        df.loc[mask_rdq, "snapshot_ts"] = pd.to_datetime(np.maximum(
            base_snap[mask_rdq].values, 
            df.loc[mask_rdq, "rdq"].values
        ))
    else:
        df["snapshot_ts"] = df["datadate"] + pd.Timedelta(days=config["safe_lag_days"])

    return df

# Check for checkpoint before running expensive computation
if checkpoint_exists("panel_clean"):
    panel_clean = load_checkpoint("panel_clean")
else:
    panel_clean = load_and_prep_compustat(CONFIG)
    save_checkpoint(panel_clean, "panel_clean")

log(f"âœ… Cell B Complete. Panel has {len(panel_clean)} rows.")

# %% [markdown]
# # Cell C: Load & Normalize Deals
#
# **Goal**: Load crude deal data, normalize columns, and handle date parsing robustly.

# %%
import csv

def load_deals_robust(config, preloaded_df=None, tic_map=None, name_map=None):
    """
    Robust deals loading with:
    1. Preloaded DF support (Cell A2).
    2. Ticker Mapping validation (FactSet rescue).
    3. Name Mapping validation (FactSet rescue fallback).
    4. Numeric CIK enforcement.
    5. Duplicate column prevention.
    6. Per-quarter dropped-deal tracking (R5.D compliance).
    
    Returns:
        Tuple[pd.DataFrame, Dict]: (deals_df, dropped_deals_stats)
        dropped_deals_stats has keys: 'by_quarter', 'total', 'coverage_by_quarter'
    """
    if preloaded_df is not None and not preloaded_df.empty:
        log(f"Using preloaded deals dataframe ({len(preloaded_df)} rows)...")
        df = preloaded_df.copy()
        
        # Ensure date_announcement is mapped to ann_date if not already
        if "date_announcement" in df.columns and "ann_date" not in df.columns:
            df["ann_date"] = df["date_announcement"]
        elif "Announcement Date" in df.columns and "ann_date" not in df.columns:
             df["ann_date"] = df["Announcement Date"]
             
    else:
        # Fallback: Load from CSV
        csv_path = resolve_path(config["inputs"]["deals"])
        if not csv_path or not os.path.exists(csv_path):
            log(f"[ERROR] Deal CSV not found.")
            return pd.DataFrame()

        # Sniff delimiter
        with open(csv_path, "r", encoding="utf-8", errors="replace") as f:
            sample = f.read(2048)
            try:
                dialect = csv.Sniffer().sniff(sample)
                sep = dialect.delimiter
            except:
                sep = ","
        
        # Load
        log(f"Loading deals with separator='{sep}'...")
        df = pd.read_csv(csv_path, sep=sep, engine="python")
    
    # --- Common Normalization ---
    
    # ALWAYS Normalize Cols (even for preloaded) to avoid case issues (CIK vs cik)
    col_map = {c: c.lower().strip().replace(" ", "").replace("_", "") for c in df.columns}
    rev_map = {v: k for k, v in col_map.items()} # normalized -> original
    
    rename_dict = {}
    
    # Only rename to 'ann_date' if we don't already have it
    has_ann_date = "ann_date" in col_map.values() or "dateannouncement" in col_map.values()
    # Actually, check if target col exists in current df
    target_exists = "ann_date" in df.columns
    
    if not target_exists:
        if "dateannouncement" in rev_map: 
             rename_dict[rev_map["dateannouncement"]] = "ann_date"
        elif "announcementdate" in rev_map:
             rename_dict[rev_map["announcementdate"]] = "ann_date"

    if "cik" in rev_map and "cik" not in df.columns: 
         rename_dict[rev_map["cik"]] = "cik"
    
    val_candidates = ["dealvalue", "transactionvalue"]
    val_orig = next((rev_map[v] for v in val_candidates if v in rev_map), None)
    if val_orig:
        rename_dict[val_orig] = "_deal_value_str"
    
    if rename_dict:
        df.rename(columns=rename_dict, inplace=True)

    # CIK extraction (Regex from URL) - Fallback
    if "cik" not in df.columns and "url" in df.columns:
        df["cik"] = df["url"].astype(str).str.extract(r'/data/(\d{1,10})/')

    # --- Ticker Mapping (Crucial for FactSet) ---
    if tic_map is not None:
        # Force CIK to numeric first (handle empty strings which are not NA)
        if "cik" in df.columns:
            df["cik"] = pd.to_numeric(df["cik"], errors="coerce")
        else:
            df["cik"] = np.nan
            
        # Debug: Show columns for diagnostics
        # log(f"DEBUG: Columns available for ticker rescue: {list(df.columns)}")

        # Candidate ticker columns (must check normalized names!)
        # We also check original 'Target Ticker' just in case normalization missed it (unlikely)
        # Normalized candidates: 'targetticker', 'ticker', 'symbol'
        tic_col = None
        candidates = ["targetticker", "ticker", "symbol", "target_ticker", "Target Ticker"]
        
        for cand in candidates:
            if cand in df.columns:
                tic_col = cand
                break
        
        if tic_col:
            mask_missing = df["cik"].isna()
            if mask_missing.sum() > 0:
                log(f"Mapping {mask_missing.sum()} missing CIKs using ticker col '{tic_col}'...")
                clean_tics = df.loc[mask_missing, tic_col].astype(str).str.upper().str.strip()
                df.loc[mask_missing, "cik"] = clean_tics.map(tic_map)
                log(f"  Filled {df.loc[mask_missing, 'cik'].notna().sum()} CIKs.")
        else:
            log(f"[WARN] No Ticker column found in FactSet data. Available cols: {list(df.columns)}")

    # --- Name Mapping (Fallback) ---
    if name_map is not None:
         mask_missing = df["cik"].isna()
         if mask_missing.sum() > 0:
             # Find target name column
             name_col = None
             for cand in ["target", "targetname", "target_name", "conm"]:
                 if cand in df.columns:
                     name_col = cand
                     break
             
             if name_col:
                 log(f"Attempting Name Map on {mask_missing.sum()} missing CIKs using col '{name_col}'...")
                 # Normalize target name
                 # clean: lower, remove punctuation, remove common legal suffixes
                 s = df.loc[mask_missing, name_col].astype(str).str.lower()
                 s = s.str.replace(r"[^\w\s]", "", regex=True) # remove punctuation
                 s = s.str.replace(r"\b(inc|corp|ltd|llc|co|plc|nv|sa|ag)\b", "", regex=True) # remove suffixes
                 s = s.str.strip()
                 
                 found_ciks = s.map(name_map)
                 df.loc[mask_missing, "cik"] = found_ciks
                 
                 filled = found_ciks.notna().sum()
                 log(f"  Name Map Filled {filled} CIKs.")

    # --- Common Cleanup ---
    # Track dropped deals per quarter BEFORE dropping (R5.D)
    dropped_deals_stats = {"by_quarter": {}, "total": 0, "coverage_by_quarter": {}}
    
    if "cik" in df.columns:
        df["cik"] = pd.to_numeric(df["cik"], errors="coerce").astype("Int64")
        
        # Track per-quarter drops before removal
        if "ann_date" in df.columns:
            df["_temp_ann_date"] = pd.to_datetime(df["ann_date"], errors="coerce")
            missing_cik_mask = df["cik"].isna()
            
            # Count drops by quarter
            for idx in df[missing_cik_mask].index:
                ann = df.loc[idx, "_temp_ann_date"]
                if pd.notna(ann):
                    q_key = f"{ann.year}Q{(ann.month-1)//3 + 1}"
                    dropped_deals_stats["by_quarter"][q_key] = dropped_deals_stats["by_quarter"].get(q_key, 0) + 1
            
            # Coverage rate per quarter (before dropping)
            for q_key, grp in df.groupby(df["_temp_ann_date"].dt.to_period("Q").astype(str)):
                total_in_q = len(grp)
                valid_in_q = grp["cik"].notna().sum()
                dropped_deals_stats["coverage_by_quarter"][q_key] = valid_in_q / total_in_q if total_in_q > 0 else 0.0
            
            df.drop(columns=["_temp_ann_date"], inplace=True)
        
        initial_len = len(df)
        df.dropna(subset=["cik"], inplace=True)
        dropped = initial_len - len(df)
        dropped_deals_stats["total"] = dropped
        if dropped > 0:
            log(f"[WARN] Dropped {dropped} deals due to missing/invalid CIK.")
    else:
        log("[ERROR] No CIK column found. Dropping ALL deals.")
        return pd.DataFrame(columns=["cik", "ann_date", "_deal_value_num", "_deal_value_log"]), dropped_deals_stats
    
    if "ann_date" in df.columns:
        # Check for duplicates (if multiple cols mapped to ann_date?)
        # DataFrame constructor error happens if Series are passed with same name?
        # No, pd.to_datetime on DataFrame with duplicate cols raises error.
        # Ensure unique columns.
        df = df.loc[:, ~df.columns.duplicated()]
        
        df["ann_date"] = pd.to_datetime(df["ann_date"], errors="coerce")
        df.dropna(subset=["ann_date"], inplace=True)
    
    # Value Cleaning
    if "_deal_value_str" in df.columns:
        df["_deal_value_num"] = pd.to_numeric(
             df["_deal_value_str"].astype(str).str.replace(r"[^0-9.]", "", regex=True),
             errors="coerce"
        )
        # Handle log
        df["_deal_value_log"] = np.log1p(df["_deal_value_num"].fillna(0))
        
    elif "_deal_value_num" in df.columns:
         df["_deal_value_log"] = np.log1p(df["_deal_value_num"].fillna(0))
    else:
        df["_deal_value_num"] = np.nan
        df["_deal_value_log"] = 0.0
        
    # Select final
    out = df[["cik", "ann_date", "_deal_value_num", "_deal_value_log"]].copy()
    if "ann_date" in out.columns:
        out = out.sort_values("ann_date").reset_index(drop=True)
        
    return out, dropped_deals_stats

# Build Ticker -> CIK map from Compustat (panel_clean)
# This allows us to link FactSet deals (which have Tickers) to CIKs
TIC_MAP = {}
NAME_MAP = {}
if "tic" in panel_clean.columns and "cik" in panel_clean.columns:
    # 1. Ticker Map
    valid_map = panel_clean[["tic", "cik"]].dropna()
    valid_map["tic"] = valid_map["tic"].astype(str).str.upper().str.strip()
    valid_map["cik"] = valid_map["cik"].astype("Int64")
    # multiple tickers might map to same CIK, just take last observed
    TIC_MAP = valid_map.set_index("tic")["cik"].to_dict()
    
    # 2. Name Map (conm)
    if "conm" in panel_clean.columns:
         valid_name = panel_clean[["conm", "cik"]].dropna()
         s = valid_name["conm"].astype(str).str.lower()
         s = s.str.replace(r"[^\w\s]", "", regex=True)
         s = s.str.replace(r"\b(inc|corp|ltd|llc|co|plc|nv|sa|ag)\b", "", regex=True)
         s = s.str.strip()
         valid_name["norm_name"] = s
         valid_name["cik"] = valid_name["cik"].astype("Int64")
         NAME_MAP = valid_name.set_index("norm_name")["cik"].to_dict()

    log(f"Generated Ticker-CIK map with {len(TIC_MAP)} entries.")
    log(f"Generated Name-CIK map with {len(NAME_MAP)} entries.")

deals_df, DROPPED_DEALS_STATS = load_deals_robust(CONFIG, preloaded_df=DEALS_DF, tic_map=TIC_MAP, name_map=NAME_MAP)\r\nlog(f\"  Dropped deals tracking: {DROPPED_DEALS_STATS['total']} total, {len(DROPPED_DEALS_STATS['by_quarter'])} quarters affected\")

# Save
deals_path = os.path.join(ARTIFACT_DIR, "deals.parquet")
deals_df.to_parquet(deals_path, index=False)
log(f"âœ… Cell C Complete. Loaded {len(deals_df)} deals. Saved to {deals_path}")

# Print Year-to-Year Stats
if not deals_df.empty and "ann_date" in deals_df.columns:
    deal_years = deals_df["ann_date"].dt.year
    year_counts = deal_years.value_counts().sort_index()
    log("\n--- Final Deal Counts by Year ---")
    log(year_counts.to_string())
    
    # Check post-2020 coverage
    post_2020 = deal_years[deal_years > 2020].count()
    log(f"Post-2020 Deals: {post_2020}")

# %% [markdown]
# # Cell D: Feature Engineering (Vectorized)
#
# **Goal**: Create financial ratios, growth rates, and return features using vectorized operations.

# %%
log("Starting Vectorized Feature Engineering (CPU Optimized)...")

# Sort for vectorized ops
panel_clean = panel_clean.sort_values(["cik", "datadate"])

from tqdm.auto import tqdm

def feature_engineering(df):
    log("Starting Vectorized Feature Engineering (CPU Optimized)...")
    
    # 1. Base Var Coalescing (Optimized: Replaces slow bfill(axis=1))
    base_vars = {
        "assets":        ["atq", "at"],
        "revenue":       ["saleq", "sale"],
        "net_income":    ["niq", "ni"],
        "debt_longterm": ["dlttq", "dltt"],
        "equity_book":   ["ceqq", "ceq"],
        "oibdp":         ["oibdpq", "oibdp"],
        "act":           ["actq", "act"],
        "lct":           ["lctq", "lct"],
        "che":           ["cheq", "che"],
        "xint":          ["xintq", "xint"],
        "xrd":           ["xrdq", "xrd"],
        "xsga":          ["xsgaq", "xsga"],
        "capx":          ["capxq", "capx"],
        "oancf":         ["oancfq", "oancf"],
        "csho":          ["cshoq", "csho"],
        "cshtrq":        ["cshtrq", "cshtr"],
        "ppent":         ["ppentq", "ppent"],
        "mkvalt":        ["mkvaltq", "mkvalt"],
    }
    
    # FAST COALESCING LOOP - Uses cascaded fillna() instead of bfill(axis=1)
    for var, cands in tqdm(base_vars.items(), desc="Coalescing Base Vars", leave=False):
        existing = [c for c in cands if c in df.columns]
        if not existing:
            df[var] = np.nan
        else:
            # Start with the first priority column
            combined = df[existing[0]].copy()
            # Fill missing values with subsequent columns in priority order
            for next_col in existing[1:]:
                combined = combined.fillna(df[next_col])
            df[var] = combined

    # 2. Ratios (Vectorized - Fast)
    EPS = 1e-9
    def safe_mag(s): return s.fillna(0).abs() + EPS
    
    df["profit_margin"] = df["net_income"] / safe_mag(df["revenue"])
    df["roa"]           = df["net_income"] / safe_mag(df["assets"])
    df["oper_margin"]   = df["oibdp"] / safe_mag(df["revenue"])
    df["leverage"]      = df["debt_longterm"] / safe_mag(df["assets"])
    df["curr_ratio"]    = df["act"] / safe_mag(df["lct"])
    df["cash_ratio"]    = df["che"] / safe_mag(df["assets"])
    df["int_coverage"]  = df["oibdp"] / safe_mag(df["xint"])
    
    df["rd_int"]   = df["xrd"]   / safe_mag(df["revenue"])
    df["sgna_int"] = df["xsga"]  / safe_mag(df["revenue"])
    df["capx_int"] = df["capx"]  / safe_mag(df["assets"])
    df["ocf_int"]  = df["oancf"] / safe_mag(df["assets"])
    
    # Clip Ratios
    ratio_cols = ["profit_margin", "roa", "oper_margin", "leverage", "curr_ratio", 
                  "cash_ratio", "int_coverage", "rd_int", "sgna_int", "capx_int", "ocf_int"]
    df[ratio_cols] = df[ratio_cols].fillna(0).clip(-5, 5)

    # 3. Size & Turnover
    df["log_csho"] = np.log1p(df["csho"].fillna(0).clip(lower=0))
    df["turnover"] = df["cshtrq"] / safe_mag(df["csho"])
    df["log_turnover"] = np.log1p(df["turnover"].fillna(0).clip(lower=0))

    # 4. Returns (fwd/back)
    df = df.sort_values(["gvkey", "datadate"]).reset_index(drop=True)
    
    if "prccq" in df.columns:
        # Groupby Shift is somewhat slow on CPU, but manageable
        prev_price = df.groupby("gvkey")["prccq"].shift(1)
        df["ret_back_1q"] = (df["prccq"] / prev_price - 1.0)
        
        next_price = df.groupby("gvkey")["prccq"].shift(-1)
        df["ret_fwd_1q"] = (next_price / df["prccq"] - 1.0)
    else:
        df["ret_back_1q"] = np.nan
        df["ret_fwd_1q"] = np.nan

    # 5. Growth Rates
    log_growth_inputs = ["assets", "revenue", "ppent", "mkvalt"]
    input_cols = []
    for raw in log_growth_inputs:
        col = f"log_{raw}"
        df[col] = np.log1p(df[raw].fillna(0).clip(lower=0))
        input_cols.append(col)
        
    # Vectorized Shifts (Global Masking) - Much faster than groupby().shift() in loop
    gvkey = df["gvkey"]
    mask_1 = (gvkey == gvkey.shift(1))
    mask_4 = (gvkey == gvkey.shift(4))
    
    df_shift_1 = df[input_cols].shift(1)
    df_shift_4 = df[input_cols].shift(4)
    
    is_q = (df["freq"] == "Q")
    
    for i, raw in enumerate(log_growth_inputs):
        col = input_cols[i]
        
        # QoQ
        prev_1 = df_shift_1[col].where(mask_1)
        df[f"dlog_{raw}_qoq"] = np.where(is_q, df[col] - prev_1, np.nan)
        
        # YoY
        prev_4 = df_shift_4[col].where(mask_4)
        prev_1_annual = df_shift_1[col].where(mask_1) 
        yoy_prev = np.where(is_q, prev_4, prev_1_annual)
        
        df[f"dlog_{raw}_yoy"] = df[col] - yoy_prev

    return df

# Run
# Run with Resume Logic
feat_path = os.path.join(ARTIFACT_DIR, "features_panel.parquet")
feat_list_path = os.path.join(ARTIFACT_DIR, "feature_list.json")

if AUTO_RESUME and os.path.exists(feat_path) and os.path.exists(feat_list_path):
    log(f"â™»ï¸ RESUMING: Found existing features panel at {feat_path}")
    try:
        features_df = pd.read_parquet(feat_path)
        with open(feat_list_path, "r") as f:
            final_feats = json.load(f)
    except Exception as e:
        log(f"[WARN] Failed to load resume artifacts for Cell D: {e}. Re-running...")
        features_df = feature_engineering(panel_clean.copy())
        features_df.to_parquet(feat_path, index=False)
        numeric_cols = features_df.select_dtypes(include=np.number).columns.tolist()
        exclude = {"gvkey", "cik", "year", "price_pit_risk_flag", "ret_fwd_1q"}
        final_feats = [c for c in numeric_cols if c not in exclude and not c.startswith("label")]
        with open(feat_list_path, "w") as f:
            json.dump(final_feats, f)
        log(f"âœ… Cell D Complete. Saved features to {feat_path}")
else:
    features_df = feature_engineering(panel_clean.copy())
    features_df.to_parquet(feat_path, index=False)
    
    # Save feature list
    numeric_cols = features_df.select_dtypes(include=np.number).columns.tolist()
    exclude = {"gvkey", "cik", "year", "price_pit_risk_flag", "ret_fwd_1q"}
    final_feats = [c for c in numeric_cols if c not in exclude and not c.startswith("label")]
    
    with open(feat_list_path, "w") as f:
        json.dump(final_feats, f)
    
    log(f"âœ… Cell D Complete. Saved features to {feat_path}")

# %% [markdown]
# # Cell E: Multi-Horizon Labeling (Vectorized)
#
# **Goal**: Generate binary targets for horizons [3m, 6m, ..., 24m] using efficient merge_asof.

# %%
def label_panel_multi_horizon(panel_df, deals_df, horizons_months):
    log("Starting Multi-Horizon Labeling...")
    
    # 1. Prepare Tables
    # Panel side
    # Clean CIK
    panel_df["cik"] = pd.to_numeric(panel_df["cik"], errors="coerce").fillna(-1).astype("Int64")
    # Must have snapshot_ts
    panel = panel_df.dropna(subset=["snapshot_ts"]).copy()
    panel = panel.sort_values("snapshot_ts").reset_index(drop=True)
    
    # Deal side
    events = deals_df[["cik", "ann_date"]].copy()
    events["cik"] = pd.to_numeric(events["cik"], errors="coerce").astype("Int64")
    events = events.dropna().sort_values("ann_date").reset_index(drop=True)
    
    # 2. Iterate Horizons
    for h_mo in horizons_months:
        h_days = int(h_mo * 30.5)
        col_name = f"label_deal_0_{h_mo}m"
        
        # We want: Event in (snapshot, snapshot + h_days]
        # merge_asof forward lookahead
        # "Find the nearest event AFTER snapshot"
        
        merged = pd.merge_asof(
            panel[["cik", "snapshot_ts"]], # Left
            events,                        # Right
            left_on="snapshot_ts",
            right_on="ann_date",
            by="cik",
            direction="forward",
            tolerance=pd.Timedelta(days=h_days)
        )
        
        # If match found, and it's strictly > snapshot (merge_asof forward >=, but strict > is safer for lookahead)
        # Actually merge_asof direction='forward' matches closest >= value. 
        # So event_date >= snapshot_ts.
        # We usually want strict future: event_date > snapshot_ts.
        
        # tolerance limits dist to h_days. 
        # So we have snapshot_ts <= ann_date <= snapshot_ts + tolerance
        
        # Check strict inequality
        is_future = merged["ann_date"] > merged["snapshot_ts"]
        panel[col_name] = (merged["ann_date"].notna() & is_future).astype(int)
        
        pos_count = panel[col_name].sum()
        log(f"Horizon {h_mo}m ({h_days}d): {pos_count} positives ({pos_count/len(panel):.2%})")

    return panel

# Run
# Run with Resume Logic
label_path = os.path.join(ARTIFACT_DIR, "labeled_panel.parquet")
cov_path = os.path.join(ARTIFACT_DIR, "label_coverage.csv")

if AUTO_RESUME and os.path.exists(label_path):
    log(f"â™»ï¸ RESUMING: Found existing labeled panel at {label_path}")
    labeled_panel = pd.read_parquet(label_path)
    if os.path.exists(cov_path):
        cov_report = pd.read_csv(cov_path, index_col=0)
    else:
        # Re-compute Coverage Report
        cov_report = labeled_panel.groupby("year")[[f"label_deal_0_{m}m" for m in CONFIG["horizons_months"]]].sum()
else:
    labeled_panel = label_panel_multi_horizon(features_df, deals_df, CONFIG["horizons_months"])
    labeled_panel.to_parquet(label_path, index=False)
    
    # Coverage Report
    cov_report = labeled_panel.groupby("year")[[f"label_deal_0_{m}m" for m in CONFIG["horizons_months"]]].sum()
    cov_report.to_csv(cov_path)
    
    log(f"âœ… Cell E Complete. Saved labeled panel to {label_path}")
print("\nLabel Coverage by Year:")
print(cov_report.tail(5))

# %% [markdown]
# # Cell E2: COMPILE Phase - Convert to Global Arrays
#
# **Goal**: Convert the labeled panel DataFrame to CompileArtifacts (float32 numpy arrays).
# This is the COMPILE phase that should only run once per input configuration.
# The resulting artifacts can be reused for all training runs.

# %%
def compile_global_arrays(panel_df, feature_cols, horizons_months, config, dropped_deals_stats=None):
    """
    COMPILE PHASE: Convert labeled panel to global numpy arrays for fast training.
    
    This function produces:
    - X_global: float32 design matrix (no NaN, no string columns)
    - y_by_horizon: dict of uint8 label arrays per horizon
    - rows_train_by_quarter / rows_test_by_quarter: precomputed splits
    - metadata arrays: gvkey, cik, datadate, ret_fwd
    
    CRITICAL: This is the ONLY place where DataFrame -> Array conversion happens.
    All subsequent training must use array slicing only.
    """
    log("=" * 60)
    log("COMPILE PHASE: Converting Panel to Global Arrays")
    log("=" * 60)
    
    t_start = time.time()
    rss_before = get_rss_mb()
    
    # 1. Compute cache key
    cache_key = compute_compile_cache_key(config)
    log(f"Cache key: {cache_key}")
    
    # 2. Check for cached artifacts
    cache_dir = os.path.join(ARTIFACT_DIR, "compile_cache")
    manifest_path = os.path.join(cache_dir, "compile_manifest.json")
    
    if not config.get("compile", {}).get("force_recompile", False) and os.path.exists(manifest_path):
        try:
            with open(manifest_path, "r") as f:
                manifest = json.load(f)
            
            if manifest.get("cache_key") == cache_key:
                log("Cache key matches - loading cached artifacts...")
                
                # Load cached arrays
                X_global = np.load(os.path.join(cache_dir, "X_global.npy"), 
                                   mmap_mode='r' if config.get("compile", {}).get("use_memmap", True) else None)
                
                with open(os.path.join(cache_dir, "feature_names.json"), "r") as f:
                    feature_names = json.load(f)
                
                y_by_horizon = {}
                for h_mo in horizons_months:
                    y_path = os.path.join(cache_dir, f"y_{h_mo}m.npy")
                    if os.path.exists(y_path):
                        y_by_horizon[h_mo] = np.load(y_path)
                
                quarter_ids = np.load(os.path.join(cache_dir, "quarter_ids.npy"), allow_pickle=True)
                
                train_npz = np.load(os.path.join(cache_dir, "rows_train_by_quarter.npz"))
                test_npz = np.load(os.path.join(cache_dir, "rows_test_by_quarter.npz"))
                rows_train = {k: train_npz[k] for k in train_npz.files}
                rows_test = {k: test_npz[k] for k in test_npz.files}
                
                metadata_gvkey = np.load(os.path.join(cache_dir, "metadata_gvkey.npy"), allow_pickle=True)
                metadata_cik = np.load(os.path.join(cache_dir, "metadata_cik.npy"))
                metadata_ret_fwd = np.load(os.path.join(cache_dir, "metadata_ret_fwd.npy"))
                
                log(f"âœ… CACHE HIT: Loaded {manifest['n_rows']} rows, {manifest['n_features']} features in {time.time()-t_start:.1f}s")
                
                TIMING_LOGGER.log_phase(
                    "compile",
                    cache_hit=True,
                    n_rows=manifest["n_rows"],
                    n_features=manifest["n_features"],
                    total_seconds=time.time() - t_start
                )
                
                return CompileArtifacts(
                    X_global=X_global,
                    feature_names=feature_names,
                    y_by_horizon=y_by_horizon,
                    quarter_ids=quarter_ids,
                    rows_train_by_quarter=rows_train,
                    rows_test_by_quarter=rows_test,
                    metadata_gvkey=metadata_gvkey,
                    metadata_cik=metadata_cik,
                    metadata_datadate=None,
                    metadata_ret_fwd=metadata_ret_fwd,
                    cache_key=cache_key,
                    cache_hit=True,
                    compile_seconds=time.time() - t_start,
                    n_rows=manifest["n_rows"],
                    n_features=manifest["n_features"]
                )
        except Exception as e:
            log(f"[WARN] Cache load failed: {e}, recompiling...")
    
    # 3. Full compile
    log("Compiling from scratch...")
    
    # Ensure we have at least basic features
    available_feats = [c for c in feature_cols if c in panel_df.columns]
    if len(available_feats) < 5:
        log(f"[WARN] Only {len(available_feats)} features found in panel")
    
    # Convert to float32 array
    X_data = panel_df[available_feats].values.astype(np.float32)
    np.nan_to_num(X_data, copy=False, nan=0.0, posinf=0.0, neginf=0.0)
    log(f"  X_global: {X_data.shape} ({X_data.nbytes / 1e6:.1f} MB)")
    
    # Labels by horizon
    y_by_horizon = {}
    for h_mo in horizons_months:
        label_col = f"label_deal_0_{h_mo}m"
        if label_col in panel_df.columns:
            y_by_horizon[h_mo] = panel_df[label_col].values.astype(np.uint8)
            log(f"  y_{h_mo}m: {y_by_horizon[h_mo].sum()} positives")
    
    # Quarter IDs
    quarter_ids = panel_df["panel_q"].values
    quarters = sorted(panel_df["panel_q"].unique())
    log(f"  Quarters: {len(quarters)} ({quarters[0]} to {quarters[-1]})")
    
    # Compute splits (expanding window: train on all rows before quarter)
    rows_train = {}
    rows_test = {}
    panel_idx = np.arange(len(panel_df), dtype=np.int32)
    
    for q in quarters:
        test_mask = quarter_ids == q
        train_mask = quarter_ids < q  # Expanding window
        
        rows_test[q] = panel_idx[test_mask]
        rows_train[q] = panel_idx[train_mask]
    
    log(f"  Computed splits for {len(quarters)} quarters")
    
    # Metadata
    metadata_gvkey = panel_df["gvkey"].values if "gvkey" in panel_df.columns else np.array([])
    metadata_cik = pd.to_numeric(panel_df["cik"], errors="coerce").fillna(-1).values.astype(np.int64)
    
    if "ret_fwd_1q" in panel_df.columns:
        metadata_ret_fwd = panel_df["ret_fwd_1q"].values.astype(np.float32)
        np.nan_to_num(metadata_ret_fwd, copy=False, nan=0.0)
    else:
        metadata_ret_fwd = np.zeros(len(panel_df), dtype=np.float32)
    
    # 4. Save to cache
    os.makedirs(cache_dir, exist_ok=True)
    
    np.save(os.path.join(cache_dir, "X_global.npy"), X_data)
    
    with open(os.path.join(cache_dir, "feature_names.json"), "w") as f:
        json.dump(available_feats, f)
    
    for h_mo, y in y_by_horizon.items():
        np.save(os.path.join(cache_dir, f"y_{h_mo}m.npy"), y)
    
    np.save(os.path.join(cache_dir, "quarter_ids.npy"), quarter_ids, allow_pickle=True)
    np.savez(os.path.join(cache_dir, "rows_train_by_quarter.npz"), **rows_train)
    np.savez(os.path.join(cache_dir, "rows_test_by_quarter.npz"), **rows_test)
    np.save(os.path.join(cache_dir, "metadata_gvkey.npy"), metadata_gvkey, allow_pickle=True)
    np.save(os.path.join(cache_dir, "metadata_cik.npy"), metadata_cik)
    np.save(os.path.join(cache_dir, "metadata_ret_fwd.npy"), metadata_ret_fwd)
    
    # Manifest
    compile_seconds = time.time() - t_start
    manifest = {
        "cache_key": cache_key,
        "compile_timestamp": datetime.now().isoformat(),
        "compile_seconds": compile_seconds,
        "n_rows": len(panel_df),
        "n_features": len(available_feats),
        "horizons_months": list(horizons_months),
        "n_quarters": len(quarters),
        "rss_before_mb": rss_before,
        "rss_after_mb": get_rss_mb(),
    }
    
    with open(manifest_path, "w") as f:
        json.dump(manifest, f, indent=2)
    
    TIMING_LOGGER.log_phase(
        "compile",
        cache_hit=False,
        n_rows=len(panel_df),
        n_features=len(available_feats),
        total_seconds=compile_seconds
    )
    
    log(f"âœ… COMPILE COMPLETE: {len(panel_df)} rows, {len(available_feats)} features in {compile_seconds:.1f}s")
    log("=" * 60)
    
    return CompileArtifacts(
        X_global=X_data,
        feature_names=available_feats,
        y_by_horizon=y_by_horizon,
        quarter_ids=quarter_ids,
        rows_train_by_quarter=rows_train,
        rows_test_by_quarter=rows_test,
        metadata_gvkey=metadata_gvkey,
        metadata_cik=metadata_cik,
        metadata_datadate=None,
        metadata_ret_fwd=metadata_ret_fwd,
        dropped_deals_by_quarter=dropped_deals_stats.get("by_quarter", {}) if dropped_deals_stats else {},
        coverage_rate_by_quarter=dropped_deals_stats.get("coverage_by_quarter", {}) if dropped_deals_stats else {},
        cache_key=cache_key,
        cache_hit=False,
        compile_seconds=compile_seconds,
        n_rows=len(panel_df),
        n_features=len(available_feats)
    )

# Run Compile Phase
COMPILE_ARTIFACTS = compile_global_arrays(
    labeled_panel, 
    final_feats,  # Feature columns from Cell D
    CONFIG["horizons_months"],
    CONFIG,
    dropped_deals_stats=DROPPED_DEALS_STATS
)

log(f"âœ… Cell E2 Complete: Compiled {COMPILE_ARTIFACTS.n_rows} rows, {COMPILE_ARTIFACTS.n_features} features")
if COMPILE_ARTIFACTS.cache_hit:
    log("  (Used cached compile artifacts)")


# %% [markdown]
# # Cell F: Training Dry-Run Harness (Stability)
#
# **Goal**: Verify model stability by running multiple trials on a single holdout quarter. Measure Jaccard overlap of top predictions.

# %%
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import roc_auc_score, brier_score_loss, precision_recall_curve, auc

def dry_run_harness(df, target_col, n_trials=5, top_k=50):
    log(f"Running Dry-Run Harness for target: {target_col}")
    
    # 1. Select Holdout Quarter (recent but with labels)
    # Pick the last quarter that has at least 50 positives to be decent
    # Or just hardcode a known good quarter e.g. 2020Q4 if available
    valid_q = df.groupby("panel_q")[target_col].sum()
    candidates = valid_q[valid_q > 10].index.sort_values(ascending=False)
    
    if len(candidates) > 4:
        holdout_q = candidates[4] # Go back a bit to ensure data settlement
    else:
        holdout_q = candidates[0]
        
    log(f"Selected Holdout Quarter: {holdout_q}")
    
    train_mask = df["panel_q"] < holdout_q
    test_mask = df["panel_q"] == holdout_q
    
    X_cols = [c for c in df.columns if c in final_feats]
    
    train_df = df[train_mask].copy()
    test_df = df[test_mask].copy()
    
    log(f"Train size: {len(train_df)}, Test size: {len(test_df)}")
    
    results = []
    top_k_sets = []
    
    for i in range(n_trials):
        # Vary seed
        seed = 42 + i
        
        # Simple Downsampling of negatives for speed/balance
        pos = train_df[train_df[target_col]==1]
        neg = train_df[train_df[target_col]==0].sample(n=len(pos)*10, random_state=seed) # 1:10 ratio
        train_sub = pd.concat([pos, neg]).sample(frac=1, random_state=seed)
        
        clf = HistGradientBoostingClassifier(
            learning_rate=0.05, 
            max_iter=100, 
            max_depth=5,
            random_state=seed
        )
        
        clf.fit(train_sub[X_cols], train_sub[target_col])
        probs = clf.predict_proba(test_df[X_cols])[:, 1]
        
        # Metrics
        roc = roc_auc_score(test_df[target_col], probs)
        precision, recall, _ = precision_recall_curve(test_df[target_col], probs)
        pr_auc = auc(recall, precision)
        
        # Top K
        top_k_idx = np.argsort(probs)[-top_k:]
        top_k_ciks = test_df.iloc[top_k_idx]["cik"].values
        top_k_sets.append(set(top_k_ciks))
        
        results.append({
            "trial": i,
            "seed": seed,
            "auc": roc,
            "pr_auc": pr_auc
        })
        
    # Stability: Jaccard
    jaccards = []
    for i in range(len(top_k_sets)):
        for j in range(i+1, len(top_k_sets)):
            s1, s2 = top_k_sets[i], top_k_sets[j]
            jaccards.append(len(s1 & s2) / len(s1 | s2))
            
    mean_jaccard = np.mean(jaccards)
    log(f"Dry Run Complete. Mean AUC: {pd.DataFrame(results)['auc'].mean():.3f}")
    log(f"Stability (Mean Jaccard of Top {top_k}): {mean_jaccard:.2%}")
    
    return pd.DataFrame(results), mean_jaccard

# Prepare quarter column - Always do this, needed for Cell G too
labeled_panel["panel_q"] = labeled_panel["datadate"].dt.to_period("Q")

# Run Stability Suite with COMPILE_ARTIFACTS (NEW)
stability_path = os.path.join(ARTIFACT_DIR, "stability_report.json")

if AUTO_RESUME and os.path.exists(stability_path):
    log(f"â™»ï¸ RESUMING: Found existing stability report at {stability_path}")
    with open(stability_path, "r") as f:
        stability_report = json.load(f)
    stability_score = stability_report.get("mean_jaccard_at_k", 0.0)
else:
    # Select holdout quarter (last quarter with sufficient positives)
    quarters = sorted(COMPILE_ARTIFACTS.rows_test_by_quarter.keys())
    # Pick 5th from end to ensure data settlement
    holdout_q = quarters[-5] if len(quarters) > 5 else quarters[-1]
    
    log(f"Running Stability Suite on holdout quarter: {holdout_q}")
    stability_report = run_stability_suite(
        quarter=holdout_q,
        horizon=3,  # Default to 3-month horizon
        artifacts=COMPILE_ARTIFACTS,
        stability_config=CONFIG["stability"],
        sampling_config=CONFIG["sampling"]
    )
    
    # Save report
    with open(stability_path, "w") as f:
        json.dump(stability_report, f, indent=2, default=str)
    
    stability_score = stability_report.get("mean_jaccard_at_k", 0.0)
    log(f"âœ… Cell F Complete. Stability Score (Jaccard@50): {stability_score:.2%}")

# Also run legacy dry-run harness for comparison if not using inner-loop mode
if not CONFIG["inner_loop"]["enabled"]:
    dry_path = os.path.join(ARTIFACT_DIR, "dry_run_results.json")
    
    if AUTO_RESUME and os.path.exists(dry_path):
        log(f"â™»ï¸ RESUMING: Found existing dry run results at {dry_path}")
        with open(dry_path, "r") as f:
             dry_res = pd.DataFrame(json.load(f))
    else:
        dry_res, legacy_jaccard = dry_run_harness(labeled_panel, "label_deal_0_3m", n_trials=CONFIG["calibration"]["n_dryrun_trials"])
        with open(dry_path, "w") as f:
            json.dump(dry_res.to_dict('records'), f, indent=4)
        log(f"  Legacy dry run Jaccard: {legacy_jaccard:.2%}")


# %% [markdown]
# # Cell G: Full Backtest & Baselines + Visuals
#
# **Goal**: Expanding window backtest comparisons vs S&P 500.
#
# **INNER LOOP MODE**: Set CONFIG["inner_loop"]["enabled"] = True to run only 1 quarter
# for fast development iteration (target: â‰¤60 seconds).

# %%
# 1. Basics
TARGET_HORIZON = "label_deal_0_3m"
TARGET_HORIZON_MONTHS = 3

# ================================================
# INNER LOOP MODE (Fast Development)
# ================================================
if CONFIG["inner_loop"]["enabled"]:
    log("=" * 60)
    log("âš¡ INNER LOOP MODE: Fast single-quarter iteration")
    log("=" * 60)
    
    t_inner_start = time.time()
    
    # Pick quarter
    quarters = sorted(COMPILE_ARTIFACTS.rows_test_by_quarter.keys())
    inner_q = CONFIG["inner_loop"]["quarter"]
    if inner_q is None:
        inner_q = quarters[-5] if len(quarters) > 5 else quarters[-1]
    
    inner_horizon = CONFIG["inner_loop"]["horizon"]
    log(f"  Quarter: {inner_q}, Horizon: {inner_horizon}m")
    
    # Run single train_eval_one
    task = TrainTask(quarter=inner_q, horizon_months=inner_horizon, seed=42)
    result = train_eval_one(task, COMPILE_ARTIFACTS, CONFIG["sampling"])
    
    inner_elapsed = time.time() - t_inner_start
    log(f"  AUC: {result.metrics.get('auc', 0.5):.3f}")
    log(f"  Total time: {inner_elapsed:.1f}s (target: â‰¤60s)")
    log(f"  Timing breakdown: {result.timing}")
    
    if inner_elapsed <= 60:
        log("âœ… INNER LOOP PASSED: â‰¤60 seconds")
    else:
        log(f"âš ï¸ INNER LOOP SLOW: {inner_elapsed:.1f}s > 60s target")
    
    # Run AutoML if requested
    if CONFIG.get("automl", {}).get("enabled_inner_loop", False):
        best_config, preds, trials = run_one_shot_automl(
            inner_q, inner_horizon, COMPILE_ARTIFACTS,
            CONFIG["automl"], CONFIG["sampling"]
        )
        log(f"  AutoML best config: {best_config}")
    
    log("=" * 60)
    log("âš¡ Inner loop complete. Set CONFIG['inner_loop']['enabled'] = False for full backtest.")
    log("=" * 60)
    
    # Skip rest of Cell G in inner loop mode
    results_backtest = []
    res_df = pd.DataFrame()
    
else:
    # ================================================
    # FULL BACKTEST MODE
    # ================================================
    
    # 2. S&P 500 Baseline (use robust function)
    start_date = labeled_panel["datadate"].min().strftime("%Y-%m-%d")
    end_date = datetime.now().strftime("%Y-%m-%d")
    
    spy_map = get_spy_quarterly_returns(
        start_date, end_date, 
        fail_fast=CONFIG.get("benchmark", {}).get("spy_fail_fast", False)
    )
    
    # 3. Decide: Fast mode (COMPILE_ARTIFACTS) or Legacy mode (DataFrame)
    USE_FAST_BACKTEST = True  # Use new train_eval_one
    
    quarters = sorted(COMPILE_ARTIFACTS.rows_test_by_quarter.keys())
    start_idx = 10  # Burn-in
    results_backtest = []
    
    log(f"Starting Backtest over {len(quarters)-start_idx} quarters (fast_mode={USE_FAST_BACKTEST})...")
    t_backtest_start = time.time()
    
    for q_idx, q in enumerate(quarters[start_idx:], start=start_idx):
        
        if USE_FAST_BACKTEST:
            # FAST PATH: Use train_eval_one with array slicing
            task = TrainTask(quarter=q, horizon_months=TARGET_HORIZON_MONTHS, seed=42)
            result = train_eval_one(task, COMPILE_ARTIFACTS, CONFIG["sampling"])
            
            if len(result.predictions) == 0:
                continue
            
            probs = result.predictions
            test_idx = COMPILE_ARTIFACTS.rows_test_by_quarter[q]
            
            # Top 50 selection
            top_50_idx = np.argsort(probs)[-50:]
            ret_fwd = COMPILE_ARTIFACTS.metadata_ret_fwd[test_idx]
            port_ret = np.clip(ret_fwd[top_50_idx], -0.5, 1.0).mean()
            univ_ret = np.clip(ret_fwd, -0.5, 1.0).mean()
            
            auc_score = result.metrics.get("auc", 0.5)
            
        else:
            # LEGACY PATH: Use DataFrame-based training (for comparison)
            X_cols = [c for c in labeled_panel.columns if c in final_feats]
            
            train_mask = labeled_panel["panel_q"] < q
            test_mask = labeled_panel["panel_q"] == q
            
            train = labeled_panel[train_mask]
            test = labeled_panel[test_mask]
            
            if len(test) == 0: 
                continue
            if train[TARGET_HORIZON].sum() < 10: 
                continue
            
            # Downsample train
            pos = train[train[TARGET_HORIZON]==1]
            neg = train[train[TARGET_HORIZON]==0].sample(n=min(len(pos)*10, len(train[train[TARGET_HORIZON]==0])), random_state=42)
            train_bal = pd.concat([pos, neg])
            
            # Train
            clf = HistGradientBoostingClassifier(learning_rate=0.05, max_depth=4, random_state=42)
            clf.fit(train_bal[X_cols], train_bal[TARGET_HORIZON])
            
            # Predict
            probs = clf.predict_proba(test[X_cols])[:, 1]
            
            # Top 50
            top_50_idx = np.argsort(probs)[-50:]
            selected = test.iloc[top_50_idx].copy()
            
            port_ret = selected["ret_fwd_1q"].clip(-0.5, 1.0).mean() if "ret_fwd_1q" in selected.columns else 0.0
            univ_ret = test["ret_fwd_1q"].clip(-0.5, 1.0).mean() if "ret_fwd_1q" in test.columns else 0.0
            
            try:
                auc_score = roc_auc_score(test[TARGET_HORIZON], probs)
            except:
                auc_score = 0.5
        
        # SPY return for this quarter
        spy_ret = spy_map.get(q, np.nan)
        if isinstance(spy_ret, pd.Series): 
            spy_ret = spy_ret.item()
        
        results_backtest.append({
            "quarter": str(q),
            "auc": auc_score,
            "port_ret": port_ret,
            "univ_ret": univ_ret,
            "spy_ret": spy_ret,
            "excess_vs_spy": (port_ret - spy_ret) if pd.notna(spy_ret) else np.nan
        })
        
        if q_idx % 4 == 0:
            log(f"Q {q}: AUC={auc_score:.2f}, Port={port_ret:.1%}, SPY={spy_ret if pd.notna(spy_ret) else 'N/A'}")
    
    backtest_elapsed = time.time() - t_backtest_start
    log(f"Backtest complete in {backtest_elapsed:.1f}s ({len(results_backtest)} quarters)")
    
    TIMING_LOGGER.log_phase(
        "backtest",
        n_quarters=len(results_backtest),
        total_seconds=backtest_elapsed,
        fast_mode=USE_FAST_BACKTEST
    )
    
    res_df = pd.DataFrame(results_backtest)
    res_path = os.path.join(ARTIFACT_DIR, "results_backtest.parquet")
    res_df.to_parquet(res_path)
    
    # 4. Visuals (Equity Curve)
    if len(res_df) > 0:
        res_df_plot = res_df.fillna(0)
        res_df_plot["cum_port"] = (1 + res_df_plot["port_ret"]).cumprod()
        res_df_plot["cum_spy"] = (1 + res_df_plot["spy_ret"]).cumprod()
        
        plt.figure(figsize=(10, 6))
        plt.plot(res_df_plot["quarter"], res_df_plot["cum_port"], label="Model Strategy", linewidth=2.5)
        plt.plot(res_df_plot["quarter"], res_df_plot["cum_spy"], label="S&P 500", linestyle="--", color="gray")
        plt.title("Cumulative Wealth: Model vs S&P 500")
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(ARTIFACT_DIR, "equity_curve.png"), dpi=100)
        plt.show()
    
    log("âœ… Cell G Complete. Backtest finished.")

# Save timing log
TIMING_LOGGER.save()

log("Pipeline Complete.")

# Calibration Footer (Required by Guidelines)
print("\n" + "="*60)
print("Persona Used: Senior Quant ML Engineer")
print("Difficulty Tuning: ~70% ZPD (Modular Pipeline), ~20% Staff+ (Stability Harness)")
print("Concrete Suggestion: Run the 'Dry Run' cell with n_trials=20 to get statistically significant stability metrics.")
print("="*60)

# %% [markdown]
# # Cell H: Event Study Verification (Jump Plot)
#
# **Goal**: Validate data alignment by plotting normalized price around M&A announcements.

# %%
def run_event_study(panel_df, deals_df, min_deal_value=100):
    """
    High-Precision Event Study: Price vs. Announcement Date.
    Filters for major deals (> $100M) to ensure clear signal.
    """
    log("Running Event Study Verification...")
    
    if "_deal_value_num" not in deals_df.columns:
        log("[SKIP] _deal_value_num missing in deals. Cannot run event study.")
        return
    
    # Filter for Major Deals
    precise_events = deals_df[deals_df["_deal_value_num"] > min_deal_value][["cik", "ann_date"]].copy()
    precise_events = precise_events.rename(columns={"ann_date": "event_date"})
    
    if "prccq" not in panel_df.columns:
        log("[SKIP] prccq missing in panel. Cannot run event study.")
        return
    
    price_history = panel_df[["cik", "datadate", "prccq"]].dropna().copy()
    
    # Merge
    merged = pd.merge(price_history, precise_events, on="cik", how="inner")
    merged["days_rel"] = (merged["datadate"] - merged["event_date"]).dt.days
    window = merged[(merged["days_rel"] >= -365) & (merged["days_rel"] <= 365)].copy()
    
    if window.empty:
        log("[WARN] No matching price/deal pairs for event study.")
        return
    
    def normalize_precise(g):
        baseline = g[(g["days_rel"] >= -90) & (g["days_rel"] <= -10)]
        if not baseline.empty:
            base_price = baseline.sort_values("days_rel", ascending=False).iloc[0]["prccq"]
            if base_price > 0:
                g["norm_price"] = g["prccq"] / base_price
                return g
        return None
    
    norm_df = window.groupby(["cik", "event_date"]).apply(normalize_precise)
    
    if norm_df is None or norm_df.empty:
        log("[WARN] Not enough data for event study normalization.")
        return
    
    norm_df["week_rel"] = (norm_df["days_rel"] / 7).round().astype(int)
    stats = norm_df.groupby("week_rel")["norm_price"].quantile([0.25, 0.50, 0.75]).unstack()
    
    # Plot
    plt.figure(figsize=(12, 7))
    plt.plot(stats.index, stats[0.50], color="crimson", linewidth=3, label="Median Stock Price")
    plt.fill_between(stats.index, stats[0.25], stats[0.75], color="crimson", alpha=0.1, label="IQR (25-75%)")
    plt.axvline(0, color="black", linestyle="--", linewidth=2, label="Announcement Day")
    plt.axhline(1.0, color="gray", linestyle=":", alpha=0.5)
    plt.title("Event Study: Price vs. M&A Announcement", fontsize=14)
    plt.xlabel("Weeks Relative to Announcement")
    plt.ylabel("Normalized Price (1.0 = Pre-Deal)")
    plt.xlim(-20, 20)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(ARTIFACT_DIR, "event_study_plot.png"), dpi=150)
    plt.show()
    
    # Verdict
    try:
        pre = stats.loc[-2, 0.50] if -2 in stats.index else 1.0
        post = stats.loc[2, 0.50] if 2 in stats.index else 1.0
        jump = post / pre - 1
        log(f"Median Jump (-2wk to +2wk): {jump:.1%}")
        if jump > 0.15:
            log("âœ… VERDICT: Sharp Step-Function Detected. Data is aligned.")
        else:
            log("âš ï¸ VERDICT: Signal muted. Check for rumor leak or deal mix.")
    except Exception as e:
        log(f"[WARN] Could not compute jump: {e}")

# Run Event Study
run_event_study(labeled_panel, deals_df)
log("âœ… Cell H Complete: Event Study.")

# %% [markdown]
# # Cell I: Correlation Screening & Advanced Portfolio Construction
#
# **Goal**: Build correlation matrix on trailing returns and select portfolio with correlation caps.

# %%
# Hyperparameters
DEAL_PREMIUM_EST = 0.25   # Assumed median deal premium (quarter-scale)
COST_PER_TRADE = 0.003    # 30 bps per roundtrip
NET_ALPHA_MIN = 0.0       # Threshold for net_alpha > 0
K_MAX = 50                # Hard cap on bets
K_MIN = 10                # Minimum bets
CORR_LAG_Q = 8            # Trailing quarters for correlation
RHO_MAX = 0.80            # Max allowed |corr| between selected names
N_SIMS_RANDOM = 200       # Random baseline simulations

def clip_returns(s, lower=-0.8, upper=0.8):
    """Clip extreme returns."""
    return s.replace([np.inf, -np.inf], np.nan).clip(lower=lower, upper=upper)

def build_corr_matrix(panel_df, candidate_ciks, holdout_qtr, lag_q=CORR_LAG_Q):
    """
    Build cross-sectional correlation matrix of trailing backward returns.
    """
    if lag_q <= 0 or len(candidate_ciks) < 2:
        return None
    
    panel_df["panel_q"] = panel_df["datadate"].dt.to_period("Q")
    q_min = holdout_qtr - lag_q
    mask = (panel_df["panel_q"] >= q_min) & (panel_df["panel_q"] < holdout_qtr)
    
    df_hist = panel_df.loc[mask, ["datadate", "cik", "ret_back_1q"]].copy()
    df_hist = df_hist.dropna(subset=["ret_back_1q"])
    df_hist = df_hist[df_hist["cik"].isin(candidate_ciks)]
    
    if df_hist["cik"].nunique() < 2:
        return None
    
    # Aggregate duplicates
    df_hist = df_hist.groupby(["datadate", "cik"], as_index=False)["ret_back_1q"].mean()
    
    try:
        pivot = df_hist.pivot(index="datadate", columns="cik", values="ret_back_1q")
    except ValueError:
        return None
    
    if pivot.shape[1] < 2 or pivot.shape[0] < 2:
        return None
    
    corr = pivot.corr().replace([np.inf, -np.inf], np.nan).fillna(0.0)
    return corr

def select_with_corr_screen(candidates, corr_mat, k_max=K_MAX, k_min=K_MIN, rho_max=RHO_MAX):
    """
    Greedy selection with correlation cap.
    """
    if candidates.empty:
        return candidates.copy()
    
    selected_rows = []
    selected_ciks = []
    use_corr = corr_mat is not None and corr_mat.shape[0] >= 2
    
    for _, row in candidates.iterrows():
        if len(selected_rows) >= k_max:
            break
        
        net_a = row.get("net_alpha", 0.0)
        if net_a <= NET_ALPHA_MIN and len(selected_rows) >= k_min:
            break
        
        cik_i = row["cik"]
        
        # Correlation screen
        if use_corr and selected_ciks and (cik_i in corr_mat.index):
            valid_sel = [c for c in selected_ciks if c in corr_mat.columns]
            if valid_sel:
                max_abs = np.abs(corr_mat.loc[cik_i, valid_sel]).max()
                if max_abs > rho_max:
                    continue
        
        selected_rows.append(row)
        selected_ciks.append(cik_i)
    
    return pd.DataFrame(selected_rows) if selected_rows else candidates.head(0)

def simulate_random_portfolios(ret_series, K, n_sims=N_SIMS_RANDOM, seed=123):
    """Simulate equal-weight random K-name portfolios."""
    rng = np.random.default_rng(seed)
    ret_clean = clip_returns(ret_series).dropna()
    n = len(ret_clean)
    if n == 0 or n < K or K <= 0:
        return None
    
    vals = ret_clean.to_numpy()
    return np.array([vals[rng.choice(n, size=K, replace=False)].mean() for _ in range(n_sims)])

log("âœ… Cell I Complete: Correlation Screening Functions Defined.")

# %% [markdown]
# # Cell J: Quarter Deep-Dive Reports (Best/Worst/Median)
#
# **Goal**: Generate detailed reports for selected quarters.

# %%
def generate_quarter_report(panel_df, quarter, target_col, probs, artifact_dir):
    """
    Generate a detailed report for a specific quarter.
    """
    q_mask = panel_df["panel_q"] == quarter
    df_q = panel_df[q_mask].copy()
    df_q["prob_acq"] = probs
    
    # Top 50 bets
    top_50 = df_q.nlargest(50, "prob_acq")
    
    # Calculate hit rate
    hit_rate = top_50[target_col].mean() if target_col in top_50.columns else np.nan
    port_ret = clip_returns(top_50["ret_fwd_1q"]).mean() if "ret_fwd_1q" in top_50.columns else np.nan
    
    report = {
        "quarter": str(quarter),
        "n_firms": len(df_q),
        "n_positives": int(df_q[target_col].sum()) if target_col in df_q.columns else 0,
        "hit_rate_top50": float(hit_rate) if not np.isnan(hit_rate) else None,
        "port_ret_top50": float(port_ret) if not np.isnan(port_ret) else None,
        "top_10_ciks": top_50["cik"].head(10).tolist(),
        "top_10_probs": top_50["prob_acq"].head(10).tolist(),
    }
    
    # Save
    q_dir = os.path.join(artifact_dir, "quarter_reports", str(quarter).replace("/", "_"))
    os.makedirs(q_dir, exist_ok=True)
    
    with open(os.path.join(q_dir, "report.json"), "w") as f:
        json.dump(report, f, indent=2, default=str)
    
    top_50.to_parquet(os.path.join(q_dir, "bets.parquet"), index=False)
    
    return report

def select_analysis_quarters(results_df, metric_col="excess_vs_spy", n_random=3, seed=42):
    """
    Select best, worst, median, and random quarters for deep-dive.
    """
    valid = results_df.dropna(subset=[metric_col])
    if valid.empty:
        return []
    
    sorted_df = valid.sort_values(metric_col)
    
    quarters = []
    quarters.append(("worst", sorted_df.iloc[0]["quarter"]))
    quarters.append(("best", sorted_df.iloc[-1]["quarter"]))
    quarters.append(("median", sorted_df.iloc[len(sorted_df)//2]["quarter"]))
    
    # Random
    rng = np.random.default_rng(seed)
    random_idx = rng.choice(len(valid), size=min(n_random, len(valid)), replace=False)
    for i, idx in enumerate(random_idx):
        quarters.append((f"random_{i+1}", valid.iloc[idx]["quarter"]))
    
    return quarters

log("âœ… Cell J Complete: Quarter Report Functions Defined.")

# %% [markdown]
# # Cell K: Network Correlation GIF
#
# **Goal**: Create animated network visualization of firm correlations over time.

# %%
try:
    import networkx as nx
    import imageio
    NETWORK_AVAILABLE = True
except ImportError:
    log("[WARN] networkx or imageio not installed. Network GIF will be skipped.")
    NETWORK_AVAILABLE = False

def create_network_frame(panel_df, deals_df, quarter, target_col, top_n=30, corr_threshold=0.5):
    """
    Create a single network frame for a quarter.
    Nodes = firms, Edges = correlation > threshold, Color = M&A event.
    """
    if not NETWORK_AVAILABLE:
        return None
    
    q_mask = panel_df["panel_q"] == quarter
    df_q = panel_df[q_mask].copy()
    
    if len(df_q) < 5:
        return None
    
    # Get top N by some metric (e.g., market cap or assets)
    if "mkvalt" in df_q.columns:
        top_firms = df_q.nlargest(top_n, "mkvalt")["cik"].unique()
    else:
        top_firms = df_q["cik"].unique()[:top_n]
    
    # Build correlation matrix
    corr_mat = build_corr_matrix(panel_df, top_firms, quarter, lag_q=4)
    if corr_mat is None or corr_mat.shape[0] < 3:
        return None
    
    # Build graph
    G = nx.Graph()
    
    for cik in top_firms:
        if cik in corr_mat.index:
            # Check if this firm had an event
            had_event = 0
            if target_col in df_q.columns:
                firm_data = df_q[df_q["cik"] == cik]
                if not firm_data.empty:
                    had_event = int(firm_data[target_col].max())
            G.add_node(cik, event=had_event)
    
    # Add edges
    for i, cik1 in enumerate(corr_mat.index):
        for cik2 in corr_mat.index[i+1:]:
            corr_val = corr_mat.loc[cik1, cik2]
            if abs(corr_val) > corr_threshold:
                G.add_edge(cik1, cik2, weight=abs(corr_val))
    
    if len(G.nodes) < 3:
        return None
    
    # Draw
    fig, ax = plt.subplots(figsize=(10, 10))
    pos = nx.spring_layout(G, seed=42)
    
    colors = ["red" if G.nodes[n].get("event", 0) == 1 else "lightblue" for n in G.nodes]
    sizes = [300 if G.nodes[n].get("event", 0) == 1 else 100 for n in G.nodes]
    
    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=sizes, ax=ax)
    nx.draw_networkx_edges(G, pos, alpha=0.3, ax=ax)
    nx.draw_networkx_labels(G, pos, font_size=6, ax=ax)
    
    ax.set_title(f"Firm Correlation Network - {quarter}\nRed = M&A Event")
    ax.axis("off")
    
    # Save to buffer (compatible with matplotlib 3.8+)
    fig.canvas.draw()
    buf = fig.canvas.buffer_rgba()
    image = np.asarray(buf, dtype="uint8")
    image = image[:, :, :3]  # Convert RGBA to RGB
    plt.close(fig)
    
    return image

def create_network_gif(panel_df, deals_df, quarters, target_col, output_path, fps=1):
    """
    Create animated GIF of network evolution across quarters.
    """
    if not NETWORK_AVAILABLE:
        log("[SKIP] Network GIF creation skipped - dependencies not available.")
        return
    
    log(f"Creating Network GIF for {len(quarters)} quarters...")
    
    frames = []
    for q in tqdm(quarters, desc="Network Frames"):
        frame = create_network_frame(panel_df, deals_df, q, target_col)
        if frame is not None:
            frames.append(frame)
    
    if len(frames) < 2:
        log("[WARN] Not enough frames for GIF.")
        return
    
    imageio.mimsave(output_path, frames, fps=fps, loop=0)
    log(f"âœ… Network GIF saved to {output_path}")

# Run Network GIF (on last 8 quarters)
if NETWORK_AVAILABLE and "labeled_panel" in dir() and "panel_q" in labeled_panel.columns:
    recent_quarters = sorted(labeled_panel["panel_q"].unique())[-8:]
    gif_path = os.path.join(ARTIFACT_DIR, "network_correlation.gif")
    create_network_gif(labeled_panel, deals_df, recent_quarters, TARGET_HORIZON, gif_path)

log("âœ… Cell K Complete: Network GIF.")

# %% [markdown]
# # Cell L: Full Advanced Backtest with Correlation Screening
#
# **Goal**: Run the complete backtest with all advanced features.

# %%
def run_advanced_backtest(panel_df, target_col, feature_cols, horizons=[3]):
    """
    Run full expanding-window backtest with:
    - Correlation screening
    - Dynamic K selection
    - Multiple baselines (Universe, Random, S&P 500)
    """
    log("Starting Advanced Backtest...")
    
    panel_df["panel_q"] = panel_df["datadate"].dt.to_period("Q")
    quarters = sorted(panel_df["panel_q"].unique())
    
    # Need at least 10 quarters of burn-in
    if len(quarters) < 12:
        log("[WARN] Not enough quarters for advanced backtest.")
        return None
    
    results = []
    
    for q_idx in tqdm(range(10, len(quarters)), desc="Backtest"):
        q = quarters[q_idx]
        
        train_mask = panel_df["panel_q"] < q
        test_mask = panel_df["panel_q"] == q
        
        train = panel_df[train_mask]
        test = panel_df[test_mask].copy()
        
        if len(test) == 0 or train[target_col].sum() < 10:
            continue
        
        # Downsample train
        pos = train[train[target_col] == 1]
        neg = train[train[target_col] == 0]
        if len(neg) > len(pos) * 10:
            neg = neg.sample(n=len(pos) * 10, random_state=42)
        train_bal = pd.concat([pos, neg])
        
        # Train
        X_cols = [c for c in feature_cols if c in train_bal.columns]
        clf = HistGradientBoostingClassifier(learning_rate=0.05, max_depth=5, random_state=42)
        clf.fit(train_bal[X_cols].fillna(0), train_bal[target_col])
        
        # Predict
        test["prob_acq"] = clf.predict_proba(test[X_cols].fillna(0))[:, 1]
        test["net_alpha"] = (test["prob_acq"] - train[target_col].mean()) * DEAL_PREMIUM_EST - COST_PER_TRADE
        
        # Correlation screening
        candidates = test.sort_values("net_alpha", ascending=False).head(int(K_MAX * 4))
        corr_mat = build_corr_matrix(panel_df, candidates["cik"].unique(), q)
        selected = select_with_corr_screen(candidates, corr_mat)
        
        if len(selected) < K_MIN:
            selected = test.nlargest(K_MIN, "prob_acq")
        
        # Portfolio return
        port_ret = clip_returns(selected["ret_fwd_1q"]).mean() if "ret_fwd_1q" in selected.columns else np.nan
        univ_ret = clip_returns(test["ret_fwd_1q"]).mean() if "ret_fwd_1q" in test.columns else np.nan
        spy_ret = spy_map.get(q, np.nan) if "spy_map" in dir() else np.nan
        
        # AUC
        try:
            auc_val = roc_auc_score(test[target_col], test["prob_acq"])
        except:
            auc_val = 0.5
        
        results.append({
            "quarter": str(q),
            "auc": auc_val,
            "n_bets": len(selected),
            "port_ret": port_ret,
            "univ_ret": univ_ret,
            "spy_ret": spy_ret,
            "excess_vs_univ": port_ret - univ_ret if not np.isnan(univ_ret) else np.nan,
            "excess_vs_spy": port_ret - spy_ret if not np.isnan(spy_ret) else np.nan,
            "hit_rate": selected[target_col].mean() if target_col in selected.columns else np.nan,
        })
    
    return pd.DataFrame(results)

# Run Advanced Backtest
if "labeled_panel" in dir() and "final_feats" in dir():
    adv_results = run_advanced_backtest(labeled_panel, TARGET_HORIZON, final_feats)
    if adv_results is not None:
        adv_path = os.path.join(ARTIFACT_DIR, "results_advanced_backtest.parquet")
        adv_results.to_parquet(adv_path)
        log(f"âœ… Advanced Backtest saved to {adv_path}")
        
        # Summary
        print("\n" + "="*60)
        print("ADVANCED BACKTEST SUMMARY")
        print("="*60)
        print(f"Mean AUC:         {adv_results['auc'].mean():.3f}")
        print(f"Mean Port Return: {adv_results['port_ret'].mean():.2%}")
        print(f"Mean Excess/Univ: {adv_results['excess_vs_univ'].mean():.2%}")
        print(f"Mean Excess/SPY:  {adv_results['excess_vs_spy'].dropna().mean():.2%}")
        print(f"Mean Hit Rate:    {adv_results['hit_rate'].mean():.2%}")

log("âœ… Pipeline Complete with All Features.")

# Final Calibration Footer
print("\n" + "="*60)
print("Persona Used: Senior Quant ML Engineer")
print("Difficulty Tuning: ~70% ZPD (Full Pipeline), ~20% Staff+ (Correlation Screening + Network GIF), ~10% Aspirational (Advanced Portfolio Construction)")
print("Concrete Suggestion: Review the network GIF to identify clustered firms that may co-move during M&A waves.")
print("="*60)

# %% [markdown]
# # Cell M: Probability Calibration Pipeline
#
# **Goal**: Correct raw model probabilities for downsampling bias and calibrate to empirical frequencies.
#
# **Two-Layer Approach**:
# 1. **Layer A (Prior Correction)**: Adjust for case-control sampling using log-odds correction
# 2. **Layer B (Calibration)**: Map to empirical frequencies using isotonic regression or Platt scaling
#
# ---
# ## Compute / GPU Utilization Note (A100)
#
# The current pipeline is primarily **Pandas** for data prep/feature engineering and **scikit-learn HistGradientBoostingClassifier** for modeling.
# This configuration is **CPU-bound** and will **not materially utilize an NVIDIA A100 (80GB) GPU**.
#
# GPU acceleration will only occur if migrated to:
# - **RAPIDS cuDF** for DataFrame operations
# - **XGBoost/LightGBM** with CUDA-enabled training
#
# The redesign prioritizes **correctness and reproducibility** first. GPU paths are optional accelerations.

# %%
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.linear_model import LogisticRegression
import pickle

# Configuration
USE_GPU = False  # Set True to enable GPU paths (requires XGBoost + CUDA)
CAL_WINDOW_Q = 4  # Number of quarters for calibration set

def sigmoid(x):
    """Numerically stable sigmoid."""
    return np.where(x >= 0, 
                    1 / (1 + np.exp(-x)), 
                    np.exp(x) / (1 + np.exp(x)))

def logit(p, eps=1e-7):
    """Safe logit transformation."""
    p = np.clip(p, eps, 1 - eps)
    return np.log(p / (1 - p))

def prior_correction(p_sample, pi_true, pi_sample):
    """
    Layer A: Prior correction for case-control (downsampling) adjustment.
    
    Given model trained on downsampled data with sample base rate pi_sample,
    correct to true population base rate pi_true.
    
    Formula: logit(p) = logit(p_s) + log(pi/(1-pi)) - log(pi_s/(1-pi_s))
    """
    if pi_true <= 0 or pi_true >= 1 or pi_sample <= 0 or pi_sample >= 1:
        return p_sample  # Cannot correct, return raw
    
    log_odds_true = np.log(pi_true / (1 - pi_true))
    log_odds_sample = np.log(pi_sample / (1 - pi_sample))
    
    logit_p_sample = logit(p_sample)
    logit_p_corrected = logit_p_sample + log_odds_true - log_odds_sample
    
    return sigmoid(logit_p_corrected)

def fit_platt_calibrator(probs, labels):
    """
    Layer B Option 1: Platt scaling (logistic regression on logits).
    Returns calibrator function and parameters.
    """
    if len(np.unique(labels)) < 2:
        return None, None
    
    X_logit = logit(probs).reshape(-1, 1)
    lr = LogisticRegression(solver='lbfgs', max_iter=1000)
    lr.fit(X_logit, labels)
    
    params = {"coef": float(lr.coef_[0, 0]), "intercept": float(lr.intercept_[0])}
    
    def calibrator(p):
        return lr.predict_proba(logit(p).reshape(-1, 1))[:, 1]
    
    return calibrator, params

def fit_isotonic_calibrator(probs, labels):
    """
    Layer B Option 2: Isotonic regression calibration.
    """
    from sklearn.isotonic import IsotonicRegression
    
    if len(np.unique(labels)) < 2:
        return None, None
    
    ir = IsotonicRegression(out_of_bounds='clip')
    ir.fit(probs, labels)
    
    def calibrator(p):
        return ir.predict(p)
    
    return calibrator, {"method": "isotonic"}

def compute_calibration_metrics(y_true, p_pred, n_bins=10):
    """
    Compute calibration metrics: Brier score, ECE, reliability bins.
    """
    # Brier score
    brier = np.mean((p_pred - y_true) ** 2)
    
    # Reliability bins
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(p_pred, bin_edges) - 1
    bin_indices = np.clip(bin_indices, 0, n_bins - 1)
    
    bins = []
    ece = 0.0
    for i in range(n_bins):
        mask = bin_indices == i
        if mask.sum() > 0:
            mean_pred = p_pred[mask].mean()
            mean_true = y_true[mask].mean()
            count = mask.sum()
            bins.append({
                "bin": i,
                "mean_predicted": mean_pred,
                "mean_realized": mean_true,
                "count": int(count)
            })
            ece += (count / len(p_pred)) * abs(mean_pred - mean_true)
    
    return {
        "brier": brier,
        "ece": ece,
        "bins": bins
    }

def calibrate_quarter(panel_df, train_df, test_df, target_col, model, feature_cols, 
                      pi_train_full, pi_train_sample, artifact_dir, quarter_str):
    """
    Full calibration pipeline for a single quarter.
    
    Returns: DataFrame with p_raw, p_prior_corrected, p_calibrated columns
    """
    # Get raw probabilities
    X_test = test_df[feature_cols].fillna(0)
    p_raw = model.predict_proba(X_test)[:, 1]
    
    # Layer A: Prior correction
    if pi_train_sample != pi_train_full:
        p_prior_corrected = prior_correction(p_raw, pi_train_full, pi_train_sample)
        log(f"Prior correction applied: Ï€_full={pi_train_full:.4f}, Ï€_sample={pi_train_sample:.4f}")
    else:
        p_prior_corrected = p_raw.copy()
        log("No downsampling detected, skipping prior correction.")
    
    # Layer B: Calibration using held-out calibration set
    # Use most recent CAL_WINDOW_Q quarters before holdout as calibration set
    if "panel_q" not in train_df.columns:
        train_df["panel_q"] = train_df["datadate"].dt.to_period("Q")
    
    quarters = sorted(train_df["panel_q"].unique())
    cal_quarters = quarters[-CAL_WINDOW_Q:] if len(quarters) > CAL_WINDOW_Q else quarters
    
    cal_mask = train_df["panel_q"].isin(cal_quarters)
    cal_set = train_df[cal_mask]
    
    pi_cal = cal_set[target_col].mean() if len(cal_set) > 0 else pi_train_full
    
    if len(cal_set) > 50 and len(np.unique(cal_set[target_col])) == 2:
        # Get calibration set predictions
        X_cal = cal_set[feature_cols].fillna(0)
        p_cal_raw = model.predict_proba(X_cal)[:, 1]
        
        # Apply prior correction to cal set too
        if pi_train_sample != pi_train_full:
            p_cal_corrected = prior_correction(p_cal_raw, pi_train_full, pi_train_sample)
        else:
            p_cal_corrected = p_cal_raw
        
        # Fit calibrator on calibration set
        try:
            calibrator, cal_params = fit_isotonic_calibrator(p_cal_corrected, cal_set[target_col].values)
            if calibrator is not None:
                p_calibrated = calibrator(p_prior_corrected)
                log("Isotonic calibration applied.")
            else:
                p_calibrated = p_prior_corrected.copy()
                cal_params = {"method": "none", "reason": "calibrator_fit_failed"}
        except Exception as e:
            log(f"Isotonic failed ({e}), falling back to Platt scaling.")
            calibrator, cal_params = fit_platt_calibrator(p_cal_corrected, cal_set[target_col].values)
            if calibrator is not None:
                p_calibrated = calibrator(p_prior_corrected)
            else:
                p_calibrated = p_prior_corrected.copy()
                cal_params = {"method": "none", "reason": str(e)}
    else:
        p_calibrated = p_prior_corrected.copy()
        cal_params = {"method": "none", "reason": "insufficient_cal_data"}
        log(f"Calibration skipped: insufficient calibration data ({len(cal_set)} rows)")
    
    # Save calibration artifacts
    cal_dir = os.path.join(artifact_dir, "calibration", f"quarter_{quarter_str}")
    os.makedirs(cal_dir, exist_ok=True)
    
    with open(os.path.join(cal_dir, "calibrator_params.json"), "w") as f:
        json.dump(cal_params, f, indent=2, default=str)
    
    # Compute metrics
    y_test = test_df[target_col].values
    metrics_raw = compute_calibration_metrics(y_test, p_raw)
    metrics_corrected = compute_calibration_metrics(y_test, p_prior_corrected)
    metrics_calibrated = compute_calibration_metrics(y_test, p_calibrated)
    
    # Print calibration report
    print(f"\n{'='*50}")
    print(f"CALIBRATION REPORT: {quarter_str}")
    print(f"{'='*50}")
    print(f"Ï€_train_full (true base rate):    {pi_train_full:.4f}")
    print(f"Ï€_train_sample (after downsample): {pi_train_sample:.4f}")
    print(f"Ï€_calibration_set:                 {pi_cal:.4f}")
    print(f"Ï€_holdout:                         {y_test.mean():.4f}")
    print(f"\nBrier Scores:")
    print(f"  p_raw:              {metrics_raw['brier']:.4f}")
    print(f"  p_prior_corrected:  {metrics_corrected['brier']:.4f}")
    print(f"  p_calibrated:       {metrics_calibrated['brier']:.4f}")
    print(f"ECE (p_calibrated):   {metrics_calibrated['ece']:.4f}")
    
    # Save calibration bins
    bins_df = pd.DataFrame(metrics_calibrated["bins"])
    bins_df.to_csv(os.path.join(cal_dir, "calibration_bins.csv"), index=False)
    
    # Return result
    result_df = test_df.copy()
    result_df["p_raw"] = p_raw
    result_df["p_prior_corrected"] = p_prior_corrected
    result_df["p_calibrated"] = p_calibrated
    
    return result_df, {
        "quarter": quarter_str,
        "pi_train_full": pi_train_full,
        "pi_train_sample": pi_train_sample,
        "pi_cal": pi_cal,
        "pi_holdout": float(y_test.mean()),
        "brier_raw": metrics_raw["brier"],
        "brier_corrected": metrics_corrected["brier"],
        "brier_calibrated": metrics_calibrated["brier"],
        "ece_calibrated": metrics_calibrated["ece"],
    }

def plot_reliability_diagram(y_true, p_raw, p_corrected, p_calibrated, title, save_path):
    """
    Plot reliability diagram comparing raw, corrected, and calibrated probabilities.
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for ax, (probs, name) in zip(axes, [(p_raw, "Raw"), (p_corrected, "Prior Corrected"), (p_calibrated, "Calibrated")]):
        prob_true, prob_pred = calibration_curve(y_true, probs, n_bins=10, strategy='uniform')
        
        ax.plot([0, 1], [0, 1], "k--", label="Perfect")
        ax.plot(prob_pred, prob_true, "s-", color="crimson", label=name)
        ax.set_xlabel("Mean Predicted Probability")
        ax.set_ylabel("Fraction of Positives")
        ax.set_title(f"{name}")
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    fig.suptitle(title, fontsize=14)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.show()

def plot_probability_histograms(p_raw, p_calibrated, title, save_path):
    """
    Plot histograms showing probability compression after calibration.
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    axes[0].hist(p_raw, bins=50, color='steelblue', alpha=0.7, edgecolor='white')
    axes[0].set_xlabel("Probability")
    axes[0].set_ylabel("Count")
    axes[0].set_title("Raw Model Probabilities")
    axes[0].set_xlim(0, 1)
    
    axes[1].hist(p_calibrated, bins=50, color='crimson', alpha=0.7, edgecolor='white')
    axes[1].set_xlabel("Probability")
    axes[1].set_ylabel("Count")
    axes[1].set_title("Calibrated Probabilities")
    axes[1].set_xlim(0, 1)
    
    fig.suptitle(title, fontsize=14)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.show()

log("âœ… Cell M Complete: Probability Calibration Pipeline Defined.")

# %% [markdown]
# # Cell N: Calibrated Backtest with Probability Rescaling
#
# **Goal**: Run the full backtest using calibrated probabilities instead of raw scores.

# %%
def run_calibrated_backtest(panel_df, target_col, feature_cols, artifact_dir):
    """
    Full backtest with probability calibration.
    Strategy uses p_calibrated instead of raw probabilities.
    """
    log("Starting Calibrated Backtest...")
    
    panel_df = panel_df.copy()
    panel_df["panel_q"] = panel_df["datadate"].dt.to_period("Q")
    quarters = sorted(panel_df["panel_q"].unique())
    
    if len(quarters) < 12:
        log("[WARN] Not enough quarters for calibrated backtest.")
        return None, None
    
    results = []
    calibration_summary = []
    
    for q_idx in tqdm(range(10, len(quarters)), desc="Calibrated Backtest"):
        q = quarters[q_idx]
        quarter_str = str(q).replace("/", "_")
        
        train_mask = panel_df["panel_q"] < q
        test_mask = panel_df["panel_q"] == q
        
        train_full = panel_df[train_mask]
        test = panel_df[test_mask].copy()
        
        if len(test) == 0 or train_full[target_col].sum() < 10:
            continue
        
        # True base rate (before downsampling)
        pi_train_full = train_full[target_col].mean()
        
        # Downsample
        pos = train_full[train_full[target_col] == 1]
        neg = train_full[train_full[target_col] == 0]
        if len(neg) > len(pos) * 10:
            neg = neg.sample(n=len(pos) * 10, random_state=42)
        train_sample = pd.concat([pos, neg])
        
        # Sample base rate (after downsampling)
        pi_train_sample = train_sample[target_col].mean()
        
        # Train model
        X_cols = [c for c in feature_cols if c in train_sample.columns]
        clf = HistGradientBoostingClassifier(learning_rate=0.05, max_depth=5, random_state=42)
        clf.fit(train_sample[X_cols].fillna(0), train_sample[target_col])
        
        # Calibrate
        test_calibrated, cal_metrics = calibrate_quarter(
            panel_df, train_full, test, target_col, clf, X_cols,
            pi_train_full, pi_train_sample, artifact_dir, quarter_str
        )
        calibration_summary.append(cal_metrics)
        
        # Portfolio using calibrated probabilities
        test_calibrated["net_alpha"] = (test_calibrated["p_calibrated"] - pi_train_full) * DEAL_PREMIUM_EST - COST_PER_TRADE
        candidates = test_calibrated.sort_values("net_alpha", ascending=False).head(K_MAX * 2)
        
        # Correlation screening
        corr_mat = build_corr_matrix(panel_df, candidates["cik"].unique(), q) if "cik" in candidates.columns else None
        selected = select_with_corr_screen(candidates, corr_mat) if corr_mat is not None else candidates.head(K_MAX)
        
        if len(selected) < K_MIN:
            selected = test_calibrated.nlargest(K_MIN, "p_calibrated")
        
        # Calculate returns
        port_ret = clip_returns(selected["ret_fwd_1q"]).mean() if "ret_fwd_1q" in selected.columns else np.nan
        univ_ret = clip_returns(test["ret_fwd_1q"]).mean() if "ret_fwd_1q" in test.columns else np.nan
        spy_ret = spy_map.get(q, np.nan) if "spy_map" in dir() else np.nan
        
        # AUC
        try:
            auc_val = roc_auc_score(test[target_col], test_calibrated["p_calibrated"])
        except:
            auc_val = 0.5
        
        # Expected deals in selection
        expected_deals = selected["p_calibrated"].sum() if "p_calibrated" in selected.columns else np.nan
        realized_deals = selected[target_col].sum() if target_col in selected.columns else np.nan
        
        results.append({
            "quarter": str(q),
            "auc": auc_val,
            "n_bets": len(selected),
            "port_ret": port_ret,
            "univ_ret": univ_ret,
            "spy_ret": spy_ret,
            "excess_vs_univ": port_ret - univ_ret if not np.isnan(univ_ret) else np.nan,
            "excess_vs_spy": port_ret - spy_ret if not np.isnan(spy_ret) else np.nan,
            "hit_rate": selected[target_col].mean() if target_col in selected.columns else np.nan,
            "expected_deals": expected_deals,
            "realized_deals": realized_deals,
            "mean_p_calibrated_top50": test_calibrated.nlargest(50, "p_calibrated")["p_calibrated"].mean(),
        })
        
        # Print top 10 bets with calibrated probabilities
        if q_idx % 8 == 0:  # Print every 8th quarter
            print(f"\n--- Top 10 Bets for {q} ---")
            top_10 = selected.nlargest(10, "p_calibrated")
            cols_show = [c for c in ["cik", "p_calibrated", "p_raw", target_col] if c in top_10.columns]
            print(top_10[cols_show].to_string(index=False))
    
    # Save results
    results_df = pd.DataFrame(results)
    results_path = os.path.join(artifact_dir, "results_calibrated_backtest.parquet")
    results_df.to_parquet(results_path)
    
    cal_summary_df = pd.DataFrame(calibration_summary)
    cal_summary_path = os.path.join(artifact_dir, "calibration_summary.parquet")
    cal_summary_df.to_parquet(cal_summary_path)
    
    log(f"âœ… Calibrated Backtest saved to {results_path}")
    log(f"âœ… Calibration Summary saved to {cal_summary_path}")
    
    # Plot calibration over time
    if len(cal_summary_df) > 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # Brier scores over time
        axes[0, 0].plot(cal_summary_df["quarter"], cal_summary_df["brier_raw"], label="Raw", marker="o")
        axes[0, 0].plot(cal_summary_df["quarter"], cal_summary_df["brier_corrected"], label="Corrected", marker="s")
        axes[0, 0].plot(cal_summary_df["quarter"], cal_summary_df["brier_calibrated"], label="Calibrated", marker="^")
        axes[0, 0].set_xlabel("Quarter")
        axes[0, 0].set_ylabel("Brier Score")
        axes[0, 0].set_title("Brier Score Over Time (Lower is Better)")
        axes[0, 0].legend()
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # ECE over time
        axes[0, 1].plot(cal_summary_df["quarter"], cal_summary_df["ece_calibrated"], color="crimson", marker="o")
        axes[0, 1].set_xlabel("Quarter")
        axes[0, 1].set_ylabel("ECE")
        axes[0, 1].set_title("Expected Calibration Error Over Time")
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # Base rates
        axes[1, 0].plot(cal_summary_df["quarter"], cal_summary_df["pi_train_full"], label="Train Full", marker="o")
        axes[1, 0].plot(cal_summary_df["quarter"], cal_summary_df["pi_holdout"], label="Holdout", marker="s")
        axes[1, 0].set_xlabel("Quarter")
        axes[1, 0].set_ylabel("Base Rate")
        axes[1, 0].set_title("Base Rates Over Time")
        axes[1, 0].legend()
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # Mean calibrated prob vs hit rate
        if len(results_df) > 0:
            axes[1, 1].scatter(results_df["mean_p_calibrated_top50"], results_df["hit_rate"], alpha=0.7)
            axes[1, 1].plot([0, 0.5], [0, 0.5], "k--", label="Perfect Calibration")
            axes[1, 1].set_xlabel("Mean Calibrated Prob (Top 50)")
            axes[1, 1].set_ylabel("Realized Hit Rate (Top 50)")
            axes[1, 1].set_title("Calibration vs Realized Hit Rate")
            axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(artifact_dir, "calibration_diagnostics.png"), dpi=150)
        plt.show()
    
    # Summary
    print("\n" + "="*60)
    print("CALIBRATED BACKTEST SUMMARY")
    print("="*60)

# ================================================
# RUNTIME VERIFICATION HARNESS (Section 10 of TODO.md)
# ================================================
try:
    from src.verify.verify_performance import run_verification_harness, print_verification_summary
    is_second_run = COMPILE_ARTIFACTS.cache_hit
    verification_result = run_verification_harness(
        timing_log=TIMING_LOGGER.rows,
        compile_artifacts=COMPILE_ARTIFACTS,
        is_second_run=is_second_run
    )
    print_verification_summary(verification_result)
    verify_path = os.path.join(ARTIFACT_DIR, "verification_result.json")
    with open(verify_path, "w") as f:
        json.dump({"passed": verification_result.passed, "details": verification_result.details}, f, indent=2, default=str)
    log(f"Verification result saved: {verify_path}")
except Exception as e:
    log(f"[WARN] Verification harness error: {e}")

    print(f"Mean AUC:               {results_df['auc'].mean():.3f}")
    print(f"Mean Port Return:       {results_df['port_ret'].mean():.2%}")
    print(f"Mean Excess/Universe:   {results_df['excess_vs_univ'].mean():.2%}")
    print(f"Mean Excess/SPY:        {results_df['excess_vs_spy'].dropna().mean():.2%}")
    print(f"Mean Hit Rate:          {results_df['hit_rate'].mean():.2%}")
    print(f"Mean Expected Deals:    {results_df['expected_deals'].mean():.2f}")
    print(f"Mean Realized Deals:    {results_df['realized_deals'].mean():.2f}")
    print(f"Mean Brier (Calibrated):{cal_summary_df['brier_calibrated'].mean():.4f}")
    
    return results_df, cal_summary_df

# Run Calibrated Backtest
if "labeled_panel" in dir() and "final_feats" in dir():
    cal_results, cal_summary = run_calibrated_backtest(labeled_panel, TARGET_HORIZON, final_feats, ARTIFACT_DIR)

log("âœ… Cell N Complete: Calibrated Backtest.")

# %% [markdown]
# # Performance Budget Table
#
# | Stage | CPU Time | GPU Time (Optional) | Notes |
# |-------|----------|---------------------|-------|
# | Data Loading | ~10s | N/A | I/O bound, GPU won't help |
# | Feature Engineering | ~30-60s | ~5-10s (cuDF) | Pandas -> RAPIDS cuDF |
# | Training (per quarter) | ~2-5s | ~0.5-1s (XGBoost CUDA) | sklearn -> XGBoost GPU |
# | Backtest Loop | ~5-15min | ~2-5min | Mostly training time |
# | Plotting | ~10s | N/A | Matplotlib is CPU-only |

# %%
# Final Footer with All Features
print("\n" + "="*70)
print("M&A PREDICTION PIPELINE - COMPLETE")
print("="*70)
print("\nFeatures Implemented:")
print("  âœ… Multi-horizon labeling (3m-24m)")
print("  âœ… S&P 500 baseline comparison")
print("  âœ… Stability dry-run harness (Jaccard overlap)")
print("  âœ… Event study verification (jump plot)")
print("  âœ… Correlation screening for portfolio construction")
print("  âœ… Network correlation GIF")
print("  âœ… Quarter deep-dive reports")
print("  âœ… Probability calibration (prior correction + isotonic/Platt)")
print("\nCompute Mode: CPU (HistGradientBoosting)")
print("GPU Acceleration: Not enabled (set USE_GPU=True for XGBoost CUDA)")
print("\nPersona Used: Senior Quant ML Engineer")
print("Difficulty: ~70% ZPD, ~20% Staff+, ~10% Aspirational")
print("="*70)
